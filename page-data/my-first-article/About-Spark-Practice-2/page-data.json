{"componentChunkName":"component---node-modules-gatsby-theme-portfolio-minimal-src-templates-article-index-tsx","path":"/my-first-article/About-Spark-Practice-2/","result":{"pageContext":{"article":{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org/streaming/\">Spark Streaming</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABKklEQVR42o2S3UvDMBTF94crDp+UOR8m6iaCU5ko3Z6GdtBG6frdPYgvE6Z0XSuKlcZ+0I1ikxrtEMfasvNwSeD+cpKTW0pWEcYJRj91UaWszgURDKN4mcyA58C/LalB72x6f/flvM1vUeDseV4YhtCBvu8H09mMOY802r3aiZ4ffmGUAacm9rvdPDlVFRWAW4EXJEnhLo4vW62Absa2leucwpb1Ut7Y3N6q1A8be7X9amW3WjtYWy9/vFoJinHem1MYQkgMGYZ1HMcYG6PR08SYmKb56bq5gaWkruttqjMcPvJ9XlU1WVYUWQEsEAVpoA3IQX+dmfCYojqiKNM3PZYB3e41x/XJQhLlRv2IBEF6EEJFaWeK5B9FUdE/E3+UI7zKkOQPKF7mvwHvMaPrfenbCQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png","srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/41200/spark-st-cover.png 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/f979a/spark-st-cover.png 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/322ad/spark-st-cover.webp 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/de3b3/spark-st-cover.webp 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/2b2b5/spark-st-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>💬 Apache Spark Streaming Practice</h1>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/elk-kafka-spark\" alt=\"Total\" width=\"600\">\n<p>오늘은 <strong>Spark Streaming</strong>을 중점적으로 활용한 실시간 처리 파이프라인을 구성하는 실습을 진행보겠습니다.</p>\n<p>아직 진행중이라 구성은 조금씩 바뀔 수 있습니다.</p>\n<p>Filebeat로 적재를 할지 그냥 Logstash를 쓸지 아님 다른 stack 을 쓸 수 있어요.</p>\n<p>근데 지금은 일단 Logstash로 진행중입니다.</p>\n<p><strong>전체적인 구성</strong>은 위에 이미지와 동일합니다.</p>\n<p><strong>Logstash</strong>는 실시간으로 발생하는 데이터를 수집해서 <strong>Kafka</strong>로 보내고</p>\n<p><strong>Kafka</strong>에서 메시지를 받고, <strong>Spark Streaming</strong>을 활요해서</p>\n<p><strong>Jupyter</strong>나 <strong>Zeplin</strong>에서 분석도 진행해볼 예정입니다.</p>\n<p><strong>HDFS</strong>에 데이터 저장까지 완료입니다.</p>\n<h2>🧹 Logstash</h2>\n<p>Logstash는 실시간 데이터를 수집하여 Kafka에 적재하는 역할입니다.</p>\n<p>현재 일단 Elk stack으로 사용하는게 아니라 따로 Logstash 컨테이너를 만들어 수집하도록 하고 있습니다.</p>\n<p><a href=\"https://github.com/jms0522/hadoop_system/tree/main/hadoop/logstash\">logstash.conf</a>에서 코드는 보실 수 있습니다.</p>\n<p>Logstash에서 conf에서 filter를 통해 데이터를 변환해서 kafka에 적잽합니다.</p>\n<h2>🔧 Kafka</h2>\n<p>카프카는 메시징 큐의 역할로 스트리밍의 비동기 처리를 가능하게 하고, 처리 속도 차이를 흡수 할 수 있습니다.</p>\n<p>또한 스트리밍 데이터의 데이터 유실을 방지할 수 있어 브로커로 두고 여기서 Spark를 통해 분석을 할 수 있습니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/kafka_logs\" alt=\"logs\" width=\"600\">\n<p>위의 이미지처럼 카프카에 실시간으로 로그 데이터가 들어오는 걸 확인할 수 있다.</p>\n<h2>📊 Spark Streaming</h2>\n<p>카프카 Streaming을 통해 데이터를 분석</p>\n<p>분석된 데이터는 s3, hdfs등을 통해 저장.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/kafka-hdfs\" alt=\"hdfs\" width=\"600\">\n<p>-- 진행중....</p>","categories":["Spark","Streaming","ALL"],"date":"September 07, 2024","description":"Spark의 Streaming 실습.","id":"80664fe7-53aa-5349-be37-1127cd091cfc","keywords":["Pipeline","Streaming","Blog","Spark"],"slug":"/my-first-article/About-Spark-Practice-2/","title":" 🌟 [Spark] Spark Practice -2 (Streaming) ","readingTime":{"text":"3 min read"}},"listingPagePath":"/blog"}},"staticQueryHashes":["3262260831","948380417"],"slicesMap":{}}