{"componentChunkName":"component---node-modules-gatsby-theme-portfolio-minimal-src-templates-article-listing-index-tsx","path":"/blog/","result":{"pageContext":{"articles":[{"banner":{"alt":"Kafka","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka Parallel Consumer</h1>\n<p>오늘은 <strong>Parallel Consumer</strong> 에 대해서 알아보겠습니다.</p>\n<p>카프카에서 처리량을 늘리기 위한 쉬운 방법 중 하나는 파티션을 늘리는 것 입니다.</p>\n<p>파티션 수가 늘어나면 그 만큼 컨슈머가 늘어나기 때문입니다.</p>\n<p>그에 따른 병렬 처리로 병목 현상도 줄어들고 자원을 더 효율적으로 사용할 수 있습니다.</p>\n<p>하지만 처리량을 늘린다고 파티션 수만 증가 시키다 보면 많은 파티션 수에 따른 사이드 이펙트도 존재합니다.</p>\n<p>또한 파티션 수는 한 번 늘리면 줄일 수 없기에 신중해야 합니다.</p>\n<p><em><strong>그럼 어떤 방식으로 처리량을 늘릴 수 있을까요?</strong></em></p>\n<h2>무지성 파티션 늘리기</h2>\n<p>파티션을 늘린다면 처리량은 증가하겠지만 여러 단점이 존재합니다.</p>\n<p>카프카는 파티션별로 데이터를 저장하는데 단순 데이터 뿐만 아니라 메타데이터까지 저장하기 때문에 파티션이 늘면</p>\n<p>필요한 리소스가 증가한다는 점을 유의해야합니다.</p>\n<p>또한 파티션 단위로 설정된 replicas 수 만큼 복제가 되기 때문에 디스크 사용량이나 지연률이 증가합니다.</p>\n<h2>Parallel Consumer ?</h2>\n<p>파티션을 늘리지 않으면서 처리량을 늘리기 위해서 사용하는 게 바로 Parallel Consumer 입니다.</p>\n<p>Parallel Consumer는 병렬 처리 단위를 파티션이 아닌 메시지 단위로 설정을 할 수 있습니다.</p>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/parellel-consumer/kafka-basic.png\" alt=\"Kafka\"></p>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/parellel-consumer/kafka-parallel.png\" alt=\"Parallel Consumer\"></p>\n<p>위의 그림과 같이 하나의 파티션인데 복수의 스레드를 활용하여 3개의 메시지를 동시에 처리하는 것을 볼 수 있습니다.</p>","categories":["Kafka","ALL"],"date":"November 20, 2024","description":"Kafka Parallel Consumer 소개글입니다.","id":"dbff0aac-22c1-52b1-b0ee-ad924701c314","keywords":["Pipeline","Kafka","KRaft","Consumer"],"slug":"/my-first-article/About-Kafka-Parallel-Consumer/","title":" 🚀 [Kafka] About Kafka Parallel Consumer ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Python","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACBUlEQVR42nVS3WsTQRDPv1afpE8+1IdaBCntk+Cr/4T4EU0VWmgRBBWFWmgrFWvBGsnlvEZtk5756uU+kmx6yX3s3t5md2/dS06EaofZZWZ2ZoeZ3y8nhEhS4RNNpDv0YKsDusCzum7HcXvAa5sDGSGEijSbZypETvwj4zHzgwihGKJYGhgTGMVBiDlPLmSmxREJOuDoDGh4HE6CibhEEoJYX6fWdx70s+KS/nJtb+Hp+7li7YV0acIZZQxjRsbpjSLGOWNMPsXFNZS/glevhevzNBikxZQSG9QMq4IiP2sBoVDLolYVP3+II00gmE3kn+NmGRta7JpyQTl5fNTvjfRR2BWcn4HhvZJe0FoPlF9P1EbhW/NRuX5fqa9oTSx/lDslkHar3G0LTnM+Ahsflzb2Fwu7c063fNywr789XN5V59+VFnfUpR315pZyY0tZ3laCkcdgCF/djp7NwvwMrmzKztxx9VPzs9k7oXHMuYA4/o/GRI4t0aThADe+4OZXirwMKsZZz9UNoAUREJcLGzqkXqTOcUKidNtTYnyoPMxvz67v31rdW3B9S0b4hDJ/NE2SG4/e3EErV9HjGbh5V6Kem6LaH7UU/fXhyfOqcTCm8UV4JyyUBnEtfPoJ6wfEtTOcJYyG5XXssA+QYfoO8P6WTfiCItI2gWGfWyBoOoE9QFOG/QZbXI2rEPtZ1wAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/bb69e7beaad15b7bb2c900d4d08956b9/2588e/thread-buffer.png","srcSet":"/static/bb69e7beaad15b7bb2c900d4d08956b9/0333b/thread-buffer.png 157w,\n/static/bb69e7beaad15b7bb2c900d4d08956b9/62f4c/thread-buffer.png 314w,\n/static/bb69e7beaad15b7bb2c900d4d08956b9/2588e/thread-buffer.png 627w","sizes":"(min-width: 627px) 627px, 100vw"},"sources":[{"srcSet":"/static/bb69e7beaad15b7bb2c900d4d08956b9/f7586/thread-buffer.webp 157w,\n/static/bb69e7beaad15b7bb2c900d4d08956b9/1e353/thread-buffer.webp 314w,\n/static/bb69e7beaad15b7bb2c900d4d08956b9/2ddd1/thread-buffer.webp 627w","type":"image/webp","sizes":"(min-width: 627px) 627px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>🪵 Thread</h1>\n<p>Python 으로 작업을 하다보면 멀티스레딩을 사용해야 하는 경우가 많은데, 파이썬은 GIL (Global Interpreter Lock) 으로</p>\n<p>CPU를 동시에 쓰는 것을 제한하고 있어서 한 번에 하나의 스레드만 코드를 실행 시킬 수 있습니다.</p>\n<p>물론 multiprocessing 모듈을 사용할 수 있는데 기본적으로 파일 기반 잠금이나 잠금 개체 혹은 queue를 사용하게 코드를 만들게 됩니다.</p>\n<p>queue 같은 경우는 데이터를 queue에 쌓아두고 새로운 thread 로 이 데이터를 읽는 역할을 해서 데이터를 빼내는 방법으로 진행 했습니다.</p>\n<p>또한 많은 프로세스가 하나의 파일의 대한 작업을 수행할 때 I/O로 인하여 충돌이 발생할 수 있는데 asyncio, concurrent.futures 같은</p>\n<p>비동기 방법이나 스레딩 방법을 사용하는 방법이 있습니다.</p>\n<p>fcntl. os 모듈 같은 경우 충돌을 방지하는 목적으로 순차적으로 처리하게 되는데 그 도중 필연적으로 다른 프로세스들은 대기 상태가 생기게 되고,</p>\n<p>이것은 데이터의 병목 현상과 성능을 저하시킬 우려가 있습니다.</p>\n<h1>Thread ?</h1>\n<p>일단 서론이 너무 길었는데 스레드란 결국 실행 흐름을 나누는 기본 단위입니다.</p>\n<p>하나의 프로세스는 여러 개의 스레드를 가질 수 있습니다.</p>\n<p>각 스레드는 독립적이게 작업을 수행하면서도, 다른 스레드와 메모리를 공유할 수 있습니다.</p>\n<p>메모리 공간을 공유하기에 데이터를 주고 받기가 편리하고 프로세스보다 가볍게 관리가 가능합니다.</p>\n<p>병렬로 실행이 가능하지만 앞서 말했지만 파이썬에서는 제약이 좀 있습니다.</p>\n<p>또한 동기화가 필요한데, 같은 자원에 여러 스레드가 한 번에 접근을 한다면 데이터가 손상될 가능성이 있습니다.</p>\n<p>다음 포스트에선 buffer에 대해서 알아보고 차이점과 같이 쓸 때 좋은 이점에 대해서 포스팅 하겠습니다 🫡</p>","categories":["ETC","ALL"],"date":"November 14, 2024","description":"Thread & Buffer 간단한 소개글입니다.","id":"76de97bb-2490-5308-8918-85dc4e2623b1","keywords":["Python","Thread","Buffer"],"slug":"/my-first-article/About-Thread-Buffer/","title":" 🚀 [etc] Thread & Buffer - 1 ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"sql","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB9klEQVR42mP4DwZ///0DkleefTl+79WHrwv33vRackZjxnHJKUfFZ5+UX33R9PLTnLdfLh6++enn7z//YYABQv35C9K8/dpb9qKjmaufLz37as+tG4funjlx7/Sxuxf2XX+04PA7n77rkdMuQdT/w9S8+9Z7hsyDTNkHGHIOMRWe5K88b9Bw2aD6gnzRCZnCI0JZhxLnXIPoxKJ51813QG0sxccYsg8xFBxmyD/Ml3VQsfiYWvkJlbLj8sXH4mdfw20zUHPmQeHqE+ELr0nXnxKsOF6w4pZNyxm9mpNevecl8o4k4NG8B+jstAPrL72efviJZvvZledfLj76dMv5l5Wrb68+9Vwg83DS3OsENMcuvTH72POk5TfXnHvFEL5rybFny48/m7n/MVPC/qQ51/H6OfuQ+4zL+2+979n3aOmZF7MOPF57+nnZilsXHn6Im3UlZd51AqHtPvNK6ILrrMVHJWpOrDr9YsmRJ9qVxwuX3YyecSVpDm4/77zxjiH3EHPuYYaMg4xFRxnyDovnH/Xtv6BddVIy/6hQ1uH4Wbg1H7jzniH9AHPRUbbSY8wlx4CkcuUJpZLj6uUndKpOCmcfjp99FaTzH6pmCOf337/VW+8LVB5nyD3MmH0QFG25h2UKjsoUHlUtOxY8+dL5hx/BaRmaPAEU6Rrp9FWz5QAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/44307aa0a1573d82578603d26ff363af/71292/sql-cover.png","srcSet":"/static/44307aa0a1573d82578603d26ff363af/699d3/sql-cover.png 85w,\n/static/44307aa0a1573d82578603d26ff363af/4f8ea/sql-cover.png 170w,\n/static/44307aa0a1573d82578603d26ff363af/71292/sql-cover.png 340w","sizes":"(min-width: 340px) 340px, 100vw"},"sources":[{"srcSet":"/static/44307aa0a1573d82578603d26ff363af/96d64/sql-cover.webp 85w,\n/static/44307aa0a1573d82578603d26ff363af/01006/sql-cover.webp 170w,\n/static/44307aa0a1573d82578603d26ff363af/18f98/sql-cover.webp 340w","type":"image/webp","sizes":"(min-width: 340px) 340px, 100vw"}]},"width":660,"height":399.88235294117646}}}},"body":"<h1>Procedure 잠금 제어에 대하여</h1>\n<p>오랜만입니다.</p>\n<p>그 동안 몸이 좀 안 좋았고 너무 바빠서 작성을 잘 못했습니다.</p>\n<p>오늘은 데이터베이스 프로시저를 생성할 때 잠금 동작을 제어하는 옵션 2가지를 살펴 보려고 합니다.</p>\n<hr>\n<h2>WITH(NOLOCK)</h2>\n<p><strong>테이블 혹은 뷰</strong>에 대해 공유 잠금을 걸지 않고 데이터를 읽는 것 입니다.</p>\n<p>SELECT 문에서 사용 시, 해당 테이블을 다른 트랜잭션이 업데이트 하더라도 즉시 데이터를 읽을 수 있습니다.</p>\n<p>당연하게도 다른 트랜잭션이 커밋하지 않은 데이터도 읽을 수 있어 변경이 될 가능성이 높습니다.</p>\n<h2>SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED</h2>\n<p>트랜잭션의 격리 수준을 <strong>'READ UNCOMMITTED'</strong> 으로 설정합니다.</p>\n<p>커밋되지 않은 데이터도 포함하여 읽을 수 있는 설정입니다.</p>\n<p>해당 트랜잭션이 실행 되는 모든 SELECT문에서 실행이 됩니다.</p>\n<h2>사용 목적</h2>\n<p>읽기 작업이 많은 시스템에서 성능을 높이고 싶을 때 사용합니다.</p>\n<p>무결성이 중요한 작업에선 사용을 하지 않는 게 좋습니다.</p>\n<ul>\n<li>\n<p>WITH(NOLOCK):</p>\n<p>특정 쿼리에서만 적용하고 싶을 때 사용</p>\n</li>\n<li>\n<p>SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED:</p>\n<p>트랜잭션 범위 내의 모든 읽기 작업에 적용하고 싶을 때 사용.</p>\n</li>\n</ul>","categories":["SQL","Procedure","ALL"],"date":"November 13, 2024","description":"프로시저에 대하여","id":"4e2960c8-1401-5e59-8933-5477f8a92549","keywords":["Procedure","SQL"],"slug":"/my-first-article/About-Mssql-1/","title":" ⚙️ [SQL] 프로시저 잠금 무시 ","readingTime":{"text":"2 min read"}},{"banner":{"alt":"Docker","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABxElEQVR42mNgwA0YGerrmcCstJmsnPX7ZRjq53OA+fX/mcDyJACI4vpNXCCDmCv2unG2n/zIWrHPGMlAUsF/Rtb6A3Ecrcd/A/EHzo5Tv5kr9rtwtB6z52g+UUK8wVBFTJX70jnbjv8HGvaTvfXYT862Ez/ZGo+8Zu84s5B/zr3/nI0Hl2L4CJuroAwm3okXTvIuePBfYOb1n+Kzr//hn3n9j+Dsm78EF9z/z9V5YhpD6QYpND04wKpVzCCKu+/cDJkNH/7bb7j/x2DNg/98S5/8FV3z5j9P/9mjDPZTeIgzDARCIQYyeM2XSF91fvumG6+/6C+99YVz4sV7PO3H+xkytwhCYv4MKzwVEARpm7gY4iFJxLZqu2RE6x5xhsRVoiTHKpjK367AkLelkCF/qydD4Q4zhrTd/AzZ2x0Y8rYlA+XUwbh4pzGQr8eQu8kef3os3cDLkL+tnKFody1Q4zyG3K3ZDHlbFzEUbPEHis0ByvUA8WSGgq0bGUr3zmco2GVC2KF5m4KBtkcADfJhyN+SAKYLtscCLfADijsz5G5OArLTgdiBoX4/CxHpEBjQ4IgBBsGq/5AIgmsEBQtSzP7HjGUAnDC6XUKGXWIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/f900c67fc8811fda8bf377b3493c5401/7fa9b/docker-cover.png","srcSet":"/static/f900c67fc8811fda8bf377b3493c5401/bbe8a/docker-cover.png 70w,\n/static/f900c67fc8811fda8bf377b3493c5401/09252/docker-cover.png 140w,\n/static/f900c67fc8811fda8bf377b3493c5401/7fa9b/docker-cover.png 279w","sizes":"(min-width: 279px) 279px, 100vw"},"sources":[{"srcSet":"/static/f900c67fc8811fda8bf377b3493c5401/2e00d/docker-cover.webp 70w,\n/static/f900c67fc8811fda8bf377b3493c5401/f8c65/docker-cover.webp 140w,\n/static/f900c67fc8811fda8bf377b3493c5401/75714/docker-cover.webp 279w","type":"image/webp","sizes":"(min-width: 279px) 279px, 100vw"}]},"width":660,"height":399.78494623655916}}}},"body":"<h1>🐳 Docker</h1>\n<h2>🔎 Overview</h2>\n<p>요즘 서비스를 컨테이너로 감싸면서 관리를 위해 쿠버네티스를 공부하고 싶은 마음이 있었는데</p>\n<p>그 전에 docker를 완벽하게 공부하고 싶다는 생각을 하게 되었습니다.</p>\n<p>요즘 뭔가를 배울 때 기본이 튼튼해야 한다는 생각을 많이 받아서 기초 부터 공부해겠습니다.</p>\n<h2>사용 이유</h2>\n<p>서비스를 제공하는 입장에서 하나의 OS에 모든 서비스를 운영하게 된다면 모종의 이유로 장애가 발생한다면</p>\n<p>그 안에 있는 모든 서비스에 영향이 생겨 제대로 운영이 불가할 겁니다.</p>\n<p>그래서 가상화 또는 컨테이너에 논리적인 컴퓨팅을 생성하여 독립적인 환경을 제공하여 주는데</p>\n<p>컨테이너는 하이퍼바이저와는 달리 가상의 OS를 만들지 않고 베이스 환경의 OS를 공유하면서 필요한 프로세스만\n격리하는 방식으로 보다 가볍고 속도가 빠른 장점이 있습니다.</p>\n<p>하이퍼바이저와 달리 호스트 OS의 커널을 사용하기 때문입니다.</p>\n<p>확장성도 우수한데 그냥 같은 이미지로 실행하여 컨테이너를 생성하면 됩니다.</p>\n<p>더 나아가 쿠버네티스로 대량의 컨테이너들을 관리 운영도 가능합니다.</p>\n<h2>사용법</h2>\n<p>일단 편집 안 하고 대충 올림</p>\n<p>OS + 구성요소 + 소프트웨어 를 압축한 파일 = 이미지</p>\n<p>docker pull 이미지 이름 : 이미지 다운</p>\n<p>docker run -d -p port:port</p>\n<h2>매타 데이터 확인</h2>\n<p>docker image inspect 이미지명: 이미지 세부 정보</p>\n<p>docker container inspect 컨테이너명 : 컨테이너 이미지 세부 정보</p>\n<p>docker rum --env KEy=VALUE 이미지명 : 컨테아너 실행 시 메타 데이터의 env 덮어쓰기</p>\n<h2>기본 사용법</h2>\n<p>docker create : 이미지를 컨테이너로 생성</p>\n<p>docker run(start + create)</p>\n<p>docker restart : 프로세스 재시작</p>\n<p>docker pause : 일시정지 (메모리에 저장)</p>\n<p>docker unpause : 일시 정지한 시점부터 다시 시작</p>\n<p>docker stop : 아예 멈춰버림 (메모리와 cpu 상태 종료)</p>\n<p>docker rm -f : 실행 중 이미지 삭제</p>\n<p>docker rm : 이미지 삭제</p>\n<p>docker logs 컨테이너명 : 컨테이너 로그</p>\n<p>docker commit -m 커밋명 실행중인 컨테이너명 생성할 이미지명</p>","categories":["Docker","ALL"],"date":"November 01, 2024","description":"Docker 소개글입니다.","id":"2bdc819b-76cd-50d9-8254-819de8906a9c","keywords":["Pipeline","Docker","Container"],"slug":"/my-first-article/About-Docker-1/","title":" 🚀 [Docker] About Docker","readingTime":{"text":"4 min read"}},{"banner":{"alt":"Log Data","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABVElEQVR42qWR30+CUBTH+f+rF6F0ZQv8bRYgEpkg0xAbYMivaDke7MUHMOd04uamdeZ9aU59qLO7u3vu7uf7Pedc7Psfge293Ww2f4TX6zXscRx/bWMymcA+n8+3qkdh5Gm+mhmSMgxD07Suqr50u3yND8PwmDMiXcdh7mml/QypoesgBJdwKBWLo9Hod1PYDjmbzVLJpKp2HMeBtKMouWw2CsN33+8ZPSSE+tpT9udw+FSvr1arxWIBbUdRNB6P4eC5rmmaSHEXRjm8yOdyUkM8TxCpiySeSIiiGASBLMtQttXv27Z9EIZuqyw7+BiIDfHs5PQ6nZabTRhV+vKK5ziQtizriLNH3pAMTbM0A9MG83arVeM4AiceBcE9BKNYLpe6pmUoCshK+baQLwgPAnyVJElVtup53kEYTXs6ncI/lUslMIQFXUAVd5UKgeP+m78D/wB3doaKZwucxAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/e5edfa90bed9e5d0db0ac09baed45d34/98851/log-cover.png","srcSet":"/static/e5edfa90bed9e5d0db0ac09baed45d34/4ec1c/log-cover.png 150w,\n/static/e5edfa90bed9e5d0db0ac09baed45d34/ca4aa/log-cover.png 300w,\n/static/e5edfa90bed9e5d0db0ac09baed45d34/98851/log-cover.png 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/e5edfa90bed9e5d0db0ac09baed45d34/40ac4/log-cover.webp 150w,\n/static/e5edfa90bed9e5d0db0ac09baed45d34/9e761/log-cover.webp 300w,\n/static/e5edfa90bed9e5d0db0ac09baed45d34/2ea1b/log-cover.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":660,"height":400.40000000000003}}}},"body":"<h1>🪵 Log Data</h1>\n<p>요즘 log data 를 만지고 있습니다.</p>\n<p>서비스에서 들어오고 적재가 되는 대량의 데이터에서 어떻게 하면 효율적이게 로그를 관리 및 추적 할 수 있을까 고민입니다.</p>\n<p>Log data와 효울적인 생성 및 관리를 위해 내용을 한 번 찾아봐서 글을 적게 됩니다.</p>\n<p>사실 글에서 유용한 건 없을 거 같습니다.</p>\n<p>서비스 마다, 환경에 따라 관리할 수 있는 방법이 다르니까요.</p>\n<p>그래도 보편적으로 이렇게 하면 좋다더라~ 이 정도로만 작성할게요.</p>\n<p>로그 데이터와 방법 공부해보죠!</p>\n<hr>\n<h1>Log Data 란?</h1>\n<h1>효율적인 관리 방법</h1>\n<h1>마무리</h1>","categories":["ETC","ALL"],"date":"October 30, 2024","description":"Kafka 간단한 소개글입니다.","id":"93efb634-d0c4-5583-be68-d835a0ae9e2c","keywords":["Log data"],"slug":"/my-first-article/About-Log-1/","title":" 🚀 [etc] Log Data ","readingTime":{"text":"1 min read"}},{"banner":{"alt":"Kafka","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka KRaft Properties Option</h1>\n<p>오늘은 굉장히 피곤한 관계로 간단하게 Broker, Controller의 properties option을 살펴 보겠습니다.</p>\n<p>KRaft 모드에선 server.properties를 설정 할 필요가 없는데요.</p>\n<p>Broker는 broker.properties로 Controller는 controller.properties로 실행하면 되기 때문이죠.</p>\n<p>하지만 그 외 플러그인이나 추가 서비스를 연결 한다고 하면 server.properties도 수정하면 되겠습니다.</p>\n<p>오늘은 broker와 contorller의 주요 옵션만 빠르게 확인 하겠습니다.</p>\n<hr>\n<h1>Broker Properties (broker.properties)</h1>\n<h2>브로커 설정 파일인 broker.properties는 메시지의 전송, 저장 및 네트워크 동작을 관리합니다.</h2>\n<h3>필수 옵션</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">•\tnode.id: 브로커의 고유 ID를 지정합니다. 클러스터 내에서 각 브로커는 고유한 ID를 가져야 합니다.\n\n•\tprocess.roles: 브로커 역할을 지정합니다. 이 옵션은 broker로 설정합니다.\n\n•\tlisteners: 브로커가 요청을 수신할 주소와 포트를 지정합니다. 예를 들어 PLAINTEXT://:9092로 설정할 수 있습니다.\n\n•\tlog.dirs: 메시지 로그 데이터를 저장할 디렉토리를 지정합니다.\n\n•\tmetadata.quorum.listeners: 메타데이터 관리를 위해 컨트롤러와 통신할 IP 및 포트를 지정합니다.\n\n•\tcontroller.listener.names: 브로커가 컨트롤러와 통신할 때 사용할 listener 이름을 지정합니다.\n\n•\tinter.broker.listener.name: 브로커 간 통신에 사용할 listener 이름을 지정합니다.</code></pre></div>\n<h3>선택 옵션</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">•\tnum.network.threads: 네트워크 요청을 처리하는 쓰레드 수를 설정합니다. 기본값은 3입니다.\n\n•\tnum.io.threads: I/O 요청을 처리하는 쓰레드 수를 설정합니다. 기본값은 8입니다.\n\n•\tsocket.send.buffer.bytes: 소켓 송신 버퍼 크기 설정입니다. 기본값은 100KB입니다.\n\n•\tsocket.receive.buffer.bytes: 소켓 수신 버퍼 크기 설정입니다. 기본값은 100KB입니다.\n\n•\tlog.segment.bytes: 세그먼트 파일의 최대 크기입니다. 기본값은 1GB입니다.\n\n•\tlog.retention.hours: 로그 보관 시간입니다. 기본값은 168시간 (7일)입니다.\n\n•\tlog.retention.bytes: 로그 보관 용량의 최대 크기입니다.\n\n•\tauto.create.topics.enable: 새로운 주제를 자동으로 생성할지 여부를 설정합니다. 기본값은 true입니다.\n\n•\tnum.partitions: 새로 생성되는 주제의 기본 파티션 수를 설정합니다. 기본값은 1입니다.</code></pre></div>\n<h1>Controller Properties (controller.properties)</h1>\n<h2>컨트롤러 설정 파일인 controller.properties는 클러스터의 메타데이터 관리와 브로커 조정을 담당합니다.</h2>\n<h3>필수 옵션</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">•\tnode.id: 컨트롤러의 고유 ID를 지정합니다. 각 컨트롤러는 클러스터 내에서 고유한 ID를 가져야 합니다.\n\n•\tprocess.roles: 컨트롤러 역할을 지정합니다. 이 옵션은 controller로 설정합니다.\n\n•\tcontroller.quorum.voters: 컨트롤러 선출 과정에 참여하는 모든 컨트롤러의 ID와 주소를 지정합니다. \n    \n    예를 들어 0@localhost:9093,1@localhost:9094와 같이 설정합니다.\n\n•\tlisteners: 컨트롤러가 다른 노드와 통신할 때 사용할 주소와 포트를 지정합니다.\n\n•\tlistener.security.protocol.map: listener 이름에 따른 보안 프로토콜을 설정합니다. 예: CONTROLLER:PLAINTEXT.</code></pre></div>\n<h3>선택 옵션</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">•\tcontroller.metadata.log.dirs: 메타데이터 로그 저장 경로를 지정합니다.\n\n•\tcontroller.request.timeout.ms: 컨트롤러 요청에 대한 타임아웃 시간(ms)을 지정합니다.\n\n•\tcontroller.heartbeat.interval.ms: 컨트롤러 간 하트비트 주기(ms) 설정입니다.\n션\n•\tcontroller.heartbeat.timeout.ms: 컨트롤러 간 하트비트 타임아웃 시간 설정입니다.\n\n•\tcontroller.threads: 메타데이터 업데이트 작업을 수행하는 쓰레드 수를 지정합니다. 기본값은 1입니다.</code></pre></div>\n<h3>기타 보안 및 네트워크 옵션</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">•\tssl.endpoint.identification.algorithm: SSL 엔드포인트 확인 알고리즘을 설정합니다. 기본값은 https입니다.\n\n•\tsecurity.inter.broker.protocol: 브로커 간 통신에서 사용할 보안 프로토콜을 설정합니다.\n\n•\tsasl.mechanism.inter.broker.protocol: SASL 인증 방식 설정입니다.</code></pre></div>\n<hr>\n<h3>마무리</h3>\n<p>필수 옵션은 당연히 지정을 해야하고 더 많은 선택 옵션들을 어떻게 지정하냐에 따라 클러스터의 성능 및</p>\n<p>처리하고자 하는 데이터에 맞는 구현이 가능합니다.</p>\n<p>'데이터 처리량' 이 하나도 늘리는데 굉장히 많은 방법이 있어, 찾아보면서 공부를 해봐야겠습니다..</p>\n<p>요즘 회사 때문에 정신이 없네요..</p>\n<p>그럼 이만..😢</p>","categories":["Kafka","ALL"],"date":"October 29, 2024","description":"Kafka 간단한 소개글입니다.","id":"db2ead53-157b-5679-9169-2299c11c2678","keywords":["Pipeline","Kafka","Kraft"],"slug":"/my-first-article/About-Kafka-Properties-Option/","title":" 🚀 [Kafka] About Kafka Kraft-Properties ","readingTime":{"text":"6 min read"}},{"banner":{"alt":"Kafka","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka Record</h1>\n<h2>🔎 Overview</h2>\n<p>카프카의 데이터를 부르는 명칭인 Record에 대하여.</p>\n<h2>Record</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/record.png\" alt=\"Record\" width=\"600\">\n<h2>구조</h2>\n<p>레코드는 타임스탬프, 헤더 , 메시지 키, 오프셋으로 구성되어 있습니다.</p>\n<p>프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장이 됩니다.</p>\n<p>한 번 저장된 레코드는 임의로 삭제 할 수 없고 오직 로그의 리텐션 기간 및 용량을 설정해 놓은 값에 따라서 삭제 됩니다.</p>\n<h3>timestamp</h3>\n<p>시간을 저장하는 용도로 사용합니다.</p>\n<p>따로 설정을 하지 않으면 기본 값인 프로듀서의 생성 시간(CreateTime)이 들어가게 됩니다.</p>\n<p>혹은 브로커에 적재된 시간(LogAppendTime)으로 설정이 가능합니다.</p>\n<p>해당 옵션은 토픽 단위로 설정이 가능하며 log.message.timestamp.type 의 설정을 통해 가능합니다.</p>\n<p>이러한 옵션을 통해 메시지의 생성 시간과 적재 시간의 차이를 계산하여 네트워크 전송 지연 시간을 체크할 수 있을 것 같습니다.</p>\n<h3>offset</h3>\n<p>프로듀서가 전송한 레코드가 브로커에 적재 될 시점에 오프셋이 지정됩니다.</p>\n<p>0부터 시작하여 1씩 증가하게 되고 컨슈머는 오프셋을 기준으로 읽은 데이터와 처리해야 할 데이터를 구분합니다. (Commit)</p>\n<p>각 메시지는 파티션 별로 고유한 오프셋을 가지므로 컨슈머에서 중복 처리를 방지하기 위한 목적으로도 사용합니다.</p>\n<h3>Header</h3>\n<p>key/value 데이터를 추가할 수 있습니다.</p>\n<h3>Message key</h3>\n<p>메시지 값의 분류하기 위한 용도로 사용되며 이를 파티셔닝이라고 부릅니다.</p>\n<p>간단한 설명으로 메시지 키를 통해 어떤 파티션에 들어갈건지 지정할 수 있다고 생각하시면 됩니다.</p>\n<p>메시지 키를 지정하지 않으면 (null) RR 방식으로 파티션에 분배해 저장합니다.</p>\n<p>null이 아닌 메시지 키는 특정 파티션에 맵핑되어 전달됩니다.</p>\n<h3>Value</h3>\n<p>메시지 값은 실질적으로 처리 할 데이터가 담기는 공간입니다.</p>\n<p>메시지의 포맷 값은 사용자애 의해 지정이 되는데, 다양한 형태가 가능합니다.</p>\n<p>필요에 따라 사용자 지정 포맷으로 직렬화/역직렬화 클래스를 만들어 사용도 가능합니다.</p>\n<p>브로커에 저장된 레코드의 메시지 값은 어떤 포맷으로 직렬화 되어 저장되는지 알 수 없기에 컨슈머는 역직렬화 포맷을 알아야 합니다.</p>\n<p>여기서 잠깐 ☝️</p>\n<p>직렬화와 역직렬화는 데이터 무결성을 유지하고 전송 효율을 높여줍니다.</p>\n<p>정확히 무엇인지 한 번 봐보죠!</p>\n<h4>직렬화 Serialization ?</h4>\n<p>프로듀서와 컨슈머가 전송을 하고 수신을 할 때 데이터를 각각 직렬화 하고 역직렬화 하는데 이것은 kafka가 메시지를 바이트 배열 형식으로 처리하기 때문입니다.</p>\n<p>직렬화는 객체를 바이트 배열로 바꾸는거라 생각하면 될 것 같습니다.</p>\n<h4>역직렬화 Deserialization ?</h4>\n<p>바이트 배열을 원래 형식으로 복원하는 과정입니다.</p>\n<p>바이트 배열을 객체로 복원한다고 생각하시면 됩니다.</p>\n<h2>마무리</h2>\n<p>오늘은 record에 대해서 알아보았는데요.</p>\n<p>뭐든 하나하나 쉽게 가는 게 없는 거 같네요 😅</p>","categories":["Kafka","ALL"],"date":"October 28, 2024","description":"Kafka KRaft 소개글입니다.","id":"82e420b7-b6a7-5104-a377-f0e3b383e1f4","keywords":["Pipeline","Kafka"],"slug":"/my-first-article/About-Kafka-Record/","title":" 🚀 [Kafka] About Kafka Record","readingTime":{"text":"5 min read"}},{"banner":{"alt":"Kafka","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka KRaft Cluster</h1>\n<p>오늘은 Kafka의 KRaft Cluster 구성에 대해서 공부했습니다.</p>\n<p>KRaft 모드에서 클러스터로 실행하기 위해 설정을 할 필요가 있는데 무엇인지 왜 인지 알아보겠습니다.</p>\n<h2>컨트롤러와 브러커 역할을 정하기</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/controller.png\" alt=\"Controller\" width=\"600\">\n<p><strong>controller.properties, broker.properties</strong> 에서 <strong>process.roles</strong>에서 <strong>contorller</strong>나 <strong>broker</strong>로 역할을 지정합니다.</p>\n<p>KRaft 모드에서는 각 노드는 고유한 아이디를 가져야 합니다.</p>\n<p><strong>node.id</strong>에서 1,2,3,4 번 등 각 번호를 지정합니다.</p>\n<p><strong>controller.quorum.voters</strong>=1@ip:9093, 2@ip:9093에서는 컨트롤러를 지정합니다.</p>\n<p>지정한 node.id@ip:port 로 구성합니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/broker.png\" alt=\"Broker\" width=\"600\">\n<p>broker도 마찬가지입니다.</p>\n<p><strong>controller.quorum.voters</strong> 는 마찬가지로 컨트롤러로 지정하고 브로커는 역할에 broker로 지정해줍니다.</p>\n<h2>Storage 포맷하기</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/controller.png\" alt=\"Controller\" width=\"600\">\n<p>Kafka KRaft 모드로 실행하기 위해선 클러스터 내의 스토리지를 모두 포맷을 해줘야 합니다.</p>\n<p>그 이유는 클러스터에 참여하는 모든 브로커가 동일한 메타 데이터 구조를 가져야 하기 때문입니다.</p>\n<p>kafka-storage 스크립트를 활용해서 포맷을 할건데 그 전에!</p>\n<p>모든 같은 값으로 포맷을 해주기 위해선 같은 포맷 키 값이 필요하고 randon uuid를 통해 생성해줍니다.</p>\n<p>그리고 그렇게 생성된 키값을 가지고 foramt -t 를 이용하여 값은 키값으로 포맷을 해줘야합니다.</p>\n<p>왜 그러냐 하면 포맷 키는 동일한 클러스터에 속함을 보장하기 때문입니다.</p>\n<p>다른 키 값으로 포맷을 진행 할 시, 같은 클러스터로 인식하지 못합니다.</p>\n<h2>Topic 생성</h2>\n<p>컨트롤러 2개 브로커 2개로 구성을 하셨다면 토픽을 생성하여 데이터를 받을 준비를 하실텐데</p>\n<p>topic은 broker에서 밖에 생성하지 못합니다.</p>\n<p>컨트롤러는 메타데이터를 관리, 브로커 상태 점검 등 다른 역할을 하기에 토픽 생성은 브로커에서만 가능합니다.</p>\n<h2>Linux 자동 실행</h2>\n<p>저는 EC2에서 ubuntu로 실행을 하였는데요. 각각 4개의 서버</p>\n<p>저는 Linux system을 통해 자동실행을 설정했습니다.</p>\n<p>여기서 중요한 점은 자동실행이 시작될 때 컨트롤러가 브로커보다 먼저 실행이 되어야 한다는겁니다.</p>\n<p>클러스터가 시작될 때 브로커가 컨트롤러에게서 메타데이터를 가져와야 하는데</p>\n<p>실행이 같이 된다거나 좀 늦던가 하면 실행이 제대로 되지 않습니다.</p>\n<p>그래서 저는 script를 새로 생성을 했습니다.</p>\n<p>start-broker-after-controller.sh를 생성했는데요</p>\n<p>내용은 간단합니다.</p>\n<p>\"controller가 실행되어 있는 걸 확인 후에 broker.properties를 실행하라고 했습니다.\"</p>\n<p>그리고 이제 service 파일로 생성을 한 후 각각 자동실행이 되도록 설정하면 되는데요</p>\n<p>브로커에서만 컨트롤러에 의존성이 있기 때문에 브로커만 제가 만든 sh 파일로 실행을 하면 됩니다.</p>\n<h2>느낀점</h2>\n<p>KRaft 모드를 실행할 때 많은 옵션의 조정이 필요 한 건 아니지만, 이해를 제대로 하고 하는 것과</p>\n<p>그냥 docker-compose up -d를 해서 무지성으로 하는 것과 많이 다른 걸 느꼈습니다.</p>\n<p>각각의 properties에 있는 옵션들도 모두 잘 확인할 필요가 있다고 느낍니다.</p>\n<p>특히 로그 파일 쌓이는 위치를 바꿔서 실제 메시지 데이터를 누가 만들고 관리하는지,</p>\n<p>운영 데이터도 마찬가지로 위치를 지정해서 확인하고 관리 하는 것도 필요할 거 같습니다.</p>\n<p>사실 글이 너무 두서 없는데요, 정확한 정보는 찾아보시길 추천드립니다.</p>","categories":["Kafka","ALL"],"date":"October 27, 2024","description":"Kafka KRaft 소개글입니다.","id":"bfd646fa-b8ab-51e9-b889-8c868cc76f33","keywords":["Pipeline","Kafka","KRaft"],"slug":"/my-first-article/About-Kafka-KRaft-Cluster/","title":" 🚀 [Kafka] About Kafka KRaft-Cluster ","readingTime":{"text":"6 min read"}},{"banner":{"alt":"Kafka","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka Topic &#x26; Partition</h1>\n<h2>🔎 Overview</h2>\n<p>카프카의 데이터를 구분하기 위한 단위 Topic과 Partition에 대해여.</p>\n<h2>토픽이란 ?</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/topic-partition.png\" alt=\"topic\" width=\"600\">\n<p><strong>Topic</strong>은 카프카에서 데이터를 구분하기 위한 단위입니다.</p>\n<p>토픽은 한 개 이상의 파티션을 소유하고 있습니다.</p>\n<p>파티션에는 프로듀서가 보낸 데이터들이 저장됩니다.</p>\n<p>이 데이터를 <strong>레코드 (Record)</strong> 라고 부릅니다.</p>\n<p>파티션은 Queue와 비슷한 구조라고 생각을 하면 이해하기 편합니다.</p>\n<p>FIFO 구조로 먼저 들어간 레코드는 컨슈머가 먼저 가져가게 됩니다.</p>\n<p>하지만 Queue는 데이터를 pop() 하며 삭제를 하지만 <strong>카프카는 삭제 하지 않습니다</strong>.</p>\n<p>파티션의 레코드는 컨슈머가 데이터를 가져가는 것과 별개로 관리되어, 여러 컨슈머 그룹이 <strong>토픽의 데이터를 여러 번 가져갈 수 있습니다</strong>.</p>\n<hr>\n<h2>토픽 생성 규칙</h2>\n<p>이름의 생성 규칙이 있는데요</p>\n<p>모호하게 작성을 하지 않는 게 중요합니다.</p>\n<p>토픽의 이름의 변경이 불가능 하기에 미리 규칙에 따라 생성하는 것을 추천합니다.</p>\n<p>&#x3C;환경>&#x3C;팀-명>&#x3C;어플리케이션 이름>&#x3C;메시지-타입></p>\n<p>&#x3C;프로젝트명>&#x3C;서비스명>&#x3C;환경>&#x3C;이벤트명></p>\n<p>&#x3C;카프카-클러스터명>&#x3C;환경>&#x3C;서비스명>&#x3C;메시지-타입></p>\n<p>ex) dev/marketing1-team.website-platform.json</p>\n<p>이런식으로 작성하면 더 편리하게 관리 할 수 있습니다.</p>\n<h2>리더와 팔로우 파티션?</h2>\n<h2>특정 브로커에 파티션 쏠림 현상</h2>\n<p>특정 브로커에 리더 파티션 쏠림 현상이 생길 수 있습니다. (클러스터 운영에서 중요한 부분)</p>\n<p>이럴 때 kafka-reassign-partition.sh 명령으로 파티션 재분배가 가능합니다.</p>\n<h2>파티션은 컨슈머와 1:1 관계</h2>\n<p>컨슈머와 파티션은 1:1 관계이기에 파티션 늘리면 컨슈머도 비례 해서 늘어나야 합니다.</p>\n<p>물론 장애 발생 시 컨슈머가 1,2번 파티션을 담당할 수 있습니다.</p>\n<p>컨슈머 + 파티션 늘리면 처리량이 늘어납니다.</p>\n<h2>파티션 개수를 줄이는 건 지원하지 않는다.</h2>\n<p>한 번 늘리면 줄이는 것은 불가합니다.</p>\n<p>데이터의 순서와 무결성을 보장하기 힘들어지기 때문입니다.</p>\n<p>토픽을 삭제하고 재성성하는 방법 밖에 없습니다. (마이그레이션 방법도 있긴 합니다.)</p>\n<p>파티션 개수를 늘릴 땐 신중하게 고민하고 실행합시다.</p>","categories":["Kafka","ALL"],"date":"October 26, 2024","description":"Kafka KRaft 소개글입니다.","id":"4dcc4a6c-35c0-5571-b14f-93d5ba136fd8","keywords":["Pipeline","Kafka"],"slug":"/my-first-article/About-Kafka-Topic-Partition/","title":" 🚀 [Kafka] About Kafka Topic & Partition","readingTime":{"text":"4 min read"}},{"banner":{"alt":"Kafka","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka KRaft</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/kafka.png\" alt=\"Kafka\"></p>\n<p>오늘은 Kafka의 KRaft 모드에 대해서 공부했습니다.</p>\n<p>KRaft 모드와 장단점을 알아봅시다!</p>\n<h2>Zookeeper With Kafka</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/zookeeper.png\" alt=\"Total\" width=\"600\">\n<p>Kafka는 Zookeeper라는 관리 도구를 사용합니다.</p>\n<p>Zookeeper는 Kafka의 메타데이터, 브로커 상태, 토픽, 컨트롤러 등을 관리합니다.</p>\n<p>어떤 브로커가 특정 파티션의 리더인지 결졍 및 수행을 하고,</p>\n<p>토픽 및 권한에 대한 구성을 저장합니다.</p>\n<p>다양한 변동 사항이 있을 시, 이를 카프카에게 보고합니다.</p>\n<p>하지만 결국 Kafka 자체가 아닌 외부에서 메타데이터를 관리하여 확장성에 제한이 있고</p>\n<p>데이터 중복, 브로커의 메타데이터와 Zookeeper의 메타데이터의 불일치로 인한 데이터 정합성에 문제가 생기고</p>\n<p>서버/시스템의 추가 및 복잡성이 증가하는 한계점이 있습니다.</p>\n<p>또한 관리적인 부분에서도 둘은 서로 다른 어플리케이션이므로, 서로 다른 환경, 파일 등을 가지고 있어 동시에 운영을 해야합니다.</p>\n<p>이 한계점을 보완하는게 KRaft 모드입니다.</p>\n<h2>KRaft : Without Zookeeper</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/kraft.png\" alt=\"Total\" width=\"600\">\n<p>KRaft 모드는 즉 Zookeeper가 관리를 하는 게 아닌 Kafka 자체가 관리를 할 수 있게 해줍니다.</p>\n<p>즉 Zookeeper의 의존성을 제거합니다.</p>\n<p>단순화가 되어 확장성, 안정성, 일관성에도 도움이 됩니다.</p>\n<p>이는 운영을 좀 더 쉽게 만들어 줄 수 있습니다.</p>\n<p>Zookeeper가 관리하던 메타데이터를 카스카 클러스터 내의 브로커 중 컨트롤러를 선출한 뒤 별도의 토픽으로 메타데이터를 관리하게 됩니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/kraft-detail.png\" alt=\"Total\" width=\"600\">\n<p>컨트롤러가 늘어나고 이들 중 리더 컨트롤러가 액티브 컨트롤러이면서 리더 역할을 담당합니다.</p>\n<p>리더 역할을 하는 컨트롤러는 write 역할도 맡아 수행합니다.</p>\n<p>이 처럼 자체에서 관리를 함으로 간편한 운영과 확장성이 있습니다.</p>\n<p>KRaft 모드에서 주요한 성능 개선 중 하는 바로 파티션의 리더 선출 최적화입니다.</p>\n<p>컨트롤러의 주요 역할이 파티션의 리더를 선출하는 것인데 대량의 파티션에 대한 작업 시간은 다소 소요되게 됩니다.</p>\n<p>이는 데이터 파이프라인에 좋지 않은 요소로 작용할 수 있습니다.</p>\n<p>Zookeeper에서는 이 때문에 클러스터의 전체 파티션 수를 200,000 정도로 제한 했으나</p>\n<p>KRaft에서는 더 많은 파티션을 만들 수 있습니다.</p>\n<p>또 다른 장점은 복구에 필요한 시간입니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/recovery.png\" alt=\"Total\" width=\"600\">\n<p>위에 복구 시간 소요를 비교해놓은 자료입니다.</p>\n<p>복구 시간의 차이가 엄청 나는데 이는 KRaft 모드에서는 컨트롤러가 메모리 내에 메타데이터 캐시를 유지하고 있고,</p>\n<p>메타데이터 동기화와 관리 과정을 효울적으로 개선했기 때문입니다.</p>\n<p><a href=\"https://devocean.sk.com/blog/techBoardDetail.do?ID=165711&#x26;boardType=techBlog\">참고</a></p>","categories":["Kafka","ALL"],"date":"October 23, 2024","description":"Kafka 간단한 소개글입니다.","id":"ea998534-eb77-5d38-ad9e-418365708b0b","keywords":["Pipeline","Kafka","Kraft"],"slug":"/my-first-article/About-Kafka-Kraft/","title":" 🚀 [Kafka] About Kafka Kraft ","readingTime":{"text":"5 min read"}},{"banner":{"alt":"Kafka","caption":null,"src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka Controller</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/kafka.png\" alt=\"Kafka\"></p>\n<p>오늘은 Controller에 대해 알아보겠습니다.</p>\n<p>zookeeper로 관리하는 버전이 아닌, KRaft mode로 설명하겠습니다.</p>\n<p>KRaft mode에 대해선 다음 포스팅에 작성하겠습니다.</p>\n<h2>Controller는 무엇인가?</h2>\n<h3>리더 선출, 브로커 장애 감지 및 복구, 파티션 재배치, 메타 데이터 정보 교환</h3>\n<ul>\n<li>\n<p>클러스터에서 하나의 브로커가 컨트롤러 역할을 수행.</p>\n</li>\n<li>\n<p>컨트롤러는 나머지 브러커(팔로워)의 상태를 주기적으로 체크.</p>\n</li>\n<li>\n<p>죽은 브로커가 발생시, 담당하던 파티션의 새리더를 선출.</p>\n</li>\n<li>\n<p>새롭게 선출된 리더에 정보를 교환.</p>\n</li>\n<li>\n<p>메타데이터 정보 교환 (kraft)</p>\n</li>\n</ul>\n<h3>파티션 리더 선출?</h3>\n<p>카프카의 각 파티션에는 리더와 팔로워가 존재합니다.</p>\n<p>리더는 클라이언트의 읽기/쓰기 요청을 처리하고, 팔로워는 리더를 복제하여 리더가 중단, 변경 시 새로운 리더를 선출합니다.</p>\n<h3>브로커 추가 및 제거 감지</h3>\n<p>새로운 브로커가 클러스터에서 제거 및 추가 시, 감지하여 클러스터 상태를 업데이트 합니다 (리밸런싱)</p>\n<h3>클러스터 메터데이터 관리</h3>\n<p>실제 메시지 로그 아닌 브로커의 모든 상태를 기록하는 로그 데이터 관리를 합니다.</p>\n<p>여기서 메타데이터는 무엇일까요?</p>\n<p>카프카 클러스터를 운영하는데 있어 중요한 역할을 하는 로그로,</p>\n<ol>\n<li>브로커 정보</li>\n</ol>\n<ul>\n<li>각 브로커의 id,주소,포트 등</li>\n</ul>\n<ol start=\"2\">\n<li>파티션 및 토픽 정보</li>\n</ol>\n<ul>\n<li>각 토픽의 파티션 수, 레클리카 목록, 누가 리더인지.</li>\n</ul>\n<ol start=\"3\">\n<li>리더 - 팔로워 정보</li>\n</ol>\n<ul>\n<li>리더 브로커와 팔로워 브로커에 대한 정보.</li>\n</ul>\n<ol start=\"4\">\n<li>레플리카 상태</li>\n</ol>\n<ul>\n<li>각 파티션 레플리카가 어느 브로커에 있는지 및 레클리카가 최신 상태인지</li>\n</ul>\n<ol start=\"5\">\n<li>클러스터 현재 상태 정보</li>\n</ol>\n<ul>\n<li>컨트롤러 역할을 하고 있는 브로커에 대한 정보</li>\n</ul>","categories":["Kafka","ALL"],"date":"October 21, 2024","description":"Kafka 간단한 소개글입니다.","id":"99154462-45ab-5044-9a18-7c657e71b25a","keywords":["Pipeline","Kafka","Controller","Broker"],"slug":"/my-first-article/About-Kafka-deep-1/","title":" 🚀 [Kafka] About Kafka Controller ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Inverted Index","caption":"Photo by <u><a href=\"https://esbook.kimjmin.net/06-text-analysis/6.1-indexing-data\">Inverted Index Guide</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAAA20lEQVR42sWSPQuDQAyG+/9xcBA3RRBFBMHBj0UFHUVxEBxd/BMO6lsSiNQiRWilgeO45PLkveQe+LE9bgeu67qvZVn2fds2jouPzrLfr5AqkTVNgyAIUNc1yrJEHMfwfR9VVcF1XfYVRYE0TWGaJvI8xziOnCtKD8C2beF5HqIogmVZDKRk27a5EBVMkgSO40DXdRiGgb7v8co4fbIEh2HgxCzLWFEYhgcAqfrYQ7lAjSebpgmKorBKTdOgqiq6rtuH8w67NBRRQoB5nk8hl4Gi+Ox7/O9jfwt8AqvpmQ0/tt9oAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/07234f2e857a04efb6593044576677f0/bc51f/inverted-cover.png","srcSet":"/static/07234f2e857a04efb6593044576677f0/41200/inverted-cover.png 165w,\n/static/07234f2e857a04efb6593044576677f0/f979a/inverted-cover.png 330w,\n/static/07234f2e857a04efb6593044576677f0/bc51f/inverted-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/07234f2e857a04efb6593044576677f0/322ad/inverted-cover.webp 165w,\n/static/07234f2e857a04efb6593044576677f0/de3b3/inverted-cover.webp 330w,\n/static/07234f2e857a04efb6593044576677f0/2b2b5/inverted-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<p>안녕하세요! 오늘은 역색인 (Inverted Index)에 대해서 알아보겠습니다.</p>\n<p>Elasticsearch 는 역색인 구조로 알려져 있는데요, 어떤 장점이 있는걸까요?</p>\n<p>이제부터 한 번 알아보겠습니다.</p>\n<h1>역색인 (Inverted Index) 이란?</h1>\n<p>문서 집합 내의 단어와 그 단어가 포함된 문서들의 목록을 맵핑 하는 방식입니다.</p>\n<p>이런식으로 말을 하니까 뭔가 어렵게 느껴지는데 <a href=\"https://esbook.kimjmin.net/06-text-analysis/6.1-indexing-data\">ElasticGuide</a>에 나온 설명을 한번 봅시다.</p>\n<p>일반 RDBMS에서는</p>\n<table>\n<thead>\n<tr>\n<th><strong>ID</strong></th>\n<th><strong>Text</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>doc1</td>\n<td>The quick brown fox</td>\n</tr>\n<tr>\n<td>doc2</td>\n<td>The quick brown fox jumps over the lazy dog</td>\n</tr>\n<tr>\n<td>doc3</td>\n<td>The quick brown fox jumps over the quick dog</td>\n</tr>\n<tr>\n<td>doc4</td>\n<td>Brown fox brown dog</td>\n</tr>\n<tr>\n<td>doc5</td>\n<td>Lazy jumping dog</td>\n</tr>\n</tbody>\n</table>\n<p>이런 테이블에 Text에 'fox'를 찾는다고 가정하면 Text 열을 한 줄씩 내려가면서 데이터 값을 가져올텐데요.</p>\n<table>\n<thead>\n<tr>\n<th><strong>ID</strong></th>\n<th><strong>Text</strong></th>\n<th><strong>결과</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>doc1</td>\n<td>The quick brown <strong>fox</strong></td>\n<td>fox (O) → 선택</td>\n</tr>\n<tr>\n<td>doc2</td>\n<td>The quick brown <strong>fox</strong> jumps over the lazy dog</td>\n<td>fox (O) → 선택</td>\n</tr>\n<tr>\n<td>doc3</td>\n<td>The quick brown <strong>fox</strong> jumps over the quick dog</td>\n<td>fox (O) → 선택</td>\n</tr>\n<tr>\n<td>doc4</td>\n<td>Brown <strong>fox</strong> brown dog</td>\n<td>fox (O) → 선택</td>\n</tr>\n<tr>\n<td>doc5</td>\n<td>Lazy jumping dog</td>\n<td>fox (X) → 제외</td>\n</tr>\n</tbody>\n</table>\n<p>row 안에 데이터를 모두 읽어야 하고 'like'연산을 수행하기에 시간이 오래걸리고, 속도도 느립니다.</p>\n<p>그렇다면 역색인 구조는 어떨까요?</p>\n<table>\n<thead>\n<tr>\n<th><strong>텀(Term)</strong></th>\n<th><strong>ID</strong></th>\n<th><strong>텀(Term)</strong></th>\n<th><strong>ID</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>The</td>\n<td>doc1, doc2, doc3</td>\n<td>quick</td>\n<td>doc1, doc2, doc3</td>\n</tr>\n<tr>\n<td>brown</td>\n<td>doc1, doc2, doc3, doc4</td>\n<td>fox</td>\n<td>doc1, doc2, doc3, doc4</td>\n</tr>\n<tr>\n<td>jumps</td>\n<td>doc2, doc3</td>\n<td>over</td>\n<td>doc2, doc3</td>\n</tr>\n<tr>\n<td>the</td>\n<td>doc2, doc3</td>\n<td>lazy</td>\n<td>doc2</td>\n</tr>\n<tr>\n<td>dog</td>\n<td>doc2, doc3, doc4, doc5</td>\n<td>Brown</td>\n<td>doc4</td>\n</tr>\n<tr>\n<td>Lazy</td>\n<td>doc5</td>\n<td>jumping</td>\n<td>doc5</td>\n</tr>\n</tbody>\n</table>\n<p>이렇게 저장한 게 역색인 구조인데요.</p>\n<p>역인덱스는 보통 <strong>찾아보기 페이지</strong>에 많이 비유를 하는 거 같습니다.</p>\n<p>위에 표에 보이는바와 같이 추출된 키워드 (토큰화와 정규화를 거칩니다.) 를 <strong>텀(Term)</strong> 이라고 하고,</p>\n<p>이를 이용해서 특정 키워드를 포함하고 있는 doc Id를 바로 얻을 수 있습니다.</p>\n<p>또한 역색인은 Id 값을 리스트 (역색인 리스트) 형태로 저장하기에 큰 속도의 저하 없이 빠르게 검색이 가능합니다.</p>\n<p>Term, 즉 단어가 Key가 되고 문서의 식별자 doc Id가 Value가 되는 것 입니다.</p>\n<h2>장점과 단점</h2>\n<h3>장점</h3>\n<ul>\n<li>\n<p>빠른 검색 속도</p>\n</li>\n<li>\n<p>효율적인 공간의 사용으로 대규모 문서 집합에서도 효율적입니다.</p>\n</li>\n<li>\n<p>문서가 추가 혹은 삭제 할 때 역색인 리스트를 업데이트 하기 용이합니다.</p>\n</li>\n</ul>\n<h3>단점</h3>\n<ul>\n<li>\n<p>초기 색인 생성 비용</p>\n</li>\n<li>\n<p>실시간 업데이트가 힘듬</p>\n</li>\n<li>\n<p>색인관리의 복잡함</p>\n</li>\n</ul>\n<hr>\n<h2>느낀점</h2>\n<p>오늘은 Elasticsearch에 역색인에 대해 알아보았는데, Elasticsearch에 대해서 잘 몰랐던 거 같습니다.</p>\n<p>역색인 구조라는 걸 알면서 역색인에 대해 깊이 모르고 있었네요..</p>\n<p>열심히 공부하겠습니다.</p>","categories":["ELK","Elasticsearch","ALL"],"date":"October 14, 2024","description":"역색인에 대하여","id":"eb6bdacd-73dc-5402-ba71-0b60e6c1aa00","keywords":["Pipeline","Elasticsearch","Inverted Index","역색인"],"slug":"/my-first-article/About-Elasticsearch-InvertedIndex/","title":" ⚙️ [ELK] Inverted Index ? ","readingTime":{"text":"4 min read"}},{"banner":{"alt":"Data structure","caption":"Photo by <u><a href=\"https://geundung.dev/111\">Image 출처</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABf0lEQVR42qWSu45BURSGvQrPQE8nGq1WiOJEgZNoFBK3RlSa0yAkCpGTSDQikSChEBLtuUwYjEsYzzCfs11m2vE3e93+tf+19rZ9vAGbOEzT1HXdfEC3YBjG08bAfdovMs5mszmfz9vt9tPC8Xg8nU4EiVwuF1LY+/2eIC624Ns4qJ5MJoqiDIfD6XQ6m80ajUatVsMdjUblcpkUBZ1Op16vY1MDBeKNvFqt5vO53+/P5XKpVApmIBDweDySJC0WC6rD4XCxWBwMBm632+Fw0EhcfpPNGIgJhUL0jkajyWTS6/UGg0HupOlyuUyn05BpKstyLBZDDuPcyKxht9uh0G635/P5TCajqqrL5XI6nYVCodls+nw+RFUqlW63G4lECI7H4/V6fZdNm16vV61WKW21WgwWj8cTiUSpVLper/1+nyFZYTabZRCktdvtl2yx7cPh8PXAtwUImqaR4h4Esmp6EUfpfdvPdzZ+QbPALkiJ1xarEUEifz7JWz/sf+Qf8qMq2lMz+l0AAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/af3fdee77b76877d42b072e4f301464f/b87af/structure-korean.png","srcSet":"/static/af3fdee77b76877d42b072e4f301464f/ea049/structure-korean.png 125w,\n/static/af3fdee77b76877d42b072e4f301464f/bf58b/structure-korean.png 250w,\n/static/af3fdee77b76877d42b072e4f301464f/b87af/structure-korean.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/af3fdee77b76877d42b072e4f301464f/df328/structure-korean.webp 125w,\n/static/af3fdee77b76877d42b072e4f301464f/2d2c6/structure-korean.webp 250w,\n/static/af3fdee77b76877d42b072e4f301464f/01f4a/structure-korean.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":660,"height":399.96}}}},"body":"<h1>Array &#x26; Linked List 에 대해서</h1>\n<p>오늘도 선형 자료 구조에 대해서 알아보는데요, 오늘은 그 중 Array, Linked List에 대해 공부해보겠습니다.</p>\n<p>이 두 가지 자료구조는 데이터를 저장하고 관리하는 방식에서의 차이가 있고, 각각의 장단점이 있습니다.</p>\n<h2>Array (배열)</h2>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/data-structure/array.png\" alt=\"Array\"></p>\n<p><strong>배열</strong>은 동일한 데이터 타입의 요소들의 <strong>연속된 메모리 공간</strong>에 저장되는 선형 자료 구조입니다.</p>\n<p>각 요소는 <strong>index</strong>를 통해서 접근이 가능합니다.</p>\n<h3>특징</h3>\n<ol>\n<li>\n<p><strong>인덱스 기반의 접근</strong> : 인덱스 접근이 가능하다.</p>\n</li>\n<li>\n<p><strong>고정 크기</strong> : 선언시 크기가 정해지는 고정 크기이며, 이 후 변경이 어렵다.</p>\n</li>\n<li>\n<p><strong>효율성</strong> : 연속된 메모리 공간을 사용하여 효율이 높습니다.</p>\n</li>\n<li>\n<p><strong>데이터 타입</strong> : 배열 내 모든 요소는 동일한 데이터 타입을 갖습니다.</p>\n</li>\n</ol>\n<h3>구현 방식</h3>\n<h4>정적 방식</h4>\n<ul>\n<li>\n<p>고정된 크기를 가지며, 선언 시 크기를 결정합니다.</p>\n</li>\n<li>\n<p>크기 변경이 필요할 겨웅 새로운 배열을 생성해야 합니다.</p>\n</li>\n</ul>\n<h4>동적 방식</h4>\n<ul>\n<li>\n<p>크기가 동적으로 조절할 수 있습니다.</p>\n</li>\n<li>\n<p>Python - list, Java  - ArrayList</p>\n</li>\n</ul>\n<h3>배열의 장단점</h3>\n<h4>장점</h4>\n<ul>\n<li>\n<p>인덱스를 사용하여 빠른 요소에 접근이 가능합니다.</p>\n</li>\n<li>\n<p>연속된 메모리 공간을 사용함으로 효율적입니다.</p>\n</li>\n</ul>\n<h4>단점</h4>\n<ul>\n<li>\n<p>고정된 크기를 가지고 있습니다.</p>\n</li>\n<li>\n<p><strong>배열의 중간의 요소를 삽입하거나 삭제할 때 비효율적으로 시간이 소요됩니다</strong>.</p>\n</li>\n<li>\n<p>미리 크기를 정할 경우 남는 공간으로 인해 사용하지 않는 공간이 발생할 수 있습니다.</p>\n</li>\n</ul>\n<hr>\n<h2>Linked List (연결리스트)</h2>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/data-structure/all.png\" alt=\"Linked List\"></p>\n<p><strong>연결리스트</strong>는 노드들의 연속된 연결을 통해 데이터를 저장하는 선형 자료 구조입니다.</p>\n<p>각 노드는 데이터와 다음 노드를 가리키는 <strong>포인터</strong>로 구성됩니다.</p>\n<h3>특징</h3>\n<ol>\n<li>\n<p>노드의 삽입과 삭제가 용이해서 크기를 조절할 수 있습니다.</p>\n</li>\n<li>\n<p>인덱스를 이용한 접근이 불가능하며, 노드를 순차적으로 접근해야 합니다.</p>\n</li>\n<li>\n<p>사용한만큼 메모리를 사용해서 낭비가 적습니다.</p>\n</li>\n<li>\n<p>각 노드는 포인터를 포함하여 연결됩니다.</p>\n</li>\n</ol>\n<h3>종류</h3>\n<h4>단일 연결 리스트</h4>\n<p>각 노드가 다음 노드만을 가리키는 포인터를 가지는 연결 리스트입니다.</p>\n<p>리스트의 끝은 None이기에 연결리스트의 끝을 알 수 있습니다.</p>\n<ul>\n<li>\n<p>단방향 탐색만 가능하다.</p>\n</li>\n<li>\n<p>메모리 사용이 효율적이다.</p>\n</li>\n</ul>\n<h4>이중 연결 리스트</h4>\n<p>각 노드가 이전과 그 다음 노드를 가리키는 2개의 포인터를 가지는 연결 리스트입니다.</p>\n<p>양방향 탐색이 가능합니다.</p>\n<ul>\n<li>\n<p>양방향 탐색이 가능하다.</p>\n</li>\n<li>\n<p>삽입과 삭제가 보다 유연하다.</p>\n</li>\n<li>\n<p>단방향보다 메모리를 더 많이 사용한다.</p>\n</li>\n</ul>\n<h4>원형 연결 리스트</h4>\n<p>마지막 노드가 첫 번째 노드를 가리키는 연결 리스트입니다.</p>\n<ul>\n<li>\n<p>순환 탐색이 가능합니다.</p>\n</li>\n<li>\n<p>무한하게 순회할 수 있습니다.</p>\n</li>\n</ul>\n<h3>연결리스트의 장단점</h3>\n<h4>장점</h4>\n<ul>\n<li>\n<p>노드의 삭제와 삽입이 용이해 사이즈 조절이 가능합니다.</p>\n</li>\n<li>\n<p>삽입과 삭제시 포인터만 변경하면 되므로 보다 효율적입니다.</p>\n</li>\n</ul>\n<h4>단점</h4>\n<ul>\n<li>\n<p>인덱스로 인한 접근이 불가능해서 요소에 접근 속도가 느립니다.</p>\n</li>\n<li>\n<p>구현이 복잡하고 포인터 정보를 저장하므로 메모리 사용량이 보다 많습니다.</p>\n</li>\n</ul>\n<hr>\n<h1>결론</h1>\n<p>고정된 크기에 빠른 접근을 구현한다면 배열을 사용하고, 삽입과 삭제가 자주 발생하는 데이터라면 연결리스트를 사용하는게 적합해 보입니다.</p>\n<p>자료구조는 공부하면 할 수록 어렵기도 하지만 재밌는 내용도 많아서 좋아요 👍</p>","categories":["Data structure","ALL"],"date":"October 13, 2024","description":"Array & Linked List 에 대한 것","id":"82e5a5d1-b681-5751-996a-5acda66f20af","keywords":["Data structure","Array","Linked List"],"slug":"/my-first-article/About-Algorhythm-Array-Linked-List/","title":" 📚 [Data structure] Array & Linked List","readingTime":{"text":"6 min read"}},{"banner":{"alt":"Data structure","caption":"Photo by <u><a href=\"https://geundung.dev/111\">Image 출처</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABf0lEQVR42qWSu45BURSGvQrPQE8nGq1WiOJEgZNoFBK3RlSa0yAkCpGTSDQikSChEBLtuUwYjEsYzzCfs11m2vE3e93+tf+19rZ9vAGbOEzT1HXdfEC3YBjG08bAfdovMs5mszmfz9vt9tPC8Xg8nU4EiVwuF1LY+/2eIC624Ns4qJ5MJoqiDIfD6XQ6m80ajUatVsMdjUblcpkUBZ1Op16vY1MDBeKNvFqt5vO53+/P5XKpVApmIBDweDySJC0WC6rD4XCxWBwMBm632+Fw0EhcfpPNGIgJhUL0jkajyWTS6/UGg0HupOlyuUyn05BpKstyLBZDDuPcyKxht9uh0G635/P5TCajqqrL5XI6nYVCodls+nw+RFUqlW63G4lECI7H4/V6fZdNm16vV61WKW21WgwWj8cTiUSpVLper/1+nyFZYTabZRCktdvtl2yx7cPh8PXAtwUImqaR4h4Esmp6EUfpfdvPdzZ+QbPALkiJ1xarEUEifz7JWz/sf+Qf8qMq2lMz+l0AAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/af3fdee77b76877d42b072e4f301464f/b87af/structure-korean.png","srcSet":"/static/af3fdee77b76877d42b072e4f301464f/ea049/structure-korean.png 125w,\n/static/af3fdee77b76877d42b072e4f301464f/bf58b/structure-korean.png 250w,\n/static/af3fdee77b76877d42b072e4f301464f/b87af/structure-korean.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/af3fdee77b76877d42b072e4f301464f/df328/structure-korean.webp 125w,\n/static/af3fdee77b76877d42b072e4f301464f/2d2c6/structure-korean.webp 250w,\n/static/af3fdee77b76877d42b072e4f301464f/01f4a/structure-korean.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":660,"height":399.96}}}},"body":"<h1>Stack 과 Quere에 대하여</h1>\n<h2>Stack</h2>\n<h3>정의 및 특성</h3>\n<p>스택은 후입선출(LIFO, Last-In-First-Out) 방식으로 동작하는 선형 자료구조입니다.</p>\n<p>이는 마지막에 삽입된 데이터가 가장 먼저 제거된다는 의미입니다.</p>\n<p>쉽게 생각해서 책을 쌓는 방식과 유사합니다. 새로운 책을 쌓을 때는 가장 위에 올리고, 책을 꺼낼 때도 가장 위에 있는 책부터 꺼냅니다.</p>\n<h2>주요 연산</h2>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/data-structure/stack.png\" alt=\"Stack\"></p>\n<p>Push : 스택의 최상단에 요소 추가</p>\n<p>Pop  : 스택의 최상단에서 요소 제거</p>\n<p>Peak : 스택의 최상단 요소 반환 (제거 하지 않음)</p>\n<p>IsEmpty : 스택이 비어있는지 확인</p>\n<hr>\n<h2>Queue</h2>\n<h3>정의 및 특성</h3>\n<p>큐는 선입선출(FIFO, First-In-First-Out) 방식으로 동작하는 선형 자료구조입니다.</p>\n<p>이는 먼저 삽입된 데이터가 먼저 제거된다는 의미입니다.</p>\n<p>쉽게 생각해서 줄을 서는 방식과 유사하다고 생각하면 됩니다.</p>\n<p>먼저 줄을 선 사람이 먼저 서비스를 받습니다.</p>\n<h2>주요 연산</h2>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/data-structure/queue.png\" alt=\"Queue\"></p>\n<p>Enqueue : 큐의 끝에 요소 추가</p>\n<p>Dequeue : 큐의 앞에서 요소 제거</p>\n<p>Front : 큐의 앞 요소 반환</p>\n<p>Rear : 큐의 뒤 요소 반환</p>\n<p>IsEmpty : 큐가 비어있는지 확인</p>\n<h2>변형 및 응용</h2>\n<ul>\n<li>\n<p>우선순위 큐(Priority Queue): 각 요소에 우선순위를 부여하여 높은 우선순위를 가진 요소가 먼저 제거되는 큐.</p>\n</li>\n<li>\n<p>덱(Deque: Double-Ended Queue): 양쪽 끝에서 요소를 추가하거나 제거할 수 있는 큐.</p>\n</li>\n<li>\n<p>원형 큐(Circular Queue): 배열을 사용한 큐에서 공간을 효율적으로 활용하기 위해 원형으로 연결된 큐.</p>\n</li>\n</ul>\n<hr>\n<h2>알고리즘에서의 사용</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">•\t스택\n\n    •\t깊이 우선 탐색(DFS, Depth-First Search): 그래프나 트리의 탐색에서 경로를 추적하는 데 사용.\n\n    •\t재귀 알고리즘의 구현: 함수 호출을 추적하고 관리.\n\n    •\t백트래킹 알고리즘: 가능한 모든 해를 탐색하는 알고리즘에서 상태를 추적.\n\n•   큐\n\n    •\t너비 우선 탐색(BFS, Breadth-First Search): 그래프나 트리의 탐색에서 레벨별로 탐색.\n\n    •\t다익스트라 알고리즘(Dijkstra’s Algorithm): 최단 경로를 찾는 알고리즘에서 우선순위 큐 사용.\n\n    •\t위상 정렬(Topological Sorting): 방향성 비순환 그래프에서의 노드 정렬.</code></pre></div>\n<hr>\n<p>오늘은 자료구조에서 기본이라고 할 수 있는 두 구조를 공부해보았습니다.</p>\n<p>자료구조와 알고리즘도 확실히 공부해서 더 좋은 코드를 짤 수 있는 그 날까지 열심히 하겠습니다.😀</p>","categories":["Data structure","ALL"],"date":"October 11, 2024","description":"stack과 queue에 대한 것","id":"26e604c7-7f69-50a0-9dd1-e10c51489611","keywords":["Data structure","Stack","Queue"],"slug":"/my-first-article/About-Algorhythm-Stack-Queue/","title":" 📚 [Data structure] Stack and Queue","readingTime":{"text":"4 min read"}},{"banner":{"alt":"ELK","caption":"Photo by <u><a href=\"https://www.elastic.co/kr/elasticsearch\">Elasticsearch</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNklEQVR42mP4jwf8+/cfL2DAoxOo9evrcx8vZbw/4/XhcsqPd4chEoQ0//sLJH6+OfNwrembbTK/Dwv93Mvx8ZDyry834LL4bP7z9//JQ2v3bpx0cPu8Q9f3Hb579N37q0Dhf/id/fcvyOCnT58oKijz8AoI8vKzNe1hWPA25+S7/6i6GXB49v+79++z0lJTI8NTo8JDVp9x2/96w4NPIKP/EQqwv2D96x7ccd25IeXYvs5rZ7uunpp88/zz71+RQwy7Zoj0y69fHDevEls522D7Kt2tK9JP7n/zgwjNyODu1097Xj69/vnjX2LiGeLnly9fPn327NGjx6+eP//6/sOH129ev34NFAECwpovX7585vSZE8dPnL9w4fLVKydPnTp37tzFixdv3br1n3hn/wMDZHPhmgFkOpTvrayIEwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/46705467a85bac2f6889cf13b3d56dc8/bc51f/elasticsearch-cover.png","srcSet":"/static/46705467a85bac2f6889cf13b3d56dc8/41200/elasticsearch-cover.png 165w,\n/static/46705467a85bac2f6889cf13b3d56dc8/f979a/elasticsearch-cover.png 330w,\n/static/46705467a85bac2f6889cf13b3d56dc8/bc51f/elasticsearch-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/46705467a85bac2f6889cf13b3d56dc8/322ad/elasticsearch-cover.webp 165w,\n/static/46705467a85bac2f6889cf13b3d56dc8/de3b3/elasticsearch-cover.webp 330w,\n/static/46705467a85bac2f6889cf13b3d56dc8/2b2b5/elasticsearch-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>⚒️ Elasticsearch 란?</h1>\n<p>Elasticsearch는 실시간 분산 검색 및 분석 엔진으로, 대량의 데이터를 빠르고 효율적으로 검색하고 분석할 수 있게 해줍니다.</p>\n<h2>📌 주요 특징</h2>\n<ul>\n<li><strong>실시간 검색</strong>: 데이터가 인덱싱되면 거의 즉시 검색이 가능합니다.</li>\n<li><strong>분산 구조</strong>: 클러스터링을 통해 대규모 데이터를 효율적으로 처리합니다.</li>\n<li><strong>스케일 아웃</strong>: 필요에 따라 노드를 추가하여 용량과 성능을 손쉽게 확장할 수 있습니다.</li>\n<li><strong>다양한 데이터 타입 지원</strong>: 텍스트, 숫자, 위치 정보 등 다양한 데이터 타입을 처리할 수 있습니다.</li>\n<li><strong>RESTful API</strong>: HTTP를 통해 간편하게 접근하고 조작할 수 있습니다.</li>\n<li><strong>강력한 쿼리 DSL</strong>: 복잡한 검색 및 분석 쿼리를 작성할 수 있는 도메인 특화 언어을 제공합니다.</li>\n</ul>\n<hr>\n<h2>🔧 Elasticsearch의 작동 원리</h2>\n<p>Elasticsearch이 데이터를 저장하고 검색하는 과정입니다.</p>\n<ol>\n<li><strong>데이터 인덱싱</strong>: 문서를 인덱스에 추가할 때, 텍스트 데이터를 분석(토큰화, 필터링 등)하여 역인덱스를 생성합니다.</li>\n<li><strong>검색 쿼리 처리</strong>: 검색 쿼리를 보내면, 클러스터 내 모든 관련 샤드에서 병렬로 검색을 수행합니다.</li>\n<li><strong>결과 집계</strong>: 각 샤드에서 반환된 결과를 집계하여 최종 검색 결과를 제공합니다.</li>\n</ol>\n<p>이런 과정을 통해서 빠르고 효율적인 검색을 가능하게 합니다.</p>\n<hr>\n<h2>🪓 간단한 사용 방법</h2>\n<ul>\n<li>Elasticsearch을 사용하기 위해서 다양한 작업을 알아야 하는데 하나하나 설명하겠습니다.</li>\n</ul>\n<h3>인덱스 생성하기</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">    curl -X PUT \"localhost:9200/my-index\" -H 'Content-Type: application/json' -d'\n    {\n    \"settings\": {\n        \"number_of_shards\": 3,\n        \"number_of_replicas\": 2\n    },\n    \"mappings\": {\n        \"properties\": {\n        \"title\": { \"type\": \"text\" },\n        \"date\": { \"type\": \"date\" },\n        \"views\": { \"type\": \"integer\" }\n        }\n    }\n    }\n    '</code></pre></div>\n<h3>문서 인덱싱</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">    curl -X POST \"localhost:9200/my-index/_doc/1\" -H 'Content-Type: application/json' -d'\n    {\n    \"title\": \"Elasticsearch 기본 사용법\",\n    \"date\": \"2024-04-27\",\n    \"views\": 100\n    }\n    '</code></pre></div>\n<h3>검색 쿼리</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">    curl -X GET \"localhost:9200/my-index/_search\" -H 'Content-Type: application/json' -d'\n    {\n    \"query\": {\n        \"match\": {\n        \"title\": \"Elasticsearch\"\n        }\n    }\n    }\n    '</code></pre></div>\n<p>이렇게 하면 응답은 문서들의 리스트와 관련 점수를 반환하게 됩니다.</p>\n<hr>\n<h2>💎 장점과 단점</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">장점\n\n•\t높은 성능: 분산 아키텍처와 역인덱스 구조 덕분에 대량의 데이터도 빠르게 검색할 수 있습니다.\n•\t확장성: 클러스터에 노드를 추가함으로써 손쉽게 확장할 수 있습니다.\n•\t유연한 데이터 모델링: 다양한 데이터 타입과 복잡한 매핑을 지원하여 유연하게 데이터 구조를 설계할 수 있습니다.\n•\t강력한 커뮤니티 및 생태계: Kibana, Logstash 등과의 통합을 통해 풍부한 기능을 활용할 수 있습니다.\n\n단점\n\n•\t복잡한 설정: 대규모 클러스터를 운영할 때는 설정과 관리가 복잡할 수 있습니다.\n•\t자원 소모: 메모리와 CPU 사용량이 높을 수 있어서 하드웨어 자원이 필요합니다.\n•\t학습 곡선: Elasticsearch의 다양한 기능과 쿼리 DSL을 익히는 데 시간이 걸립니다.</code></pre></div>\n<h2>🧑🏻‍💻 결론</h2>\n<p>Elasticsearch를 파이프라인을 제작하면서 사용해본적이 있지만 제대로 사용한적 없고, 이해도가 떨어진다고 생각해서 공부를 해봤습니다.</p>\n<p>감사합니다.</p>","categories":["ELK","Elasticsearch","ALL"],"date":"September 28, 2024","description":"Elasticsearch 소개하기 ","id":"5c16723b-16ee-5dc4-af6f-a4788e571c87","keywords":["Pipeline","Elasticsearch"],"slug":"/my-first-article/About-Elasticsearch-1/","title":" ⚙️ [ELK] Elasticsearch ","readingTime":{"text":"5 min read"}},{"banner":{"alt":"Snowflake","caption":"Photo by <u><a href=\"https://www.snowflake.com/ko/\">Snowflake korea</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABBUlEQVR42mNgGGLgPyMEU9vA/8g0A5IlUDF0jBPs/8/BsP2VJEP9fyZMu4h2PVTRzufcDNtelzBsfb2MYdvLEIZVV9gYtjwUZNj2ho9h1Qchhiv/2Rh2fxRm2PLcnGHXQymGHS9VgA7gYdj0VITh2CNOhHkw1+x4YwY0bC7Q0AqGba+mM2x97sSw+WU8w/a37gzb3kYA6VCgeATDzo++QAvjGXa8L2DY+jKRYeurVIaZ/1kxXbj/JQ/Qu0VABYsZtr4JBvP3fBYHu2LnZzGGbc9EGdY9EWbYdoudYddrKYZtn0QZdgBdvuurFG7fg7y9+50c0BAWGqQg9JhEihSiY5mgggEAAPL/kt8GEssdAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/3673e33227e62b14fdefb0cf6531c891/bc51f/snowflake-cover.png","srcSet":"/static/3673e33227e62b14fdefb0cf6531c891/41200/snowflake-cover.png 165w,\n/static/3673e33227e62b14fdefb0cf6531c891/f979a/snowflake-cover.png 330w,\n/static/3673e33227e62b14fdefb0cf6531c891/bc51f/snowflake-cover.png 660w,\n/static/3673e33227e62b14fdefb0cf6531c891/f57dd/snowflake-cover.png 1320w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/3673e33227e62b14fdefb0cf6531c891/322ad/snowflake-cover.webp 165w,\n/static/3673e33227e62b14fdefb0cf6531c891/de3b3/snowflake-cover.webp 330w,\n/static/3673e33227e62b14fdefb0cf6531c891/2b2b5/snowflake-cover.webp 660w,\n/static/3673e33227e62b14fdefb0cf6531c891/e36a7/snowflake-cover.webp 1320w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>❄️ Snowflake: 데이터 웨어하우징</h1>\n<hr>\n<h2>서론</h2>\n<p><strong>Snowflake</strong> 클라우드 기반 데이터 웨어하우스 솔루션입니다.</p>\n<p>이번 포스팅에서는 Snowflake가 무엇인지, 주요 특징과 장점, 아키텍처, 사용 사례 등을 자세히 살펴보겠습니다.</p>\n<hr>\n<h2>❄️ Snowflake란?</h2>\n<p><strong>Snowflake</strong>는 클라우드 네이티브 데이터 웨어하우스 플랫폼으로, 데이터 저장, 처리, 분석을 단일 통합된 시스템에서 제공합니다.</p>\n<p>AWS, Azure, Google Cloud와 클라우드에서 실행되며 확장성, 유연성, 성능 면에서 뛰어난 특징을 가지고 있습니다.</p>\n<h3>주요 특징</h3>\n<ul>\n<li><strong>클라우드 네이티브</strong>: 완전히 클라우드에서 설계되어 온프레미스 인프라의 제약에서 벗어나 유연하게 확장할 수 있습니다.</li>\n<li><strong>다중 클라우드 지원</strong>: AWS, Azure, Google Cloud 등 다양한 클라우드 플랫폼에서 실행 가능하여, 기업의 클라우드 전략에 맞게 선택할 수 있습니다.</li>\n<li><strong>편리한 데이터 공유</strong>: 데이터 공유 기능을 통해 조직 간 또는 내부 팀 간에 데이터를 쉽게 공유하고 협업할 수 있습니다.</li>\n<li><strong>자동화된 관리</strong>: 인프라 관리, 최적화, 유지보수가 자동화되어 사용자가 데이터 분석에 집중할 수 있습니다.</li>\n</ul>\n<hr>\n<h2>Snowflake의 주요 특징</h2>\n<h3>클라우드 네이티브 아키텍처</h3>\n<ul>\n<li><strong>분리된 저장 및 컴퓨팅</strong>: 데이터 저장과 컴퓨팅 리소스를 분리하여 독립적으로 확장할 수 있습니다.</li>\n<li><strong>무제한 확장성</strong>: 필요에 따라 컴퓨팅 리소스를 즉시 확장하거나 축소할 수 있어 비용 효율적입니다.</li>\n</ul>\n<h3>자동 스케일링</h3>\n<p>Snowflake는 워크로드에 따라 자동으로 컴퓨팅 리소스를 조정합니다. 이는 피크 시간대에도 성능을 유지하면서 비용을 절감할 수 있게 해줍니다.</p>\n<ul>\n<li><strong>자동 클러스터링</strong>: 데이터가 자동으로 클러스터링되어 쿼리 성능이 최적화됩니다.</li>\n<li><strong>동적 리소스 할당</strong>: 작업 부하에 따라 필요한 리소스를 자동으로 할당합니다.</li>\n</ul>\n<h3>데이터 공유</h3>\n<p>Snowflake의 데이터 공유 기능을 통해 데이터베이스를 직접 공유할 수 있습니다. 이는 데이터 복제 없이 실시간으로 데이터를 공유할 수 있어 효율적입니다.</p>\n<ul>\n<li><strong>실시간 데이터 공유</strong>: 데이터가 실시간으로 업데이트되며, 다른 사용자나 조직과 즉시 공유할 수 있습니다.</li>\n<li><strong>보안 강화</strong>: 데이터 공유 시 접근 제어와 보안이 철저하게 관리됩니다.</li>\n</ul>\n<h3>보안 및 컴플라이언스</h3>\n<p>Snowflake는 데이터 보안과 컴플라이언스를 최우선으로 합니다. 다양한 보안 기능과 규정 준수 인증을 통해 데이터를 안전하게 보호합니다.</p>\n<ul>\n<li><strong>암호화</strong>: 저장 데이터와 전송 데이터 모두 암호화됩니다.</li>\n<li><strong>역할 기반 접근 제어(RBAC)</strong>: 사용자와 역할에 따라 세밀한 접근 권한을 설정할 수 있습니다.</li>\n<li><strong>컴플라이언스 인증</strong>: GDPR, HIPAA, SOC 2 등 다양한 컴플라이언스 인증을 획득하고 있습니다.</li>\n</ul>\n<hr>\n<h2>Snowflake 아키텍처</h2>\n<p>Snowflake의 아키텍처는 세 가지 주요 계층으로 구성됩니다: 데이터 저장 계층, 컴퓨팅 계층, 클라우드 서비스 계층.</p>\n<h3>데이터 저장 계층</h3>\n<p>Snowflake는 데이터를 압축된 형태로 저장하며, 자동으로 분산 저장됩니다. 이는 고속의 데이터 접근과 높은 데이터 무결성을 보장합니다.</p>\n<ul>\n<li><strong>Columnar Storage</strong>: 컬럼 기반 저장 방식을 통해 대규모 데이터 분석에 최적화되었습니다.</li>\n<li><strong>데이터 압축</strong>: 데이터를 효율적으로 압축하여 저장 비용을 절감합니다.</li>\n</ul>\n<h3>컴퓨팅 계층</h3>\n<p>Snowflake의 컴퓨팅 계층은 독립적인 컴퓨팅 클러스터(가상 웨어하우스)로 구성됩니다. 각 클러스터는 독립적으로 작동하며, 동시에 여러 작업을 처리할 수 있습니다.</p>\n<ul>\n<li><strong>가상 웨어하우스</strong>: 독립적인 컴퓨팅 리소스로, 서로 간섭 없이 병렬로 쿼리를 처리할 수 있습니다.</li>\n<li><strong>자동 스케일링</strong>: 워크로드에 따라 자동으로 확장 및 축소됩니다.</li>\n</ul>\n<h3>클라우드 서비스 계층</h3>\n<p>클라우드 서비스 계층은 Snowflake의 전반적인 관리, 보안, 메타데이터 관리를 담당합니다.</p>\n<ul>\n<li><strong>메타데이터 관리</strong>: 데이터 구조와 쿼리 성능을 최적화하기 위한 메타데이터를 관리합니다.</li>\n<li><strong>보안 및 컴플라이언스</strong>: 데이터 보안과 컴플라이언스 준수를 위한 기능을 제공합니다.</li>\n</ul>\n<hr>\n<h2>Snowflake의 장점</h2>\n<ul>\n<li><strong>높은 성능</strong>: 분산 아키텍처와 자동 스케일링을 통해 높은 쿼리 성능을 유지합니다.</li>\n<li><strong>비용 효율성</strong>: 사용한 만큼만 비용을 지불하는 모델로, 불필요한 비용을 절감할 수 있습니다.</li>\n<li><strong>사용자 친화적</strong>: SQL을 기반으로 하여 데이터 분석가와 엔지니어가 쉽게 사용할 수 있습니다.</li>\n<li><strong>통합 데이터 관리</strong>: 다양한 데이터 소스와 쉽게 통합할 수 있어 데이터 파이프라인을 간소화합니다.</li>\n<li><strong>강력한 보안</strong>: 데이터 암호화, 접근 제어, 컴플라이언스 지원 등 강력한 보안 기능을 제공합니다.</li>\n</ul>\n<hr>\n<h2>사용 사례</h2>\n<h3>데이터 분석</h3>\n<p>Snowflake는 대규모 데이터 분석에 최적화되어 있습니다. 데이터 분석가들은 Snowflake를 사용하여 빠르고 효율적으로 데이터를 쿼리하고 분석할 수 있습니다.</p>\n<ul>\n<li><strong>비즈니스 인텔리전스</strong>: 실시간으로 데이터를 분석하여 비즈니스 의사결정을 지원합니다.</li>\n<li><strong>데이터 마이닝</strong>: 대규모 데이터셋에서 패턴과 인사이트를 도출합니다.</li>\n</ul>\n<h3>데이터 통합</h3>\n<p>Snowflake는 다양한 데이터 소스와의 통합을 지원합니다. 이를 통해 기업은 여러 시스템에서 데이터를 수집하고 통합하여 일관된 데이터 뷰를 얻을 수 있습니다.</p>\n<ul>\n<li><strong>ETL/ELT 프로세스</strong>: 데이터 추출, 변환, 로드를 효율적으로 수행합니다.</li>\n<li><strong>데이터 레이크 통합</strong>: 데이터 레이크와 연동하여 비정형 데이터도 쉽게 관리할 수 있습니다.</li>\n</ul>\n<h3>실시간 데이터 처리</h3>\n<p>Snowflake는 실시간 데이터 스트리밍과의 통합을 통해 실시간으로 데이터를 처리하고 분석할 수 있습니다.</p>\n<ul>\n<li><strong>실시간 대시보드</strong>: 실시간으로 업데이트되는 대시보드를 통해 데이터 인사이트를 제공합니다.</li>\n<li><strong>실시간 알림</strong>: 특정 조건이 만족될 때 실시간으로 알림을 받을 수 있습니다.</li>\n</ul>\n<hr>\n<h2>Snowflake와 타 데이터 웨어하우스 비교</h2>\n<h3>Snowflake vs AWS Redshift</h3>\n<table>\n<thead>\n<tr>\n<th>특징</th>\n<th>Snowflake</th>\n<th>AWS Redshift</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>클라우드 지원</strong></td>\n<td>다중 클라우드 지원 (AWS, Azure, GCP)</td>\n<td>AWS 전용</td>\n</tr>\n<tr>\n<td><strong>자동 스케일링</strong></td>\n<td>자동 컴퓨팅 리소스 확장/축소</td>\n<td>수동 스케일링</td>\n</tr>\n<tr>\n<td><strong>데이터 공유</strong></td>\n<td>네이티브 데이터 공유 기능</td>\n<td>데이터 공유 지원, 그러나 제한적</td>\n</tr>\n<tr>\n<td><strong>보안</strong></td>\n<td>완전한 데이터 암호화, RBAC</td>\n<td>데이터 암호화, IAM 기반 접근 제어</td>\n</tr>\n<tr>\n<td><strong>가격 모델</strong></td>\n<td>사용한 만큼 지불 (Pay-as-you-go)</td>\n<td>사전 예약 인스턴스 및 온디맨드</td>\n</tr>\n<tr>\n<td><strong>쿼리 성능</strong></td>\n<td>높은 동시성 및 빠른 쿼리 성능</td>\n<td>최적화 필요</td>\n</tr>\n</tbody>\n</table>\n<h3>Snowflake vs Google BigQuery</h3>\n<table>\n<thead>\n<tr>\n<th>특징</th>\n<th>Snowflake</th>\n<th>Google BigQuery</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>클라우드 지원</strong></td>\n<td>다중 클라우드 지원</td>\n<td>Google Cloud 전용</td>\n</tr>\n<tr>\n<td><strong>쿼리 언어</strong></td>\n<td>ANSI SQL 지원</td>\n<td>ANSI SQL 지원</td>\n</tr>\n<tr>\n<td><strong>데이터 공유</strong></td>\n<td>네이티브 데이터 공유 기능</td>\n<td>데이터 공유 지원, 그러나 제한적</td>\n</tr>\n<tr>\n<td><strong>자동 스케일링</strong></td>\n<td>자동 컴퓨팅 리소스 확장/축소</td>\n<td>자동 확장</td>\n</tr>\n<tr>\n<td><strong>보안</strong></td>\n<td>완전한 데이터 암호화, RBAC</td>\n<td>데이터 암호화, IAM 기반 접근 제어</td>\n</tr>\n<tr>\n<td><strong>가격 모델</strong></td>\n<td>사용한 만큼 지불 (Pay-as-you-go)</td>\n<td>사용한 만큼 지불 (Pay-as-you-go)</td>\n</tr>\n<tr>\n<td><strong>쿼리 성능</strong></td>\n<td>높은 동시성 및 빠른 쿼리 성능</td>\n<td>높은 쿼리 성능</td>\n</tr>\n</tbody>\n</table>\n<h3>Snowflake vs Microsoft Azure Synapse</h3>\n<table>\n<thead>\n<tr>\n<th>특징</th>\n<th>Snowflake</th>\n<th>Microsoft Azure Synapse</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>클라우드 지원</strong></td>\n<td>다중 클라우드 지원</td>\n<td>Azure 전용</td>\n</tr>\n<tr>\n<td><strong>통합 분석 도구</strong></td>\n<td>데이터 웨어하우스, 데이터 레이크 통합</td>\n<td>데이터 웨어하우스, 데이터 레이크, 빅 데이터 통합</td>\n</tr>\n<tr>\n<td><strong>자동 스케일링</strong></td>\n<td>자동 컴퓨팅 리소스 확장/축소</td>\n<td>수동 및 자동 스케일링 지원</td>\n</tr>\n<tr>\n<td><strong>데이터 공유</strong></td>\n<td>네이티브 데이터 공유 기능</td>\n<td>데이터 공유 지원, 그러나 제한적</td>\n</tr>\n<tr>\n<td><strong>보안</strong></td>\n<td>완전한 데이터 암호화, RBAC</td>\n<td>데이터 암호화, Azure AD 기반 접근 제어</td>\n</tr>\n<tr>\n<td><strong>가격 모델</strong></td>\n<td>사용한 만큼 지불 (Pay-as-you-go)</td>\n<td>사용한 만큼 지불 (Pay-as-you-go)</td>\n</tr>\n<tr>\n<td><strong>쿼리 성능</strong></td>\n<td>높은 동시성 및 빠른 쿼리 성능</td>\n<td>통합된 분석 기능으로 높은 쿼리 성능</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2>Snowflake 시작하기</h2>\n<h3>계정 생성</h3>\n<ol>\n<li><strong>Snowflake 웹사이트 방문</strong>: <a href=\"https://www.snowflake.com/\">Snowflake 공식 웹사이트</a>에 접속합니다.</li>\n<li><strong>무료 체험 신청</strong>: 무료 체험을 신청하여 Snowflake의 기능을 직접 체험해 볼 수 있습니다.</li>\n<li><strong>계정 설정</strong>: 이메일 인증을 통해 계정을 생성하고, 초기 설정을 완료합니다.</li>\n</ol>\n<h3>기본 설정</h3>\n<ol>\n<li><strong>워크스페이스 설정</strong>: 조직의 요구에 맞게 워크스페이스를 설정합니다.</li>\n<li><strong>데이터베이스 및 스키마 생성</strong>: 데이터를 저장할 데이터베이스와 스키마를 생성합니다.\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">DATABASE</span> my_database<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">SCHEMA</span> my_schema<span class=\"token punctuation\">;</span></code></pre></div>\n</li>\n<li><strong>사용자 및 역할 관리</strong>: 사용자와 역할을 생성하고, 적절한 권한을 부여합니다.\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">USER</span> analyst PASSWORD <span class=\"token operator\">=</span> <span class=\"token string\">'password123'</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">GRANT</span> ROLE analyst <span class=\"token keyword\">TO</span> <span class=\"token keyword\">USER</span> analyst<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">GRANT</span> <span class=\"token keyword\">USAGE</span> <span class=\"token keyword\">ON</span> <span class=\"token keyword\">DATABASE</span> my_database <span class=\"token keyword\">TO</span> ROLE analyst<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">GRANT</span> <span class=\"token keyword\">USAGE</span> <span class=\"token keyword\">ON</span> <span class=\"token keyword\">SCHEMA</span> my_schema <span class=\"token keyword\">TO</span> ROLE analyst<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">GRANT</span> <span class=\"token keyword\">SELECT</span> <span class=\"token keyword\">ON</span> <span class=\"token keyword\">ALL</span> <span class=\"token keyword\">TABLES</span> <span class=\"token operator\">IN</span> <span class=\"token keyword\">SCHEMA</span> my_schema <span class=\"token keyword\">TO</span> ROLE analyst<span class=\"token punctuation\">;</span></code></pre></div>\n</li>\n</ol>\n<h3>첫 번째 데이터 로드</h3>\n<ol>\n<li><strong>데이터 로드 준비</strong>: 로컬 시스템에서 데이터를 준비합니다 (예: CSV 파일).</li>\n<li><strong>Stage 생성</strong>: 데이터를 로드할 스테이지를 생성합니다.\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token operator\">OR</span> <span class=\"token keyword\">REPLACE</span> STAGE my_stage\nFILE_FORMAT <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">TYPE</span> <span class=\"token operator\">=</span> <span class=\"token string\">'CSV'</span> FIELD_DELIMITER <span class=\"token operator\">=</span> <span class=\"token string\">','</span> SKIP_HEADER <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n</li>\n<li><strong>데이터 로드</strong>: 데이터를 스테이지로 업로드하고, 테이블에 로드합니다.\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">PUT <span class=\"token keyword\">file</span>:<span class=\"token comment\">///path/to/data.csv @my_stage;</span>\n\n<span class=\"token keyword\">CREATE</span> <span class=\"token operator\">OR</span> <span class=\"token keyword\">REPLACE</span> <span class=\"token keyword\">TABLE</span> my_table <span class=\"token punctuation\">(</span>\n    id <span class=\"token keyword\">INT</span><span class=\"token punctuation\">,</span>\n    name STRING<span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">value</span> <span class=\"token keyword\">FLOAT</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\nCOPY <span class=\"token keyword\">INTO</span> my_table\n<span class=\"token keyword\">FROM</span> <span class=\"token variable\">@my_stage</span><span class=\"token operator\">/</span><span class=\"token keyword\">data</span><span class=\"token punctuation\">.</span>csv\nFILE_FORMAT <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">TYPE</span> <span class=\"token operator\">=</span> <span class=\"token string\">'CSV'</span> FIELD_DELIMITER <span class=\"token operator\">=</span> <span class=\"token string\">','</span> SKIP_HEADER <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n</li>\n</ol>\n<hr>\n<h2>베스트 프랙티스</h2>\n<ol>\n<li><strong>데이터 모델링 최적화</strong>: 효율적인 쿼리 성능을 위해 데이터 모델링을 신중하게 설계합니다.</li>\n<li><strong>자동화된 데이터 로드</strong>: Snowpipe를 사용하여 실시간 데이터 로드를 자동화합니다.</li>\n<li><strong>보안 강화</strong>: 역할 기반 접근 제어(RBAC)를 철저히 적용하고, 데이터 암호화를 활용합니다.</li>\n<li><strong>비용 관리</strong>: 컴퓨팅 리소스 사용을 모니터링하고, 불필요한 리소스 사용을 최소화합니다.</li>\n<li><strong>모니터링 및 알림 설정</strong>: Snowflake의 모니터링 도구를 활용하여 시스템 상태를 지속적으로 점검하고, 이상 징후에 대해 알림을 설정합니다.</li>\n</ol>\n<hr>\n<h2>결론</h2>\n<p>클라우드 네이티브 아키텍처, 자동 스케일링, 데이터 공유 기능 등 다양한 특징을 통해 데이터 관리와 분석을 효율적으로 수행할 수 있습니다.</p>\n<p>Snowflake를 활용하면 데이터 기반의 의사결정을 신속하게 내리고, 비즈니스 경쟁력을 강화할 수 있습니다.</p>\n<hr>\n<h2>추가 자료</h2>\n<ul>\n<li><a href=\"https://docs.snowflake.com/\">Snowflake 공식 문서</a></li>\n<li><a href=\"https://www.snowflake.com/guides/getting-started/\">Snowflake 빠르게 시작하기</a></li>\n<li><a href=\"https://www.snowflake.com/blog/\">Snowflake 블로그</a></li>\n<li><a href=\"https://www.snowflake.com/blog/snowflake-architecture-the-cloud-data-platform-that-bites-into-amazon-redshift/\">Snowflake 아키텍처 소개</a></li>\n<li><a href=\"https://www.snowflake.com/guides/data-sharing-in-snowflake/\">Snowflake와 데이터 공유</a></li>\n</ul>","categories":["Snowflake","ALL"],"date":"September 23, 2024","description":"Snowflake 에 대해.","id":"ff4bddde-294b-5a46-beac-6dfa0922b710","keywords":["Blog","Snowflake","Data Warehouse"],"slug":"/my-first-article/About-Snowflake-1/","title":" ❄️ [Snowflake] About Snowflake","readingTime":{"text":"16 min read"}},{"banner":{"alt":"AWS","caption":"Photo by <u><a href=\"https://aws.amazon.com/ko/pm/kinesis/#Learn_More_About_Amazon_Kinesis\">AWS Kinesis</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABoUlEQVR42mP4jwT+/f3z7+/f///+/ScOMEC1AQFQG4QFZCAZ8Q/MwGogAzLn1e3LRzpTvx1ZBHEGwkU43MIAMfPr2xf7u/NmB6gu9hG/3+b5/uw2sP1//v79+/nz558/f3369Pnv37/omoH+BFL3j26bYs83w0tuZYTG+QyVc8UWFxr9/jy+/OLd5yWrtqzdtHvTjgPPX7wCqvz79x+yZpB5Ty8cXZ3usDrTZVue8/UWnzM1nucqHH4+vPzjx5c3D848e3Ln07Oz39/e+Y/qewaIf94/un1iZu3J6ZXXFjV+2dz+ZVPT1+3d/z48+/Xnz6fnl7882v/h2dVXL5+///Dx3bv3cPdDbX518/zeSr8Dlb57Cp0/Tgr4Niviy+yY/x8fXbx+d/maLZt27F+wbMPh42c2bNm578DRL1+/oQTYlzfPj04u3V7gerY98svchK/TQr5vbf///ePnr9+ePHn65u3b+w8ePXn6/OHjp0+evfjx4yeWqHpy+dT+1pQvK8v+vbgOiSOiEwk0Yv/Bkxskfv9hAOyJBJY8wSmMiOQJAGGmhrKBConuAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/86224ccb34b5514cc6d906ef544f89d7/98851/kinesis-cover.png","srcSet":"/static/86224ccb34b5514cc6d906ef544f89d7/4ec1c/kinesis-cover.png 150w,\n/static/86224ccb34b5514cc6d906ef544f89d7/ca4aa/kinesis-cover.png 300w,\n/static/86224ccb34b5514cc6d906ef544f89d7/98851/kinesis-cover.png 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/86224ccb34b5514cc6d906ef544f89d7/40ac4/kinesis-cover.webp 150w,\n/static/86224ccb34b5514cc6d906ef544f89d7/9e761/kinesis-cover.webp 300w,\n/static/86224ccb34b5514cc6d906ef544f89d7/2ea1b/kinesis-cover.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":660,"height":400.40000000000003}}}},"body":"<h1>💫 AWS Kinesis란?</h1>\n<p>AWS Kinesis는 실시간으로 대규모 데이터를 수집, 처리, 분석할 수 있는 스트리밍 데이터 플랫폼입니다.</p>\n<p>이를 통해 사용자는 로그, 이벤트, IoT 디바이스 데이터 등을 실시간으로 처리하여 비즈니스 인사이트를 얻을 수 있습니다.</p>\n<hr>\n<h2>주요 특징</h2>\n<ul>\n<li><strong>확장성</strong>: 필요에 따라 자동으로 확장되어 대규모 데이터 스트림 가능.</li>\n<li><strong>완전관리형 서비스</strong>: 인프라 관리 없이 손쉽게 스트리밍 데이터를 처리 가능.</li>\n<li><strong>다양한 통합</strong>: AWS의 다른 서비스와 원활하게 통합되어 데이터 파이프라인을 구축 가능.</li>\n</ul>\n<hr>\n<h2>AWS Kinesis의 장점</h2>\n<ul>\n<li><strong>실시간 처리</strong>: 데이터를 실시간으로 수집하고 분석하여 빠르게 의사결정을 내릴 수 있습니다.</li>\n<li><strong>확장성</strong>: 대규모 데이터 스트림을 자동으로 처리할 수 있는 확장성을 제공합니다.</li>\n<li><strong>비용 효율성</strong>: 사용한 만큼만 비용을 지불하는 구조로, 비용 효율적인 데이터 스트리밍 솔루션을 제공합니다.</li>\n<li><strong>통합성</strong>: AWS의 다른 서비스와 원활하게 통합되어 데이터 파이프라인을 쉽게 구축할 수 있습니다.</li>\n</ul>\n<hr>\n<h3>실시간 데이터 처리</h3>\n<p>금융 서비스, 게임, 소셜 미디어 등 실시간 데이터 처리가 중요한 분야에서 Kinesis를 활용할 수 있습니다. 실시간으로 데이터를 분석하여 사용자 경험을 향상시키거나, 실시간 추천 시스템을 구축할 수 있습니다.</p>\n<h3>로그 및 이벤트 수집</h3>\n<p>애플리케이션 로그, 시스템 이벤트 등을 실시간으로 수집하고 분석하여 문제를 빠르게 감지하고 대응할 수 있습니다. 이를 통해 시스템의 안정성을 높이고, 운영 효율성을 개선할 수 있습니다.</p>\n<h3>스트리밍 데이터 분석</h3>\n<p>IoT 디바이스에서 생성되는 대규모 데이터를 실시간으로 분석하여 인사이트를 도출할 수 있습니다. 예를 들어, 제조업에서는 센서 데이터를 실시간으로 모니터링하여 생산 공정을 최적화할 수 있습니다.</p>\n<h2>AWS Kinesis와 다른 스트리밍 서비스 비교</h2>\n<table>\n<thead>\n<tr>\n<th>특징</th>\n<th>AWS Kinesis</th>\n<th>Apache Kafka</th>\n<th>Google Pub/Sub</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>확장성</strong></td>\n<td>자동 확장 지원</td>\n<td>수동 확장 필요</td>\n<td>자동 확장 지원</td>\n</tr>\n<tr>\n<td><strong>관리형 서비스</strong></td>\n<td>완전관리형</td>\n<td>자체 관리 필요</td>\n<td>완전관리형</td>\n</tr>\n<tr>\n<td><strong>통합성</strong></td>\n<td>AWS 생태계와 뛰어난 통합</td>\n<td>다양한 오픈 소스 도구와 통합 가능</td>\n<td>Google Cloud와 뛰어난 통합</td>\n</tr>\n<tr>\n<td><strong>비용</strong></td>\n<td>사용량 기반 과금</td>\n<td>자체 호스팅 시 인프라 비용 발생</td>\n<td>사용량 기반 과금</td>\n</tr>\n<tr>\n<td><strong>실시간 처리</strong></td>\n<td>뛰어난 실시간 처리 기능</td>\n<td>높은 실시간 처리 가능</td>\n<td>뛰어난 실시간 처리 기능</td>\n</tr>\n</tbody>\n</table>","categories":["AWS","Kinesis","ALL"],"date":"September 20, 2024","description":"AWS 기능들을 활용하여 파이프라인 구축해보기","id":"386aef4b-5f22-542b-b4ae-60bc4e4a088d","keywords":["Pipeline","AWS","Cloud","Kinesis"],"slug":"/my-first-article/\bAbout-AWS-Kienesis/","title":" ⚙️ [AWS] AWS Kienesis ","readingTime":{"text":"4 min read"}},{"banner":{"alt":"AWS","caption":"Photo by <u><a href=\"https://aws.amazon.com/ko/glue/\">AWS Glue</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA0UlEQVR42mP4jxv8/fMPiv5iV8DwnwKATfM/EPH7179ts16u7ni5vuvdqfXfidX8D6z5x7e/9UFXJ2fd74t9urDkDYmav/9pibk0IevWpPQHS2pfkmrzn3znYwWOp4rsL03OvE+qn/9umnN/ec+d1X2PTm75ABUlwmaQus+fv544e+r6zSvXbly6cuPar1+/iYwqkObfn55/eHTx47t3Hz58fnPr8N9nxxCuIhzPf378f3ns/+3F/+8s+//y+P8vz0jRjB4IpKawf38RjH9YAgwA6F2yI4FpcIIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/c1e1ca5b4905a67c959052c1f22daa49/bc51f/glue.png","srcSet":"/static/c1e1ca5b4905a67c959052c1f22daa49/41200/glue.png 165w,\n/static/c1e1ca5b4905a67c959052c1f22daa49/f979a/glue.png 330w,\n/static/c1e1ca5b4905a67c959052c1f22daa49/bc51f/glue.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/c1e1ca5b4905a67c959052c1f22daa49/322ad/glue.webp 165w,\n/static/c1e1ca5b4905a67c959052c1f22daa49/de3b3/glue.webp 330w,\n/static/c1e1ca5b4905a67c959052c1f22daa49/2b2b5/glue.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>⚒️ AWS Glue</h1>\n<p><a href=\"https://d1.awsstatic.com/product-marketing/Glue/aws-glue-hero.ea78a6686b1af9f04c99a9d6d32f0c6c1b5c5a9c.png\">AWS Glue 배너</a></p>\n<p><strong>AWS Glue</strong>는 Amazon Web Services(AWS)에서 제공하는 완전관리형 ETL(Extract, Transform, Load) 서비스입니다.</p>\n<p>서버리스 환경에서 데이터를 준비하고 변환하여 분석을 위한 준비 과정을 간소화합니다.</p>\n<hr>\n<h2>목차</h2>\n<ol>\n<li><a href=\"#aws-glue%EB%9E%80\">AWS Glue란?</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95\">주요 특징</a></li>\n<li><a href=\"#aws-glues-%EA%B5%AC%EC%84%B1-%EC%9A%94%EC%86%8C\">AWS Glue의 구성 요소</a>\n<ul>\n<li><a href=\"#glue-data-catalog\">Glue Data Catalog</a></li>\n<li><a href=\"#glue-etl-%EC%97%94%EC%A7%84\">Glue ETL 엔진</a></li>\n<li><a href=\"#glue-crawlers\">Glue Crawlers</a></li>\n<li><a href=\"#glue-jobs\">Glue Jobs</a></li>\n<li><a href=\"#glue-development-endpoints\">Glue Development Endpoints</a></li>\n</ul>\n</li>\n<li><a href=\"#%EC%82%AC%EC%9A%A9-%EC%82%AC%EB%A1%80\">사용 사례</a>\n<ul>\n<li><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%86%B5%ED%95%A9\">데이터 통합</a></li>\n<li><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A4%80%EB%B9%84\">데이터 준비</a></li>\n<li><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%B9%B4%ED%83%88%EB%A1%9C%EA%B7%B8%EB%A7%81\">데이터 카탈로그링</a></li>\n<li><a href=\"#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\">머신러닝</a></li>\n</ul>\n</li>\n<li><a href=\"#aws-glues-%EC%9E%A5%EC%A0%90\">AWS Glue의 장점</a>\n<ul>\n<li><a href=\"#%EC%84%9C%EB%B2%84%EB%A6%AC%EC%8A%A4-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98\">서버리스 아키텍처</a></li>\n<li><a href=\"#%ED%99%95%EC%9E%A5%EC%84%B1\">확장성</a></li>\n<li><a href=\"#%EB%B9%84%EC%9A%A9-%ED%9A%A8%EC%9C%A8%EC%84%B1\">비용 효율성</a></li>\n<li><a href=\"#aws-%EC%83%9D%ED%83%9C%EA%B3%84%EC%99%80%EC%9D%98-%EC%9B%90%ED%99%9C%ED%95%9C-%ED%86%B5%ED%95%A9\">AWS 생태계와의 원활한 통합</a></li>\n</ul>\n</li>\n<li><a href=\"#%EA%B0%80%EA%B2%A9-%EA%B0%9C%EC%9A%94\">가격 개요</a></li>\n<li><a href=\"#aws-glue-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0\">AWS Glue 시작하기</a>\n<ul>\n<li><a href=\"#aws-glue-%EC%84%A4%EC%A0%95\">AWS Glue 설정</a></li>\n<li><a href=\"#glue-crawler-%EC%83%9D%EC%84%B1\">Glue Crawler 생성</a></li>\n<li><a href=\"#etl-job-%EC%83%9D%EC%84%B1\">ETL Job 생성</a></li>\n<li><a href=\"#job-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%EB%B0%8F-%EA%B4%80%EB%A6%AC\">Job 모니터링 및 관리</a></li>\n</ul>\n</li>\n<li><a href=\"#%EB%8B%A4%EB%A5%B8-etl-%EB%8F%84%EA%B5%AC%EC%99%80%EC%9D%98-%EB%B9%84%EA%B5%90\">다른 ETL 도구와의 비교</a>\n<ul>\n<li><a href=\"#aws-glue-vs-apache-spark\">AWS Glue vs. Apache Spark</a></li>\n<li><a href=\"#aws-glue-vs-aws-data-pipeline\">AWS Glue vs. AWS Data Pipeline</a></li>\n<li><a href=\"#aws-glue-vs-%EA%B8%B0%ED%83%80-%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-etl-%EC%84%9C%EB%B9%84%EC%8A%A4\">AWS Glue vs. 기타 클라우드 ETL 서비스</a></li>\n</ul>\n</li>\n<li><a href=\"#%EB%AA%A8%EB%B2%94-%EC%82%AC%EB%A1%80\">모범 사례</a></li>\n<li><a href=\"#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C-%EB%B0%8F-%EC%B6%94%EA%B0%80-%ED%95%99%EC%8A%B5\">참고 자료 및 추가 학습</a></li>\n</ol>\n<hr>\n<h2>AWS Glue란?</h2>\n<p><strong>AWS Glue</strong>는 데이터 준비, 변환 및 로딩을 간소화하는 완전관리형 서버리스 ETL 서비스입니다. AWS Glue는 데이터를 카탈로그링하고, 변환하며, 다양한 데이터 소스 간에 데이터를 통합하는 작업을 자동화하여 데이터 분석 및 머신러닝을 위한 준비 과정을 효율적으로 수행할 수 있도록 도와줍니다.</p>\n<hr>\n<h2>주요 특징</h2>\n<ul>\n<li><strong>서버리스:</strong> 인프라를 관리할 필요 없이 AWS Glue가 자동으로 리소스를 프로비저닝, 구성 및 확장합니다.</li>\n<li><strong>Glue Data Catalog:</strong> 중앙 메타데이터 저장소로, 다양한 데이터 소스의 메타데이터를 관리합니다.</li>\n<li><strong>ETL 엔진:</strong> Apache Spark를 기반으로 하여 강력하고 확장 가능한 데이터 변환 기능을 제공합니다.</li>\n<li><strong>Crawlers:</strong> 데이터 소스를 자동으로 스캔하여 스키마를 추론하고 Data Catalog를 업데이트합니다.</li>\n<li><strong>Job 스케줄링:</strong> ETL 작업을 특정 시간에 실행하거나 이벤트에 따라 트리거할 수 있습니다.</li>\n<li><strong>AWS 서비스와의 통합:</strong> Amazon S3, Redshift, RDS 등 다양한 AWS 서비스와 원활하게 통합됩니다.</li>\n</ul>\n<hr>\n<h2>AWS Glue의 구성 요소</h2>\n<h3>Glue Data Catalog</h3>\n<p><strong>Glue Data Catalog</strong>는 AWS Glue의 핵심 메타데이터 저장소로, 데이터 소스에 대한 메타데이터를 중앙에서 관리합니다. 이를 통해 데이터의 검색, 이해 및 관리를 용이하게 합니다.</p>\n<ul>\n<li><strong>데이터베이스 및 테이블:</strong> 메타데이터를 데이터베이스와 테이블로 조직합니다.</li>\n<li><strong>파티션:</strong> 테이블을 파티션으로 분할하여 쿼리 성능을 최적화합니다.</li>\n<li><strong>버전 관리:</strong> 데이터 스키마의 버전을 관리하여 변경 사항을 추적합니다.</li>\n</ul>\n<h3>Glue ETL 엔진</h3>\n<p>AWS Glue의 <strong>ETL 엔진</strong>은 Apache Spark를 기반으로 하며, 대규모 데이터 처리와 변환 작업을 효율적으로 수행합니다.</p>\n<ul>\n<li><strong>동적 프레임:</strong> Spark의 데이터 프레임을 확장한 데이터 구조로, 복잡한 변환 작업을 단순화합니다.</li>\n<li><strong>확장성:</strong> 데이터의 양과 처리 요구에 따라 자동으로 확장됩니다.</li>\n<li><strong>코드 생성:</strong> AWS Glue는 Python 또는 Scala로 ETL 코드를 자동으로 생성하여 사용자 편의를 제공합니다.</li>\n</ul>\n<h3>Glue Crawlers</h3>\n<p><strong>Glue Crawlers</strong>는 데이터 소스를 자동으로 스캔하여 스키마를 추론하고 Data Catalog를 업데이트하는 도구입니다.</p>\n<ul>\n<li><strong>자동화된 스키마 추론:</strong> 데이터 소스의 구조를 자동으로 감지합니다.</li>\n<li><strong>정기적 스캔:</strong> 정기적으로 데이터 소스를 스캔하여 최신 메타데이터를 유지합니다.</li>\n<li><strong>다양한 데이터 소스 지원:</strong> Amazon S3, JDBC 호환 데이터베이스 등 다양한 소스를 지원합니다.</li>\n</ul>\n<h3>Glue Jobs</h3>\n<p><strong>Glue Jobs</strong>는 실제 ETL 작업을 실행하는 작업 단위입니다. 사용자는 시각적 인터페이스 또는 코드를 통해 작업을 정의할 수 있습니다.</p>\n<ul>\n<li><strong>코드 편집기:</strong> AWS Glue Studio를 사용하여 시각적으로 ETL 작업을 설계할 수 있습니다.</li>\n<li><strong>스크립트 관리:</strong> Python 또는 Scala로 작성된 ETL 스크립트를 관리할 수 있습니다.</li>\n<li><strong>재사용 가능성:</strong> 동일한 작업을 여러 데이터 소스에 반복적으로 적용할 수 있습니다.</li>\n</ul>\n<h3>Glue Development Endpoints</h3>\n<p><strong>Glue Development Endpoints</strong>는 사용자 정의 ETL 스크립트를 개발하고 테스트할 수 있는 환경을 제공합니다.</p>\n<ul>\n<li><strong>인터랙티브 개발:</strong> Jupyter 노트북을 통해 인터랙티브하게 코드를 개발하고 테스트할 수 있습니다.</li>\n<li><strong>커스터마이징:</strong> 고급 사용자는 자신의 ETL 로직을 자유롭게 작성할 수 있습니다.</li>\n<li><strong>디버깅:</strong> 개발 중 발생하는 오류를 쉽게 디버깅할 수 있는 기능을 제공합니다.</li>\n</ul>\n<hr>\n<h2>사용 사례</h2>\n<h3>데이터 통합</h3>\n<p>AWS Glue는 다양한 데이터 소스 간의 데이터를 통합하여 일관된 데이터 웨어하우스를 구축하는 데 이상적입니다. 이를 통해 기업은 여러 시스템에서 생성된 데이터를 하나의 중앙 집중식 저장소로 통합하여 보다 효과적인 분석과 의사 결정을 내릴 수 있습니다.</p>\n<h3>데이터 준비</h3>\n<p>데이터 분석을 위해 원시 데이터를 정제, 변환 및 가공하는 과정에서 AWS Glue는 강력한 ETL 기능을 제공합니다. 중복 제거, 데이터 필터링, 형식 변환 등 다양한 데이터 준비 작업을 자동화하여 데이터 과학자와 분석가의 생산성을 높입니다.</p>\n<h3>데이터 카탈로그링</h3>\n<p>AWS Glue Data Catalog는 다양한 데이터 소스의 메타데이터를 중앙에서 관리하여 데이터 검색과 이해를 용이하게 합니다. 이를 통해 데이터 자산을 체계적으로 관리하고, 데이터 거버넌스를 강화할 수 있습니다.</p>\n<h3>머신러닝</h3>\n<p>AWS Glue는 머신러닝 모델 학습을 위한 데이터를 준비하는 데 유용합니다. 대규모 데이터셋을 효율적으로 처리하고, 필요한 특징을 추출하여 머신러닝 파이프라인에 통합할 수 있습니다.</p>\n<hr>\n<h2>AWS Glue의 장점</h2>\n<h3>서버리스 아키텍처</h3>\n<p>AWS Glue는 서버리스로 동작하므로, 사용자는 인프라를 관리할 필요 없이 데이터 통합 작업에 집중할 수 있습니다. 자동으로 리소스를 프로비저닝하고 확장하여 사용자의 요구에 맞게 대응합니다.</p>\n<h3>확장성</h3>\n<p>AWS Glue는 대규모 데이터 처리 작업을 자동으로 확장할 수 있습니다. 데이터의 양과 처리 요구에 따라 리소스를 유연하게 조절하여 높은 성능을 유지합니다.</p>\n<h3>비용 효율성</h3>\n<p>사용한 만큼만 비용을 지불하는 모델로, 초기 투자 비용 없이 필요한 시점에만 리소스를 사용할 수 있습니다. 이를 통해 비용을 최적화하고 예산을 효율적으로 관리할 수 있습니다.</p>\n<h3>AWS 생태계와의 원활한 통합</h3>\n<p>AWS Glue는 Amazon S3, Redshift, RDS, Athena 등 다양한 AWS 서비스와 원활하게 통합됩니다. 이를 통해 데이터 이동과 관리가 간편해지고, 전체 데이터 파이프라인을 효율적으로 구축할 수 있습니다.</p>\n<hr>\n<h2>가격 개요</h2>\n<p>AWS Glue의 가격은 사용한 리소스와 작업 유형에 따라 다릅니다. 주요 가격 요소는 다음과 같습니다:</p>\n<ul>\n<li><strong>데이터 카탈로그:</strong> 저장된 데이터 카탈로그 항목과 메타데이터 조회 요청에 따라 비용이 청구됩니다.</li>\n<li><strong>ETL 작업:</strong> 실행된 ETL 작업의 DPU(Data Processing Unit) 시간에 따라 비용이 청구됩니다.</li>\n<li><strong>Crawlers:</strong> 실행된 크롤러의 DPU 시간에 따라 비용이 청구됩니다.</li>\n<li><strong>개발 엔드포인트:</strong> 개발 엔드포인트의 사용 시간과 리소스에 따라 비용이 청구됩니다.</li>\n</ul>\n<p>자세한 가격 정보는 <a href=\"https://aws.amazon.com/glue/pricing/\">AWS Glue 가격 페이지</a>를 참조하세요.</p>\n<hr>\n<h2>AWS Glue 시작하기</h2>\n<h3>AWS Glue 설정</h3>\n<ol>\n<li><strong>AWS 콘솔 로그인:</strong> AWS 계정으로 로그인하고 AWS 관리 콘솔로 이동합니다.</li>\n<li><strong>AWS Glue 서비스 선택:</strong> 서비스 목록에서 AWS Glue를 선택합니다.</li>\n<li><strong>필수 IAM 권한 설정:</strong> AWS Glue를 사용하기 위해 필요한 IAM 역할과 권한을 설정합니다.</li>\n</ol>\n<h3>Glue Crawler 생성</h3>\n<ol>\n<li><strong>Crawler 생성:</strong> AWS Glue 콘솔에서 \"Crawlers\" 메뉴로 이동하여 새 크롤러를 생성합니다.</li>\n<li><strong>데이터 소스 지정:</strong> 크롤러가 스캔할 데이터 소스를 지정합니다. 예를 들어, Amazon S3 버킷을 선택할 수 있습니다.</li>\n<li><strong>대상 데이터 카탈로그 선택:</strong> 크롤러가 메타데이터를 저장할 데이터 카탈로그 데이터베이스를 선택합니다.</li>\n<li><strong>스케줄 설정:</strong> 크롤러가 주기적으로 실행될 스케줄을 설정합니다.</li>\n<li><strong>Crawler 실행:</strong> 크롤러를 실행하여 데이터 소스의 메타데이터를 Data Catalog에 등록합니다.</li>\n</ol>\n<h3>ETL Job 생성</h3>\n<ol>\n<li><strong>Job 생성:</strong> AWS Glue 콘솔에서 \"Jobs\" 메뉴로 이동하여 새 ETL Job을 생성합니다.</li>\n<li><strong>Job 이름 및 IAM 역할 설정:</strong> Job의 이름과 실행할 IAM 역할을 지정합니다.</li>\n<li><strong>데이터 소스 및 타겟 설정:</strong> 데이터 소스와 변환 후 데이터를 저장할 타겟을 설정합니다.</li>\n<li><strong>스크립트 편집:</strong> AWS Glue Studio를 사용하여 시각적으로 ETL 작업을 설계하거나, 직접 Python/Scala 코드를 작성합니다.</li>\n<li><strong>Job 실행:</strong> ETL Job을 실행하여 데이터를 변환하고 타겟에 저장합니다.</li>\n</ol>\n<h3>Job 모니터링 및 관리</h3>\n<ol>\n<li><strong>Job 모니터링:</strong> AWS Glue 콘솔에서 실행 중인 Job의 상태를 모니터링하고 로그를 확인할 수 있습니다.</li>\n<li><strong>에러 관리:</strong> 실패한 Job의 에러 로그를 분석하여 문제를 해결합니다.</li>\n<li><strong>Job 최적화:</strong> Job의 성능을 최적화하기 위해 리소스 할당과 변환 로직을 조정합니다.</li>\n</ol>\n<hr>\n<h2>다른 ETL 도구와의 비교</h2>\n<h3>AWS Glue vs. Apache Spark</h3>\n<ul>\n<li><strong>AWS Glue:</strong> 완전관리형 서비스로, 서버 관리 없이 ETL 작업을 수행할 수 있습니다. Spark 기반의 강력한 변환 기능을 제공하며, AWS 생태계와의 통합이 뛰어납니다.</li>\n<li><strong>Apache Spark:</strong> 오픈 소스 클러스터 컴퓨팅 프레임워크로, 유연성과 확장성이 뛰어나지만, 인프라 관리가 필요합니다. 사용자 정의가 용이하지만, AWS Glue에 비해 설정과 관리가 복잡할 수 있습니다.</li>\n</ul>\n<h3>AWS Glue vs. AWS Data Pipeline</h3>\n<ul>\n<li><strong>AWS Glue:</strong> 서버리스 ETL 서비스로, 데이터 카탈로그링과 자동 스키마 추론 기능을 제공합니다. Spark 기반의 강력한 변환 기능을 지원합니다.</li>\n<li><strong>AWS Data Pipeline:</strong> 데이터 이동 및 변환을 위한 워크플로우 서비스로, 복잡한 데이터 파이프라인을 시각적으로 설계할 수 있습니다. AWS Glue보다 유연한 작업 스케줄링과 다양한 작업 타입을 지원하지만, 서버 관리를 필요로 할 수 있습니다.</li>\n</ul>\n<h3>AWS Glue vs. 기타 클라우드 ETL 서비스</h3>\n<ul>\n<li><strong>Google Cloud Dataflow:</strong> 완전관리형 스트리밍 및 배치 데이터 처리 서비스로, Apache Beam을 기반으로 합니다. 실시간 데이터 처리가 강점입니다.</li>\n<li><strong>Azure Data Factory:</strong> 완전관리형 데이터 통합 서비스로, 다양한 데이터 소스 간의 데이터 이동과 변환을 지원합니다. Microsoft 생태계와의 통합이 뛰어납니다.</li>\n<li><strong>AWS Glue의 강점:</strong> AWS 생태계와의 깊은 통합, 서버리스 아키텍처, 자동화된 데이터 카탈로그링 기능 등이 주요 강점입니다.</li>\n</ul>\n<hr>\n<h2>모범 사례</h2>\n<ol>\n<li><strong>Data Catalog 활용:</strong> Data Catalog를 적극 활용하여 데이터 메타데이터를 체계적으로 관리하고, 데이터 검색을 용이하게 합니다.</li>\n<li><strong>크롤러 스케줄링 최적화:</strong> 데이터 소스의 변경 빈도에 따라 크롤러 실행 스케줄을 최적화하여 최신 메타데이터를 유지합니다.</li>\n<li><strong>ETL Job 최적화:</strong> Job의 성능을 최적화하기 위해 필요한 리소스를 적절히 할당하고, 불필요한 변환 작업을 최소화합니다.</li>\n<li><strong>로그 및 모니터링 설정:</strong> AWS Glue의 로그와 모니터링 기능을 활용하여 ETL 작업의 상태를 지속적으로 모니터링하고, 오류를 신속하게 대응합니다.</li>\n<li><strong>보안 및 권한 관리:</strong> IAM 역할과 정책을 통해 AWS Glue 리소스에 대한 접근 권한을 철저히 관리하여 데이터 보안을 강화합니다.</li>\n</ol>\n<hr>\n<h2>참고 자료 및 추가 학습</h2>\n<ul>\n<li><a href=\"https://docs.aws.amazon.com/glue/index.html\">AWS Glue 공식 문서</a></li>\n<li><a href=\"https://docs.aws.amazon.com/glue/latest/dg/start.html\">AWS Glue 시작하기 가이드</a></li>\n<li><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#components-data-catalog\">AWS Glue Data Catalog 소개</a></li>\n<li><a href=\"https://aws.amazon.com/glue/pricing/\">AWS Glue Pricing</a></li>\n<li><a href=\"https://aws.amazon.com/glue/features/\">AWS Glue vs Apache Spark</a></li>\n<li><a href=\"https://aws.amazon.com/glue/getting-started/\">AWS Glue 튜토리얼</a></li>\n</ul>","categories":["AWS","Glue","ALL"],"date":"September 18, 2024","description":"AWS 기능들을 활용하여 파이프라인 구축해보기","id":"51db4bb3-acc5-593e-aec0-a70144c776d5","keywords":["Pipeline","AWS","Cloud","Glue"],"slug":"/my-first-article/About-AWS-Glue-1/","title":" ⚙️ [AWS] AWS Glue ","readingTime":{"text":"18 min read"}},{"banner":{"alt":"Pipeline","caption":"Photo by <u><a href=\"https://www.ibm.com/kr-ko/topics/data-pipeline\">About Pipeline</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACP0lEQVR42m1Q/0/TQBztH+rP/k5CYqLhJyQmRMVEQGNCUDFBUYEoQUicxWVMcBTm6BilUNaNbt3t7nptd/2yu/Y8wahR3y+fT+4+7+W9pwghXOCDoycJ0uIBDI6mcvETef5r/T8UeUMoA6W7aafCeRw2fpMlW2RczjiObduOosjzCGPsot1yHCdOEoUPCOhCunOPt7fzPM6t+/IUYQ/7STLMrkQCn6iqWi5vb30u7u1pJ4axr2mFT6riNTfOtBc9fTZCVcEjZs1AhKMBoWAfORVBDZGl0gv/A1KOsaE0oqTmFK6OI31WZH6WIm7eQSTJ3HdJbeSitsD0UdFbu4zAxT9dyMzCCxI/ZvIzjVBff4xCQawVYBVgKMJOkVjLlwT+V1tSQBk250F9LqhPSZp/PJsfXHPrz2h1NOqoKMiH3U1+8fJH9CTIuCwrzS5x5V/h3n6v+RXXbvcbzwO3Kmi9d6a6Vhl5lFDRPS2AkxUvEIZWlC+QpAgTCCFCCGOs8DzzYuHtTCNDJZFIEgZgwIdUuIueuRgd3uL6CDpZwLvXPeOp6C54/SYA/asgCvbogIlgd7rfUCHhdq2SMpHgQ6JPxkGLhRanzYhYKTGBcwwPJqBdkl0ElPs0U1ivEPV2EmMCNx7B849AGwudYno+lzbGaf8bg6UclwT5EjmboFkmtQn/eAbbW9ReHbTWlLj9ltgfUv0mrk1Cc1W4r3x7PTYfpvoNYq8n7aXMWeKd16E5j07fx/WxyHgQtjaE+ybrLn8HI2uE5GoA3oEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/7b92fac4e49287f4f0bf45aaed4eb8e7/bc51f/pipeline-cover.png","srcSet":"/static/7b92fac4e49287f4f0bf45aaed4eb8e7/41200/pipeline-cover.png 165w,\n/static/7b92fac4e49287f4f0bf45aaed4eb8e7/f979a/pipeline-cover.png 330w,\n/static/7b92fac4e49287f4f0bf45aaed4eb8e7/bc51f/pipeline-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/7b92fac4e49287f4f0bf45aaed4eb8e7/322ad/pipeline-cover.webp 165w,\n/static/7b92fac4e49287f4f0bf45aaed4eb8e7/de3b3/pipeline-cover.webp 330w,\n/static/7b92fac4e49287f4f0bf45aaed4eb8e7/2b2b5/pipeline-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>💬 데이터 파이프라인 'A - Z'</h1>\n<p>오늘은 데이터 파이프라인을 제작해보려고 합니다.</p>\n<p>여러가지 기술들을 총합하여 데이터 레이크, 마트등을 구성해보고</p>\n<p>데이터 추출, 변환 등으로 데이터 분석을 할 수 있는 대시보드까지 구현해보도록 하겠습니다.</p>\n<p>일단 첫 포스팅의 목표는 데이터를 수집해 kafka 전송, spark 데이터 변환까지가 1차 목표입니다.</p>\n<p>스트리밍 파이프라인과 배치 파이프라인을 제작하여 관리하고 모니터링까지 진행해보겠습니다 🫡</p>\n<h2>데아터 선택</h2>\n<p>스트리밍 : 시스템 로그 + ??</p>\n<p>배치 : API , 크롤링</p>\n<p>일단은 UPBIT에서 API를 통해 전체 market 데이터를 얻은 후에 지금까지의 모든 일별 데이터를 DB(postgres)에 저장했습니다.</p>\n<p>이후 하루치의 데이터는 배치를 통해 가져오게 했습니다.</p>\n<p>또한 로깅을 통해 log 기록을 남겨 저장하고 있습니다.</p>","categories":["Pipeline","ALL"],"date":"September 14, 2024","description":"데이터 파이프라인 제작.","id":"f146513c-0de6-5fa9-aebf-07f52bdfa37a","keywords":["Pipeline","Data Warehouse","Data Lake"],"slug":"/my-first-article/About-Data-Pipeline/","title":" 🌟 [Pipeline] 데이터 파이프라인 제작 ","readingTime":{"text":"2 min read"}},{"banner":{"alt":"AWS","caption":"Photo by <u><a href=\"https://docs.aws.amazon.com/\">AWS docs</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAYklEQVR42mP4TwFgGKya/4EAWZp//fr19evXHz9+kKAZYte3b99Wb9ixftvBzdv3fvv6FS5OlM1///z+/OTstydHvz05/vf7B4ippPj57/d/vz78+/OdHD//QyKHVjzj1wwA9oTIz3J7TVcAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/ef22cb543f0aa6abff95ef14f62f3814/bc51f/aws-cover.png","srcSet":"/static/ef22cb543f0aa6abff95ef14f62f3814/41200/aws-cover.png 165w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/f979a/aws-cover.png 330w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/bc51f/aws-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/ef22cb543f0aa6abff95ef14f62f3814/322ad/aws-cover.webp 165w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/de3b3/aws-cover.webp 330w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/2b2b5/aws-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>☁️ AWS 에서 Pipeline 구축 실습을 해보자 - 1</h1>\n<p>오늘은 간단한 AWS pipeline을 구축해보겠습니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws-practice-1/workflow.png\" alt=\"Component\" width=\"650\">\n<p>실습의 목적은 로우 데이터의 추출, 변환, 적재입니다.</p>\n<p>참고한 내용은 <a href=\"https://catalog.us-east-1.prod.workshops.aws/workshops/44c91c21-a6a4-4b56-bd95-56bd443aa449/en-US/lab-guide/ingest\">AWS workshop</a>에서 보실 수 있습니다.</p>\n<p>사이트에서 필수 조건을 보면</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1. AdminstratorAccess를 사용하여 AWS 계정에 액세스할 수 있어야 합니다.\n\n2. 이 랩은 us-east-1 지역 에서 실행되어야 합니다.\n\n3. 이 가이드의 링크를 따라가서 새 탭에서 여는 것이 가장 좋습니다.\n\n4. 최신 브라우저에서 이 랩을 실행하세요</code></pre></div>\n<p>이런식으로 되어있는데 왜 리전은 저기서 하라는건지 모르겠지만 나는 그렇게 하기 싫어서 변형을 좀 하려고 합니다.</p>\n<p>일단 하라는대로 진행해보고 그 후에 변형할 예정입니다.</p>\n<h2>버킷 생성</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws-practice-1/create-bucket.png\" alt=\"버킷 생성\" width=\"650\">\n<p>s3 콘솔에서 버킷을 생성해줍니다.</p>\n<p>버킷 이름은 자유</p>\n<h3>data 폴더 생성</h3>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws-practice-1/folder-data.png\" alt=\"data 폴더 생성\" width=\"650\">\n<h3>reference_data 폴더 생성</h3>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws-practice-1/reference-data.png\" alt=\"reference_data 폴더 생성\" width=\"650\">\n<p>둘 다 이름은 자유롭게 하셔도 됩니다.</p>\n<p>저는 그냥 가이드 내용대로 진행했습니다.</p>\n<h3>track_list.json 업로드</h3>\n<p>Data 다운로드 : <a href=\"https://static.us-east-1.prod.workshops.aws/public/252b2158-4ee1-410c-b074-58190ec31cd6/static/data/tracks_list.json\">track_list.json</a></p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws-practice-1/track_list.png\" alt=\"track_list.json\" width=\"650\">\n<p>파일은 샘플 데이터로 음악 트랙 목록이 json 형식으로 되어있습니다.</p>\n<p>양은 얼마 안됩니다.</p>\n<p>데이터의 스키마를 이해하고 ETL 작업에서 필요한 변환 로직을 구성하는데 쓰일 수 있습니다.</p>\n<h2>Kinesis Firehose 생성</h2>\n<p><a href=\"https://us-east-1.console.aws.amazon.com/firehose/home?region=us-east-1#/home\">Kinesis Firehose</a> 여기에서 배달 스트림 생성합니다.</p>\n<p>미국 리전으로 되어있으니 만약 다른 리전인 경우 변경해서 사용하시면 됩니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws-practice-1/create-stream.png\" alt=\"create stream\" width=\"650\">\n<p>배달지와 목적지를 선택을 하는데요</p>\n<p><strong>출처: Direct PUT</strong></p>\n<p><strong>목적지: Amazon S3</strong></p>\n<p>소스가 Direct PUT인 이유는 실시간 데이터가 아니라 저희가 직접 올리기 때문인 거 같습니다.</p>\n<p>S3 버킷 접두사: data/raw/ 뒤에 '/'가 중요하다고 하는데, 붙이지 않으면 엉뚱한 곳에 데이터를 복사한다고 합니다. -> data 폴더에 raw 폴더 생성</p>\n<p>버퍼 조건은 그냥 저장의 간격이라고 생각하시면 됩니다.</p>\n<h2>더미 데이터 생성</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws-practice-1/create-stack.png\" alt=\"create stack\" width=\"650\">\n<p>왜 리전을 us-east-1 하라고 했나 그 이유가..</p>\n<p>Cognito를 구성하는 클라우드 형성 스택 때문에 리전 변경하라고 한 거 같습니다. <a href=\"https://aws-kdg-tools-us-east-1.s3.amazonaws.com/cognito-setup.yaml\">cognito-setup.yaml</a></p>\n<p>us-east-1 리전에서 사용하도록 설계되어서 서울 리전에서 사용하려고 하면 오류가 발생합니다.</p>\n<p>뭐 다운로드를 받아서 수정해서 사용해도 될 수 있긴 합니다. 아님 리전을 us-east-1로 변경하던지.</p>\n<p>근데 사실 위에 yml 파일은  Kinesis Data Generator를 이용해 더미데이터를 만들어서 Fire hose로 전달하기 위함인데</p>\n<p>굳이 더미데이터를 사용하지 않을거면 이 기능을 이용할 필요가 없을 거 같습니다.</p>\n<p>방향을 약간 틀어서 진행해보려고 합니다.</p>\n<p>감사합니다</p>\n<p>다음 포스트에서 계속! 🫡</p>","categories":["AWS","Streaming","ALL"],"date":"September 12, 2024","description":"AWS 기능들을 활용하여 파이프라인 구축해보기","id":"7bf5b0ba-0689-5178-8629-3c62a7de64e8","keywords":["Pipeline","AWS","Cloud"],"slug":"/my-first-article/About-AWS-Pipeline-2/","title":" 🔫 AWS Pipeline 구축 실습 - 1 ","readingTime":{"text":"5 min read"}},{"banner":{"alt":"AWS","caption":"Photo by <u><a href=\"https://docs.aws.amazon.com/\">AWS docs</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAYklEQVR42mP4TwFgGKya/4EAWZp//fr19evXHz9+kKAZYte3b99Wb9ixftvBzdv3fvv6FS5OlM1///z+/OTstydHvz05/vf7B4ippPj57/d/vz78+/OdHD//QyKHVjzj1wwA9oTIz3J7TVcAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/ef22cb543f0aa6abff95ef14f62f3814/bc51f/aws-cover.png","srcSet":"/static/ef22cb543f0aa6abff95ef14f62f3814/41200/aws-cover.png 165w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/f979a/aws-cover.png 330w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/bc51f/aws-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/ef22cb543f0aa6abff95ef14f62f3814/322ad/aws-cover.webp 165w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/de3b3/aws-cover.webp 330w,\n/static/ef22cb543f0aa6abff95ef14f62f3814/2b2b5/aws-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>☁️ AWS 에서 Pipeline 구축해보자 - 1</h1>\n<p>오늘은 AWS의 기능들을 사용하여 파이프라인을 구축해보려고 합니다.</p>\n<p>AWS Glue, AWS Step Functions, Amazon ECS / EKS, AWS Lambda 등등</p>\n<p>많이 들어봤지만 사용해보지 않았던 기술들을 공부하고 사용험으로</p>\n<p>많은것을 배우고 싶습니다.</p>\n<p>이 포스트는 단계별로 작성될 예정입니다.</p>\n<h1>🚙 Socar</h1>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws/aws-component.png\" alt=\"Socar\" width=\"600\">\n<p>위의 이미지는 Socar에서 FMS(차량관제시스템)을 효율적으로 제작하기 위해 만든 파이프라인의 컴포넌트입니다.</p>\n<p>위와 같이 <strong>스트리밍 파이프라인</strong>에서 <strong>IoT core, MSK(Managed Streaming for Kafka Service), Kafka Connect</strong>등을 이용해서 구성하고</p>\n<p>실시간 데이터 조회 목적으로 <strong>DynamoDB</strong>, 데이터 레이크 목적으로 <strong>S3</strong>를 사용하고 있습니다.</p>\n<p>또한 <strong>Lambda, Redshift,  Airflow, Glue Data Catalog</strong> 등의 기술을 활용하여 배치 파이프라인도 운영중입니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/aws/data-flow.png\" alt=\"flow\" width=\"600\">\n<p>결국이 파이프라인의 목적은 데이터를 데이터베이스/스토리지에 저장하는 것인데 데이터의 흐름을 보면 위와 같습니다.</p>\n<ol>\n<li>\n<p>차량에서 다양한 상태의 데이터를 수집합니다.</p>\n</li>\n<li>\n<p>수집되는 데이터는 발송 주기에 맞춰 IoT Core로 전송합니다.</p>\n</li>\n<li>\n<p>IoT Core 메시지 브로커에 저장된 메시지는 라우팅 규칙에 따라 상위 주제별로 Kafka Topic으로 라우팅 됩니다.</p>\n</li>\n<li>\n<p>Kafka Topic의 각 파티션에 저장된 메시지는 Kafka Connect를 통해 데이터 싱크(DynamoDB, S3)로 적재됩니다. (Redis는 필터링을 하는 Kafka Consumer를 통해 적재됩니다)</p>\n</li>\n<li>\n<p>S3에 적재된 Json 포맷의 객체는 Lambda를 통해 분류/변형 후 S3에 적재됩니다 (Redshift, Athena 쿼리에 적합한 형태로 적재됩니다)</p>\n</li>\n<li>\n<p>Airflow로 스케줄링된 Redshift 쿼리를 통해 데이터를 집계하여 RDS(데이터 마트)에 저장됩니다.</p>\n</li>\n</ol>\n<p><a href=\"https://tech.socarcorp.kr/data/2023/01/17/build-fms-data-pipeline-1.html\">Socar-Tech-Blog</a></p>\n<h1>⚙️ 기능 소개 (모든 기능 아님)</h1>\n<ol>\n<li>\n<p>AWS CodePipeline</p>\n<p>•\t설명: AWS CodePipeline은 CI/CD 파이프라인 자동화를 위한 서비스입니다. 애플리케이션 코드 변경 사항을 자동으로 빌드, 테스트, 배포하는 파이프라인을 구성할 수 있습니다.\n•\t사용처: DevOps, 애플리케이션 배포 자동화.</p>\n</li>\n<li>\n<p>AWS CodeBuild</p>\n<p>•\t설명: AWS CodeBuild는 소스 코드 빌드를 자동화하는 서비스로, 테스트와 빌드를 수행하여 패키지를 생성할 수 있습니다.\n•\t사용처: 빌드 자동화, 테스트, 애플리케이션 패키징.</p>\n</li>\n<li>\n<p>AWS CodeDeploy</p>\n<p>•\t설명: AWS CodeDeploy는 애플리케이션 배포를 자동화하는 서비스로, EC2, Lambda, 또는 온프레미스 서버에 배포할 수 있습니다.\n•\t사용처: 다양한 환경에서의 애플리케이션 배포 자동화.</p>\n</li>\n<li>\n<p>AWS CodeCommit</p>\n<p>•\t설명: AWS CodeCommit은 Git 기반의 소스 코드 저장소 서비스입니다. 코드 버전 관리를 지원하며, 파이프라인의 소스 스테이지로 사용할 수 있습니다.\n•\t사용처: 소스 코드 저장소 및 버전 관리.</p>\n</li>\n<li>\n<p>AWS Lambda</p>\n<p>•\t설명: 서버리스 컴퓨팅을 위한 서비스로, 이벤트 기반으로 코드를 실행할 수 있습니다. CI/CD 파이프라인에서 특정 작업을 처리할 때 활용될 수 있습니다.\n•\t사용처: 서버리스 애플리케이션, 이벤트 기반 트리거 처리.</p>\n</li>\n<li>\n<p>Amazon S3</p>\n<p>•\t설명: AWS의 오브젝트 스토리지 서비스로, 빌드된 아티팩트, 로그, 데이터 파일 등을 저장하는 데 사용할 수 있습니다.\n•\t사용처: 빌드 결과 저장, 정적 웹 호스팅, 데이터 백업.</p>\n</li>\n<li>\n<p>Amazon EC2</p>\n<p>•\t설명: AWS의 가상 서버 서비스로, 애플리케이션 실행 환경을 제공하며 파이프라인의 배포 대상 서버로 사용할 수 있습니다.\n•\t사용처: 애플리케이션 실행, 테스트 환경 구축.</p>\n</li>\n<li>\n<p>Amazon ECS / EKS</p>\n<p>•\t설명: ECS는 AWS의 컨테이너 오케스트레이션 서비스로, Docker 컨테이너를 관리하고 배포할 수 있습니다. EKS는 Kubernetes 기반의 오케스트레이션을 제공합니다.\n•\t사용처: 컨테이너화된 애플리케이션 배포 및 관리.</p>\n</li>\n<li>\n<p>Amazon RDS</p>\n<p>•\t설명: AWS의 관리형 데이터베이스 서비스로, MySQL, PostgreSQL, Oracle, SQL Server 등 다양한 데이터베이스를 운영할 수 있습니다. 데이터 파이프라인에서 중요한 데이터 저장소로 활용됩니다.\n•\t사용처: 데이터베이스 배포 및 관리.</p>\n</li>\n<li>\n<p>AWS CloudFormation</p>\n<p>•\t설명: AWS 리소스의 프로비저닝을 코드로 정의하여 인프라를 자동으로 배포하고 관리할 수 있는 서비스입니다.\n•\t사용처: 인프라 프로비저닝 자동화, IaC(Infrastructure as Code).</p>\n</li>\n<li>\n<p>AWS Step Functions</p>\n<p>•\t설명: 서버리스 상태 머신을 이용해 복잡한 워크플로우를 관리할 수 있는 서비스입니다. 여러 AWS 서비스 간의 조정 및 작업 흐름 관리를 자동화할 수 있습니다.\n•\t사용처: 복잡한 워크플로우 및 데이터 파이프라인 관리.</p>\n</li>\n<li>\n<p>Amazon CloudWatch</p>\n<p>•\t설명: AWS 리소스 및 애플리케이션을 모니터링하고 로그를 분석하는 데 사용하는 서비스입니다. 파이프라인에서 발생하는 이벤트나 오류를 모니터링하고 대응할 수 있습니다.\n•\t사용처: 리소스 모니터링, 로그 분석, 이벤트 알림.</p>\n</li>\n<li>\n<p>Amazon SNS (Simple Notification Service)</p>\n<p>•\t설명: 알림 및 메시징 서비스로, 파이프라인 내의 상태 변화, 오류 알림 등을 발송하는 데 사용됩니다.\n•\t사용처: 알림, 메시징, 이벤트 트리거.</p>\n</li>\n<li>\n<p>Amazon SQS (Simple Queue Service)</p>\n<p>•\t설명: 메시지 큐 서비스로, 비동기적으로 작업을 처리하거나 서비스 간 메시지를 전달할 수 있습니다.\n•\t사용처: 비동기 작업 처리, 대기열을 통한 작업 분배.</p>\n</li>\n<li>\n<p>AWS Glue</p>\n<p>•\t설명: 데이터 통합 및 ETL(Extract, Transform, Load) 작업을 자동화할 수 있는 서비스입니다. 데이터 파이프라인에서 복잡한 데이터 변환 작업에 사용됩니다.\n•\t사용처: ETL 작업, 데이터 파이프라인.</p>\n</li>\n<li>\n<p>Amazon Kinesis</p>\n<p>•\t설명: 실시간 스트리밍 데이터를 처리하고 분석할 수 있는 서비스입니다. 실시간 로그, 메트릭, 데이터 스트림을 처리하는 데 활용됩니다.\n•\t사용처: 실시간 데이터 스트리밍, 로그 및 메트릭 처리.</p>\n</li>\n<li>\n<p>AWS Fargate</p>\n<p>•\t설명: 서버리스 방식으로 컨테이너를 실행하는 서비스로, ECS 및 EKS에서 사용됩니다. 서버 관리를 신경 쓰지 않고 컨테이너를 실행할 수 있습니다.\n•\t사용처: 서버리스 컨테이너 실행, 애플리케이션 배포.</p>\n</li>\n<li>\n<p>AWS SAM (Serverless Application Model)</p>\n<p>•\t설명: 서버리스 애플리케이션을 정의하고 배포하는 데 사용되는 서비스로, Lambda, API Gateway 등을 포함한 인프라를 코드로 관리할 수 있습니다.\n•\t사용처: 서버리스 애플리케이션 개발 및 배포.</p>\n</li>\n<li>\n<p>AWS App Runner</p>\n<p>•\t설명: 컨테이너화된 웹 애플리케이션을 간단하게 빌드하고 배포할 수 있는 서비스입니다.\n•\t사용처: 컨테이너 기반 웹 애플리케이션 배포.</p>\n</li>\n</ol>\n<h1>🧑🏻‍💻 시작하며..</h1>\n<p>이렇게 다양한 기업들에서 Cloud 환경을 사용하고 저도 AWS를 자주 사용하는 입장으로</p>\n<p>AWS의 기능들을 활용해서 요구사항에 맞는 파이프라인을 제작해보겠습니다.</p>","categories":["AWS","Streaming","ALL"],"date":"September 11, 2024","description":"AWS 기능들을 활용하여 파이프라인 구축해보기","id":"1a97f1eb-b4b8-56f4-a7e2-ff6b24febc02","keywords":["Pipeline","AWS","Cloud"],"slug":"/my-first-article/About-AWS-Pipeline-1/","title":" 🔫 AWS Pipeline 구축해보기 - 1 ","readingTime":{"text":"11 min read"}},{"banner":{"alt":"Airflow","caption":"Photo by <u><a href=\"https://airflow.apache.org\">Airflow</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACO0lEQVR42oWS3WvTUBTAb+dk4qNPIvgB/gH1A31xyEAnExR88GH4MJ8EBYdPIoh78MUnBd2LWVs2sV1bV9subl0qQjs6ZpdEU21T027DNV0b1vXbmJI1TeNJU3Gg4IHce88593dzvpBaLGggqqrqm1ZTWk9zRVltw7n9l/y+1RUkXDLLVMTgwX2Vy5z9utYltf8Iypw6tD1k1jhG0bSRNI8WmespHhybuVwyyRa2trLZrCAIqRS3mk7zPN9oNIyndbh0+uDm5ZPag+GxcBiRKbT05d73PDhI+pPf5/XD99aDz86Gw6GlSMTpnC6Xy39gZvCMds2ceXz7WDCM6DTAju0qOHYURZblSqUCqyj+aDabakfAUq/X1U6OaPihEx+7c3SRRJROHqa5UlPR4aYy/uJ5uVyq1WpsImEkub6+Zre/9vm8Ql6PDvXho3tj0T3fiiYysf/z6vuaaNxLJpNWi+Xj8jLESdNUIDBPEMTU5CSGvbRMYBzHgQX1fDjfR4weyFaOCDsLkgyY0umHzWYNBgmb1cqy7Nzcu2mHQ5KkGMOsrEQpinK5nHBAJv+V3pkBFLh1M7WgR6u2YC0UChMYFo/H3W4X1CwUCnk8HkiSYRggo9EoWHAcR8g/YPIM9novojfnHrGvjJgl6We1qpcNGgMPiaIIKsCgwv9BVRQly/MI25jf5xtC7v4edz+yn7gbGzd4tW1UV/3neHRbpddQzD9Lz9wgn1yI3D9OjExtBMHY6sS/ezZ3q8ZA/wIAVki+U9pyEAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png","srcSet":"/static/de7402d2a87d0256451392f6590d157b/182cc/airflow-profile.png 91w,\n/static/de7402d2a87d0256451392f6590d157b/2c6ad/airflow-profile.png 181w,\n/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png 362w","sizes":"(min-width: 362px) 362px, 100vw"},"sources":[{"srcSet":"/static/de7402d2a87d0256451392f6590d157b/98c2e/airflow-profile.webp 91w,\n/static/de7402d2a87d0256451392f6590d157b/483ec/airflow-profile.webp 181w,\n/static/de7402d2a87d0256451392f6590d157b/def5b/airflow-profile.webp 362w","type":"image/webp","sizes":"(min-width: 362px) 362px, 100vw"}]},"width":660,"height":399.2817679558011}}}},"body":"<h1>🌬️ Apache Airflow: 공부하기 - 3</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-profile.png\" alt=\"Airflow\"></p>\n<h1>📉 데이터베이스 저장 및 테이블 생성</h1>\n<h2>🌟 프로젝트 개요</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-ing/monitoring.png\" alt=\"monitoring\" width=\"600\">\n<p>간단한 Airflow를 실습합니다.</p>\n<ul>\n<li>\n<p><a href=\"https://docs.upbit.com/docs/upbit-quotation-restful-api\">Upbit-API</a>에서 서로 다른 데이터를 받아 합치는 걸 실습.</p>\n</li>\n<li>\n<p>전체 마켓 데이터 (market) , 그 마켓에 대한 가격 정보 (Price)를 가져와서 둘이 합치는 것.</p>\n</li>\n<li>\n<p>market에 대한 이름을 추출해서 Price를 구하는 코드의 파라미터로 전달하여 데이터 수신.</p>\n</li>\n</ul>\n<h1>✅ 진행 과정</h1>\n<h2>UPBIT에서 API를 전송 받음.</h2>\n<h2>market과 price를 각각의 테이블에 저장한 뒤 join.</h2>\n<h2>완료 시 슬랙으로 알림.</h2>\n<h3>📌 Market Data Preview</h3>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-3/market-upbit.png\" alt=\"market\" width=\"600\">\n<ul>\n<li>market 데이터를 받아서 테이블 생성한 뒤 저장.</li>\n</ul>\n<h3>📌 Prices Data Preview</h3>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-3/price-upbit.png\" alt=\"price\" width=\"600\">\n<ul>\n<li>prices 데이터 저장</li>\n</ul>\n<h3>📌 Combined Data Preview</h3>\n<ul>\n<li>합친 새로운 데이터 생성</li>\n</ul>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-3/combined-upbit.png\" alt=\"combined\" width=\"600\">\n<ul>\n<li>코드는 <a href=\"https://github.com/jms0522/Streaming-Data/blob/main/airflow/dags/upbit_data_pipeline.py\">Upbit Pipeline</a>에서 확인 가능합니다.</li>\n</ul>\n<h3>📌 Airflow Preview</h3>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-3/success.png\" alt=\"dags\" width=\"600\">\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-3/dag-workflow.png\" alt=\"workflow\" width=\"600\">\n<h2>📊 사용된 기술 스택</h2>\n<ul>\n<li><strong>Docker</strong>: 애플리케이션을 컨테이너로 패키징하여 독립적으로 실행</li>\n<li><strong>Apache Airflow</strong>: 워크플로우 관리 플랫폼</li>\n<li><strong>PostgreSQL</strong>: 관계형 데이터베이스 관리 시스템</li>\n<li><strong>Postgres Hook</strong>: Airflow Connections</li>\n<li><strong>Slack Hook</strong>: Airflow Connections</li>\n<li><strong>UPbit API</strong> : API</li>\n</ul>\n<h1>🧑🏻‍💻 느낀점</h1>\n<ul>\n<li>간단한 실습을 진행해봤는데요, 좀 더 효율적인 파이프라인을 고민해봐야겠습니다.😅</li>\n</ul>","categories":["Airflow","ALL"],"date":"September 10, 2024","description":"Airflow 간단한 실습입니다.","id":"1dc62146-1f35-50be-b05a-f7e6a128226e","keywords":["Workflow","Pipeline","Practice","Blog","Airflow","Crawling"],"slug":"/my-first-article/About-Airflow-Practice-3/","title":" 🌟 [Airflow] Airflow Practice-3 ","readingTime":{"text":"2 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org/streaming/\">Spark Streaming</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABKklEQVR42o2S3UvDMBTF94crDp+UOR8m6iaCU5ko3Z6GdtBG6frdPYgvE6Z0XSuKlcZ+0I1ikxrtEMfasvNwSeD+cpKTW0pWEcYJRj91UaWszgURDKN4mcyA58C/LalB72x6f/flvM1vUeDseV4YhtCBvu8H09mMOY802r3aiZ4ffmGUAacm9rvdPDlVFRWAW4EXJEnhLo4vW62Absa2leucwpb1Ut7Y3N6q1A8be7X9amW3WjtYWy9/vFoJinHem1MYQkgMGYZ1HMcYG6PR08SYmKb56bq5gaWkruttqjMcPvJ9XlU1WVYUWQEsEAVpoA3IQX+dmfCYojqiKNM3PZYB3e41x/XJQhLlRv2IBEF6EEJFaWeK5B9FUdE/E3+UI7zKkOQPKF7mvwHvMaPrfenbCQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png","srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/41200/spark-st-cover.png 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/f979a/spark-st-cover.png 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/322ad/spark-st-cover.webp 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/de3b3/spark-st-cover.webp 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/2b2b5/spark-st-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>💬 Apache Spark Streaming Practice</h1>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/elk-kafka-spark.png\" alt=\"Total\" width=\"600\">\n<p>오늘은 <strong>Spark Streaming</strong>을 중점적으로 활용한 실시간 처리 파이프라인을 구성하는 실습을 진행보겠습니다.</p>\n<p>아직 진행중이라 구성은 조금씩 바뀔 수 있습니다.</p>\n<p>Filebeat로 적재를 할지 그냥 Logstash를 쓸지 아님 다른 stack 을 쓸 수 있어요.</p>\n<p>근데 지금은 일단 Logstash로 진행중입니다.</p>\n<p><strong>전체적인 구성</strong>은 위에 이미지와 동일합니다.</p>\n<p><strong>Logstash</strong>는 실시간으로 발생하는 데이터를 수집해서 <strong>Kafka</strong>로 보내고</p>\n<p><strong>Kafka</strong>에서 메시지를 받고, <strong>Spark Streaming</strong>을 활요해서</p>\n<p><strong>Jupyter</strong>나 <strong>Zeplin</strong>에서 분석도 진행해볼 예정입니다.</p>\n<p><strong>HDFS</strong>에 데이터 저장까지 완료입니다.</p>\n<h2>🧹 Logstash</h2>\n<p>Logstash는 실시간 데이터를 수집하여 Kafka에 적재하는 역할입니다.</p>\n<p>현재 일단 Elk stack으로 사용하는게 아니라 따로 Logstash 컨테이너를 만들어 수집하도록 하고 있습니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/logstash-coll.png\" alt=\"Logs\" width=\"600\">\n<p>위의 이미지처럼 Logstash가 데이터를 읽는 모습입니다.</p>\n<p><a href=\"https://github.com/jms0522/hadoop_system/tree/main/hadoop/logstash\">logstash.conf</a>에서 코드는 보실 수 있습니다.</p>\n<p>Logstash에서 conf에서 filter를 통해 데이터를 변환해서 kafka에 적잽합니다.</p>\n<h2>🔧 Kafka</h2>\n<p>카프카는 메시징 큐의 역할로 스트리밍의 비동기 처리를 가능하게 하고, 처리 속도 차이를 흡수 할 수 있습니다.</p>\n<p>또한 스트리밍 데이터의 데이터 유실을 방지할 수 있어 브로커로 두고 여기서 Spark를 통해 분석을 할 수 있습니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/kafka_logs.png\" alt=\"logs\" width=\"600\">\n<p>위의 이미지처럼 카프카에 실시간으로 로그 데이터가 들어오는 걸 확인할 수 있다.</p>\n<h2>📊 Spark Streaming</h2>\n<p>카프카 Streaming을 통해 데이터를 분석</p>\n<p>분석된 데이터는 s3, hdfs등을 통해 저장.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/kafka-hdfs.png\" alt=\"hdfs\" width=\"600\">\n<p>-- 진행중....</p>","categories":["Spark","Streaming","ALL"],"date":"September 07, 2024","description":"Spark의 Streaming 실습.","id":"80664fe7-53aa-5349-be37-1127cd091cfc","keywords":["Pipeline","Streaming","Blog","Spark"],"slug":"/my-first-article/About-Spark-Practice-2/","title":" 🌟 [Spark] Spark Practice -2 (Streaming) ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org/streaming/\">Spark Streaming</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABKklEQVR42o2S3UvDMBTF94crDp+UOR8m6iaCU5ko3Z6GdtBG6frdPYgvE6Z0XSuKlcZ+0I1ikxrtEMfasvNwSeD+cpKTW0pWEcYJRj91UaWszgURDKN4mcyA58C/LalB72x6f/flvM1vUeDseV4YhtCBvu8H09mMOY802r3aiZ4ffmGUAacm9rvdPDlVFRWAW4EXJEnhLo4vW62Absa2leucwpb1Ut7Y3N6q1A8be7X9amW3WjtYWy9/vFoJinHem1MYQkgMGYZ1HMcYG6PR08SYmKb56bq5gaWkruttqjMcPvJ9XlU1WVYUWQEsEAVpoA3IQX+dmfCYojqiKNM3PZYB3e41x/XJQhLlRv2IBEF6EEJFaWeK5B9FUdE/E3+UI7zKkOQPKF7mvwHvMaPrfenbCQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png","srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/41200/spark-st-cover.png 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/f979a/spark-st-cover.png 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/322ad/spark-st-cover.webp 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/de3b3/spark-st-cover.webp 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/2b2b5/spark-st-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>💬 Apache Spark Streaming -2</h1>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/spark-work.png\" alt=\"work\" width=\"600\">\n<h1>🌟 Apache Spark 구조적 스트리밍(Structured Streaming)</h1>\n<p>오늘은 <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Apache Spark docs</a>를 읽고 그에 대한 내용을 정리하겠습니다.</p>\n<p>지금 Kafka와 Spark를 이용해 실시간 데이터 처리를 하고 있는데 docs를 읽어보면 훨씬 도움이 될 거 같습니다.</p>\n<h2>✅ 핵심 아이디어</h2>\n<p>구조적 스트리밍에 대한 핵심 아이디어는 <strong>스트림 데이터를 지속적으로 추가되는 테이블로 처리</strong>하는 것입니다.</p>\n<p>스트리밍 데이터를 정적 테이블과 같이 표현하고 이를 무제한 입력 테이블에서 <strong>증분 쿼리</strong>로 실행하는 것, 이것이 핵심인 거 같습니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/idea.png\" alt=\"work\" width=\"600\">\n<p>밑에 사진을 보시면 **'입력되는 데이터 스트림'**을 **'입력테이블'**로 간주하여 도착하는 모든 데이터는 입력 테이블에 추가되는 Row와 같다는 것!</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/basicidea.png\" alt=\"work\" width=\"600\">\n<p>또한 입력에 대한 결과는 <strong>결과 테이블</strong>을 생성하여 입력 테이블에 추가되는 데이터를 결과 테이블에 업데이트 합니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/answertable.png\" alt=\"work\" width=\"600\">\n<p>여기서의 Output은 외부 저장소에 쓰여지는 걸 의미합니다.</p>\n<p>출력은 3가지 모드로 정의할 수 있고, 상황에 맞춰 사용하면 됩니다. (설명 생략) 🙏🏻</p>\n<ul>\n<li>전체 모드</li>\n<li>추가 모드</li>\n<li>업데이트 모드 (2.11 부터 사용 가능)</li>\n</ul>\n<p>이 예시를 보면 정확히 이해할 수 있습니다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/ex.png\" alt=\"work\" width=\"600\">\n<p>왼쪽에서 오른쪽으로 데이터가 들어오고 wordcount를 query를 통해 결과 테이블로 업데이트된 카운트를 계산하는 <strong>증분</strong> 쿼리를 진행하고 있습니다.</p>\n<p>❗️<strong>Structured Streaming</strong>에서 중요한 점은 <strong>전체 테이블을 구체화 하지 않는다</strong>는 점입니다.</p>\n<p>여기서 최신 데이터를 읽고 결과를 업데이트한 뒤 <strong>소스 데이터는 버린다</strong>는 점입니다.</p>\n<h2>✅ 결함 허용</h2>\n<p>내결함성을 1회 보장 (정확한 한 번)</p>\n<p>스파크 스트리밍은 진행상황 등을 안정적으로 추적하여 재시작 또는 재처리를 통해 오류상황을 처리하여 내결함성을 1회 보장한다.</p>\n<h2>📊 주요 측면</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/4side.png\" alt=\"work\" width=\"600\">\n<ul>\n<li>\n<p>오류, 지연 작업 발생 시 신속한 복구가 가능</p>\n<ul>\n<li>\n<p>장애 발생 시 체크포인트를 활용하여 복구</p>\n</li>\n<li>\n<p>자동으로 작업을 다른 노드로 재비치 가능 (유연한 실패 복구)</p>\n</li>\n</ul>\n</li>\n<li>\n<p>로드 밸런싱과 리소스 사용률 개선</p>\n<ul>\n<li>\n<p>자동 스케일링 : 처리량 증가시 클러스터 크기 확장 및 축소 가능</p>\n</li>\n<li>\n<p>동적 리소스 할당 : 동적으로 자원 할당 (CPU, 메모리 등)</p>\n</li>\n</ul>\n</li>\n<li>\n<p>정적 Dataset와 인터랙티브 쿼리를 사용해 스트리밍 데이터 결합</p>\n<ul>\n<li>\n<p>스트리밍 데이터와 정적 데이터 결합이 가능 -> 복합적인 분석이 가능</p>\n</li>\n<li>\n<p>실시간 처리 동시에 쿼리를 통해 실시간 분석이 가능 -> 풍부한 인사이트 제공</p>\n</li>\n</ul>\n</li>\n<li>\n<p>고급 처리 라이브러리(SQL, 머신 러닝, 그래프 처리)와 네이티브 방식으로 통합</p>\n<ul>\n<li>머신 러닝, 그래프 관계 분석 등 다양한 라이브러리로 효율적인 분석 기능 사용 가능</li>\n</ul>\n</li>\n</ul>","categories":["Spark","Streaming","ALL"],"date":"September 05, 2024","description":"Spark의 구조적 스트리밍에 대해.","id":"d934fb55-a218-576a-ba19-c2159757596a","keywords":["Pipeline","Streaming","Blog","Spark"],"slug":"/my-first-article/About-Spark-Structured-Straming-2/","title":" 🌟 [Spark] Structured Streaming -2 ","readingTime":{"text":"5 min read"}},{"banner":{"alt":"Redshift","caption":"Photo by <u><a href=\"https://aws.amazon.com/ko/Redshift/\">Redshift</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABSUlEQVR42mNgwANCV61irq//zwQXqK9nAgoyM5AJGEGEvH+/gGrmqjjr+gNx8fPPC0Ck/jOSYyCzU9P+EIOcFRtVszb9d+u/+t+1Zf9G3dxlIf///2cm2WVAwOrQfPiqQ9Oh//Lxi347d578bVO3979F1c6rQANZSXEp3ECb2u0XbKp3/FdIWPIXaOBfq8pt/02K1l8g20D7xkPX7BsPgFz4y7njxC/rmt3/LSp2XCPXQBa72h1zzMu2PFLJ3PDfY+L1/w71ex8Z5a+eAzSQBU0tEQCURIBALmqasWrm2g32Lcc2Ri24bgyRQkpKJEUMF5cEAwOfEJDFBsTsDAzi3Az8/IJkp0EgUGBgZTVkYGPTZOHkNGFj41UDiikDMRM5hgJdxJAMxG5AHADEYUAcAcRpQMxDehgCIwWIzYHYAYpdgdgWKsaOSxcARSJzd7h5ISMAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/34f6304392b84962f8fc36435f3a0998/9d984/cover.png","srcSet":"/static/34f6304392b84962f8fc36435f3a0998/f11d6/cover.png 90w,\n/static/34f6304392b84962f8fc36435f3a0998/bdefe/cover.png 180w,\n/static/34f6304392b84962f8fc36435f3a0998/9d984/cover.png 360w","sizes":"(min-width: 360px) 360px, 100vw"},"sources":[{"srcSet":"/static/34f6304392b84962f8fc36435f3a0998/4a0b0/cover.webp 90w,\n/static/34f6304392b84962f8fc36435f3a0998/56f9a/cover.webp 180w,\n/static/34f6304392b84962f8fc36435f3a0998/279ed/cover.webp 360w","type":"image/webp","sizes":"(min-width: 360px) 360px, 100vw"}]},"width":660,"height":399.66666666666663}}}},"body":"<h1>AWS Redshift에 알아보자!</h1>\n<p>AWS Redshift는 AWS에서 제공하는 완전 관리형 데이터 웨어하우스 서비스입니다.</p>\n<p>Redshift는 대규모 데이터 분석이 가능하며, 데이터 웨어하우스를 구축하는데 효율적입니다.</p>\n<h2>목차</h2>\n<ol>\n<li><a href=\"#aws-redshift%EB%9E%80\">AWS Redshift란?</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%EA%B8%B0%EB%8A%A5\">주요 기능</a></li>\n<li><a href=\"#redshift%EC%9D%98-%EA%B5%AC%EC%84%B1-%EC%9A%94%EC%86%8C\">Redshift의 구성 요소</a></li>\n<li><a href=\"#%EC%82%AC%EC%9A%A9-%EC%82%AC%EB%A1%80\">사용 사례</a></li>\n<li><a href=\"#redshift%EC%9D%98-%EC%9E%A5%EC%A0%90\">Redshift의 장점</a></li>\n<li><a href=\"#redshift%EC%99%80-%EB%8B%A4%EB%A5%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9B%A8%EC%96%B4%ED%95%98%EC%9A%B0%EC%8A%A4-%EC%84%9C%EB%B9%84%EC%8A%A4-%EB%B9%84%EA%B5%90\">Redshift와 다른 데이터 웨어하우스 서비스 비교</a></li>\n<li><a href=\"#%EA%B2%B0%EB%A1%A0\">결론</a></li>\n</ol>\n<h2>💬 AWS Redshift란?</h2>\n<p>AWS Redshift는 대규모 데이터를 효율적으로 저장하고 분석할 수 있는 클라우드 기반 데이터 웨어하우스 기술입니다.</p>\n<p>Redshift는 페타바이트(PB) 수준의 데이터를 처리할 수 있도록 설계되었으며, SQL 기반의 쿼리를 통해 데이터를 분석할 수 있습니다.</p>\n<p>Redshift는 기존의 데이터 웨어하우스와 비교하여 비용 효율성이 높고, 성능이 우수하며, 관리가 용이하다는 장점이 있습니다.</p>\n<h2>🌟 주요 기능</h2>\n<h3>1. <strong>고성능 쿼리 처리</strong></h3>\n<ul>\n<li>Redshift는 컬럼형 스토리지를 사용하여 데이터를 효율적으로 저장하고 쿼리 성능을 극대화합니다.</li>\n<li>데이터 압축 및 저장소 최적화를 통해 쿼리 성능을 향상시킵니다.</li>\n</ul>\n<h3>2. <strong>확장성</strong></h3>\n<ul>\n<li>수십 테라바이트에서 페타바이트 규모로 데이터를 확장할 수 있습니다.</li>\n<li>필요에 따라 컴퓨팅 리소스를 쉽게 확장하거나 축소할 수 있습니다.</li>\n</ul>\n<h3>3. <strong>비용 효율성</strong></h3>\n<ul>\n<li>사용한 만큼만 비용을 지불하는 종량제 요금 모델을 제공합니다.</li>\n<li>데이터 압축 기술과 스펙트럼(Spectrum)을 활용하여 비용을 절감할 수 있습니다.</li>\n</ul>\n<h3>4. <strong>Redshift Spectrum</strong></h3>\n<ul>\n<li>Amazon S3에 저장된 데이터를 직접 쿼리할 수 있어, 데이터 이동 없이 분석이 가능합니다.</li>\n<li>Redshift 클러스터 외부의 데이터도 쉽게 분석할 수 있습니다.</li>\n</ul>\n<h3>5. <strong>완전 관리형 서비스</strong></h3>\n<ul>\n<li>클러스터 관리, 백업, 보안, 그리고 모니터링을 자동으로 수행합니다.</li>\n<li>관리 오버헤드를 줄여, 사용자는 데이터 분석에 집중할 수 있습니다.</li>\n</ul>\n<h2>💫 Redshift의 구성 요소</h2>\n<h3>1. <strong>클러스터(Cluster)</strong></h3>\n<ul>\n<li>Redshift는 하나 이상의 노드로 구성된 클러스터를 사용합니다.</li>\n<li>각 클러스터는 리더 노드(Leader Node)와 컴퓨팅 노드(Compute Node)로 구성됩니다.</li>\n</ul>\n<h3>2. <strong>리더 노드(Leader Node)</strong></h3>\n<ul>\n<li>리더 노드는 클러스터에 대한 쿼리를 수신하고, SQL 구문을 해석한 후 적절한 컴퓨팅 노드에 작업을 배포합니다.</li>\n<li>결과를 집계하고, 클라이언트 애플리케이션에 결과를 반환합니다.</li>\n</ul>\n<h3>3. <strong>컴퓨팅 노드(Compute Node)</strong></h3>\n<ul>\n<li>컴퓨팅 노드는 실제 데이터가 저장되고, 쿼리 작업이 수행되는 곳입니다.</li>\n<li>각 컴퓨팅 노드는 자체 스토리지와 CPU 자원을 갖고 있습니다.</li>\n</ul>\n<h3>4. <strong>Redshift Spectrum</strong></h3>\n<ul>\n<li>Amazon S3에 저장된 데이터를 직접 쿼리할 수 있는 기능입니다.</li>\n<li>Redshift 클러스터의 외부에 있는 데이터에 대해 SQL 쿼리를 실행할 수 있습니다.</li>\n</ul>\n<h2>📊 사용 사례</h2>\n<h3>1. <strong>비즈니스 인텔리전스(BI)</strong></h3>\n<ul>\n<li>Redshift는 BI 도구와 통합하여 대규모 데이터를 분석하고, 시각화하는 데 사용됩니다.</li>\n<li>Tableau, Power BI와 같은 도구와 쉽게 연동할 수 있습니다.</li>\n</ul>\n<h3>2. <strong>데이터 웨어하우징</strong></h3>\n<ul>\n<li>다양한 소스에서 데이터를 수집하고, 중앙 집중식 데이터 웨어하우스로 통합할 수 있습니다.</li>\n<li>과거 데이터를 저장하고, 이를 통해 트렌드 분석, 보고서 작성이 가능합니다.</li>\n</ul>\n<h3>3. <strong>로그 분석</strong></h3>\n<ul>\n<li>애플리케이션 로그, 웹 서버 로그 등 대량의 로그 데이터를 효율적으로 저장하고 분석할 수 있습니다.</li>\n<li>실시간 분석을 통해 비즈니스 결정을 지원합니다.</li>\n</ul>\n<h3>4. <strong>빅데이터 분석</strong></h3>\n<ul>\n<li>페타바이트 규모의 데이터를 처리하고 분석하는 데 최적화되어 있습니다.</li>\n<li>머신러닝과 결합하여 예측 분석, 추천 시스템 등에 활용할 수 있습니다.</li>\n</ul>\n<h2>🚀 Redshift의 장점</h2>\n<h3>1. <strong>높은 성능</strong></h3>\n<ul>\n<li>컬럼형 스토리지 및 데이터 압축을 통해 빠른 쿼리 성능을 제공합니다.</li>\n<li>병렬 처리 및 분산 시스템을 활용하여 대규모 데이터도 효율적으로 처리할 수 있습니다.</li>\n</ul>\n<h3>2. <strong>유연한 확장성</strong></h3>\n<ul>\n<li>비즈니스 요구에 맞춰 클러스터의 크기를 조정할 수 있습니다.</li>\n<li>필요에 따라 노드를 추가하거나 제거할 수 있습니다.</li>\n</ul>\n<h3>3. <strong>보안</strong></h3>\n<ul>\n<li>데이터 암호화, IAM 역할 기반 접근 제어, VPC를 통한 네트워크 격리 등 강력한 보안 기능을 제공합니다.</li>\n<li>규제 요구 사항을 충족하는 다양한 보안 인증을 지원합니다.</li>\n</ul>\n<h3>4. <strong>비용 절감</strong></h3>\n<ul>\n<li>S3와 Redshift Spectrum을 사용하여 자주 사용하지 않는 데이터를 저렴하게 저장하고 필요할 때만 쿼리할 수 있습니다.</li>\n<li>리저브드 인스턴스를 사용하여 장기적으로 비용을 절감할 수 있습니다.</li>\n</ul>\n<h2>📉 Redshift와 다른 데이터 웨어하우스 서비스 비교</h2>\n<table>\n<thead>\n<tr>\n<th>특징</th>\n<th>AWS Redshift</th>\n<th>Google BigQuery</th>\n<th>Snowflake</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>스토리지 방식</strong></td>\n<td>컬럼형 스토리지</td>\n<td>컬럼형 스토리지</td>\n<td>하이브리드(컬럼형 + JSON)</td>\n</tr>\n<tr>\n<td><strong>확장성</strong></td>\n<td>수십 테라바이트에서 페타바이트까지 확장 가능</td>\n<td>실시간 확장 가능</td>\n<td>무제한 확장 가능</td>\n</tr>\n<tr>\n<td><strong>비용 구조</strong></td>\n<td>시간당 과금 및 스토리지 사용량 기준 과금</td>\n<td>쿼리당 과금</td>\n<td>사용한 만큼 지불 (스토리지 + 컴퓨팅)</td>\n</tr>\n<tr>\n<td><strong>주요 기능</strong></td>\n<td>Redshift Spectrum, 관리형 서비스</td>\n<td>실시간 분석, BigQuery ML</td>\n<td>강력한 데이터 공유, 멀티 클라우드 지원</td>\n</tr>\n<tr>\n<td><strong>사용 사례</strong></td>\n<td>BI, 데이터 웨어하우징, 로그 분석</td>\n<td>실시간 분석, 대규모 데이터 처리</td>\n<td>데이터 공유, 멀티 클라우드 분석</td>\n</tr>\n</tbody>\n</table>\n<h2>🧑🏻‍💻 결론</h2>\n<p>AWS Redshift는 대규모 데이터 분석에 최적화된 강력한 데이터 웨어하우스 서비스입니다.</p>\n<p>데이터 웨어하우징, 로그 분석 등 다양한 사용 사례에 활용할 수 있으며, AWS의 다른 서비스와 쉽게 통합하여 더 강력한 데이터 분석 플랫폼을 구축할 수 있습니다.</p>\n<p>AWS를 많이 사용한다면 클라우드형 DB로 필수적으로 사용을 해봐야겠다는 생각을 했습니다.</p>","categories":["Redshift","ALL"],"date":"September 03, 2024","description":"Redshift 에 대해.","id":"27a19baa-d16a-589a-afc2-369da2d7f0db","keywords":["Blog","Redshift","DW"],"slug":"/my-first-article/About-Redshift-1/","title":" 🌟 [Redshift] About Redshift","readingTime":{"text":"9 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org\">Spark</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACQklEQVR42mP4Twz4+xdIfD2x+Wmm4d+Pb0Ai//4BCQbCOoHqfv8C0u+XND5OUHk7u/TXk9ufd857v6yF4S/Y1H9gAGfAAdySv18/Pit1+HZ218uW0NfdcY+i5V61RzGg2gHV/+fPHxDj718g/9fDq2+mZL9oCXu/oPbft89AL3w5tPp1T+Lf718YHj9+DFR38MChM2fOHjly9Nix4ydPnLx29drx4yeOHT5y+tL170fW3HFj+jAh+c/Xj2DT/r9sCPi0bRbIz4H+wa9evb59+/aDBw8/fPhw8cLFG9dvvHn95smTJ3fv3rt16xbISR8eH+kqmN7b/evf/xfXz39e1/u82vP/n98Mz58///79+4b1G/JzCxYsWAh084/vP6ZOmZadmbN2zVqgxn1790+fNa+ps6+1pb29d/K27oqPVS4PIiR/PrjC8OPHj5kzZqUkpx08eCg4MHTtmnWtLW1VldX79x3w8fLbumVbVGTMimXLmxuazE0t58yeAzTuz6d3v57c+vf3D0N+XmF6WubVq9eAorNmzU5NSc9Iz3z79i2Q297aUVNVW1xY+uvXr+KiUhcnt+nTZoBj/S8kgBkqyio93b0m9E/cvWt3aEj4kcNHYqLigM7eu3dfVmZOZUV1cWHJz58//X0DgMEZEhy2bt16oLbfv36CNAODNyIsqrysEmj/kcNHgUJnz5zNSM9qaW5dtXI1EG3csOnP7z8tzW1fv34FKu5o73z54iUQgDRv2by1ob4R7hhowsCe0v6hyQIAb+47xF+wYqUAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png","srcSet":"/static/be399145ad033fb2b5626e243b7bde07/4ee5e/spark-cover.png 78w,\n/static/be399145ad033fb2b5626e243b7bde07/b1f1f/spark-cover.png 156w,\n/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png 312w","sizes":"(min-width: 312px) 312px, 100vw"},"sources":[{"srcSet":"/static/be399145ad033fb2b5626e243b7bde07/ddb87/spark-cover.webp 78w,\n/static/be399145ad033fb2b5626e243b7bde07/e77d5/spark-cover.webp 156w,\n/static/be399145ad033fb2b5626e243b7bde07/b6f70/spark-cover.webp 312w","type":"image/webp","sizes":"(min-width: 312px) 312px, 100vw"}]},"width":660,"height":399.8076923076923}}}},"body":"<h1>🚀 Apache Spark</h1>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/spark-hadoop.png\" alt=\"Hadoop-Spark\" width=\"600\">\n<h1>🌟 Hadoop과 Spark의 간단한 실습을 진행해보자!</h1>\n<p>docker를 통해 EC2에서 Hadoop 클러스터를 생성하고, yarn cluster 모드를 이용한 spark의 간단한 실습입니다.</p>\n<p>오늘은 간단한 실습만 진행하고 다음엔 좀 더 복잡한 내용과 스트리밍 처리까지 진행하겠습니다. 🫡</p>\n<h2>💬 기본 구성</h2>\n<p><a href=\"https://github.com/jms0522/hadoop_system\">hadoop 및 spark 설정파일</a></p>\n<h2>💫 진행 순서</h2>\n<ul>\n<li>테스트를 진행할 데이터셋 준비</li>\n</ul>\n<p><a href=\"https://www.kaggle.com/datasets/ahmettalhabektas/new-york-cars-big-data-2023\">New York Cars ~ Big Data (2023)</a></p>\n<ul>\n<li>로컬에 다운받은 dataset을 hdfs에 전송</li>\n</ul>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/hdfs.png\" alt=\"hdfs\" width=\"600\">\n<ul>\n<li>Spark 시작 - hadoop 연결 확인</li>\n</ul>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/spark-connection.png\" alt=\"connection\" width=\"600\">\n<ul>\n<li>Jupyter notebook에서 spark로 간단한 분석</li>\n</ul>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-practice/spark-notebook.png\" alt=\"jupyter\" width=\"600\">","categories":["Spark","ALL"],"date":"September 03, 2024","description":"간단한 spark 실습 1 ","id":"f389f64c-1c55-541f-8f29-61b827b5097a","keywords":["Pipeline","Blog","Spark"],"slug":"/my-first-article/About-Spark-Practice-1/","title":" 🌟 [Spark] Spark Practice-1 ","readingTime":{"text":"1 min read"}},{"banner":{"alt":"Airflow","caption":"Photo by <u><a href=\"https://hadoop.apache.org/\">Hadoop</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABxElEQVR42mP4DwP/QOAPEP8nGjDAdP4FEhDG379/wAZhMQVNnAGu7e3bT48fn/j85SNU2f//f4FycAQEMJ1wQZDNP378vXz5/OFDQe9eM1y64Hbq9P4XT+5/+/wZ0+YvHz9+/vABYfO+fasOH5p65pTF6xcMT59w3rvLfnwvg191dtfW088e3nvw9Nnjtx8ePXvx4N6d23fvJbRNyZuz5v79uw+fv3h09zbDiRNzrlziffOK8+4ttqePWR89YDqxh8G0bYb3qrOu0zZbVE+y71vtvuAgkO09bZPZtO02i44EzdnuMXeP8+QNDCeOd758wXD2FMuzxwyrlgvu3D6jpbvZasrG6qZm3c5lZvOPqBZPsFh1wXnf4+TSKsPG2W4HnlS1duqVdFnuf85w8eKefXsUt28PP7hPcvvWEqBPSpvak+du7JowyaZxVtDSw75zd3lP3eg/d2fo4gM+s3f4z9kZseyQ/4wtXlM3MABD+t27Nx8/fnj58uXfv////v394cOHj+/fffr8+c2L529fPH//4dPbVy+B7Hfv37999QKI3n369OHtm1fPnkCjChwH/8BsRJQSlUjAcYbQ+Y9owPCfAgAAPu5ioh+z56MAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/677593b5d44628e115a130a808c62588/881d6/hadoop-cover.png","srcSet":"/static/677593b5d44628e115a130a808c62588/4fc02/hadoop-cover.png 83w,\n/static/677593b5d44628e115a130a808c62588/0a969/hadoop-cover.png 167w,\n/static/677593b5d44628e115a130a808c62588/881d6/hadoop-cover.png 333w","sizes":"(min-width: 333px) 333px, 100vw"},"sources":[{"srcSet":"/static/677593b5d44628e115a130a808c62588/18b83/hadoop-cover.webp 83w,\n/static/677593b5d44628e115a130a808c62588/e4f8c/hadoop-cover.webp 167w,\n/static/677593b5d44628e115a130a808c62588/486ee/hadoop-cover.webp 333w","type":"image/webp","sizes":"(min-width: 333px) 333px, 100vw"}]},"width":660,"height":400.3603603603604}}}},"body":"<h1>🐘 Hadoop에 대해 알아보자!</h1>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/hadoop/hadoop-comp.png\" alt=\"Hadoop\" width=\"600\">\n<h2>✅ 하듑 (Hadoop)이란?</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/hadoop/hadoop-arch.png\" alt=\"Hadoop\" width=\"600\">\n<ul>\n<li>하둡은 대규모 데이터 세트를 분산 저장 및 처리하기 위한 오픈 소스 프레임워크.</li>\n<li>성능 좋은 한 대의 컴퓨터가 아니라, 적당한 성능의 컴퓨터 여러대를 클러스토화, 병렬 동시 처리하여 속도를 높이는 것을 목표로 개발.</li>\n</ul>\n<h2>🚀 하듑의 장점</h2>\n<ul>\n<li><strong>확장성</strong>: 클러스터에 노드를 추가함으로써 쉽게 확장할 수 있다 - 장비의 추가가 용이.</li>\n<li><strong>비용 효율성</strong>: 저렴한 하드웨어를 사용하여 대규모 데이터 처리가 가능 - RDBMS에 비해 라이선스 및 구축 비용이 저렴함.</li>\n<li><strong>내결함성</strong>: 데이터를 여러 노드에 자동으로 복제하여 하드웨어 고장 시에도 데이터 손실을 방지.</li>\n<li><strong>유연성</strong>: 구조화된 데이터, 반구조화된 데이터, 비구조화된 데이터 모두 처리할 수 있음.</li>\n</ul>\n<h2>❌ 하듑의 단점</h2>\n<p>-<strong>HDFS</strong>: 저장된 데이터 변경 불가</p>\n<p>-<strong>실시간 데이터</strong>: 실시간 데이터 처리에 부적합</p>","categories":["Hadoop","ALL"],"date":"September 02, 2024","description":"Hadoop 소개글","id":"3a0b024d-88a8-5c84-ba68-289510e0f992","keywords":["Pipeline","Blog","Hadoop"],"slug":"/my-first-article/About-Hadoop-1/","title":" 🌟 [Hadoop] Hadoop-1 ","readingTime":{"text":"2 min read"}},{"banner":{"alt":"Kubernetes","caption":"Photo by <u><a href=\"https://developer.hashicorp.com/terraform/docs\">Terraform Docs</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABkklEQVR42p1SQUsCQRid/9MxkAKpQ9f0FiHBBkGoGJWooSFkCCILCh47WaFISAfLDKUS06MIXjZR0w1CzfWgqYG7O2vfOrRYdMgey8ybx77HN983aDweYyx//wAa9PFoJJEISd7lVRSlP5kfksPjA4595oltKkISIUP6mYIxVkR0lxiszL9Y9K3ryz73JoIkCDgWq7daH5Nf5QiAACrGQIifiKhe5U1Uc2ezadhobK8375Pdvf3s4sJVPl9LpW4Y5olUoayFQoFhGHJEsPW6OJ0cHlnbq+pXi6mmUkWXl66rVc5qNdvtjlAoFA6Hg8FgPB6PRqNGo9Hn80UiERARxJErnJ10D3c7bierN6Szjw1QAoGAVqulKMrlcvn9fq/Xq9FoaJr2eDw2m02n0yHSZ4DT3N5aa9zGeoLcJ1kplUqZTCaXy5XLZZZlgSQSiUqlAjpUUSwWEWkywO3g1HNs5LQHnOe/zV2a4JdRKeZOW6RdnYvznlILGPAXFE6CoNXA0XQS1Nt/n+GtoanpQ7Y00/P8BCTEfC+SE6yPAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/f0f70743ca55edfb23c04d14d14f5093/be504/cover-t.png","srcSet":"/static/f0f70743ca55edfb23c04d14d14f5093/966b5/cover-t.png 80w,\n/static/f0f70743ca55edfb23c04d14d14f5093/37b8e/cover-t.png 159w,\n/static/f0f70743ca55edfb23c04d14d14f5093/be504/cover-t.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/f0f70743ca55edfb23c04d14d14f5093/41d58/cover-t.webp 80w,\n/static/f0f70743ca55edfb23c04d14d14f5093/d2420/cover-t.webp 159w,\n/static/f0f70743ca55edfb23c04d14d14f5093/7b932/cover-t.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>💬 Terraform 에 대해 알아보자!</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/terraform-images/cover-t.png\" alt=\"Terraform\"></p>\n<p>각각의 EC2에 하듑 네임노드와 데이터노드를 구성하다, 각기 다른 EC2나 인프라에서 환경 구축을 어떻게 간결하고 일관성 있게 관리하는지 궁금해졌다.</p>\n<p>그러다 terraform 이란 기술로 인프라를 코드로 정의하고 관리할 수 있다는 걸 알게 되어 한 번 사용해봤다.</p>\n<h2>📌 Terraform이란?</h2>\n<p>Terraform은 HashiCorp에서 개발한 오픈 소스 인프라 관리 도구로, 인프라를 코드로 정의하고 관리할 수 있게 해준다.</p>\n<p>이를 통해 클라우드 환경(AWS, Azure, GCP 등)이나 온프레미스 환경에서 서버, 네트워크, 데이터베이스 등의 인프라를 자동으로 배포하고 관리할 수 있다.</p>\n<h3>✓ Terraform의 장점은?</h3>\n<ul>\n<li><strong>자동화</strong> : 코드를 통해 자동으로 배포 관리가 가능하다.</li>\n<li><strong>일관성</strong> : 코드를 통해 정의함으로 동일한 환경을 반복적으로 생성 가능.</li>\n<li><strong>재사용성</strong> : 복잡한 인프라를 모듈을 통해 쉽게 재사용 가능.</li>\n<li><strong>다양한 클라우드 지원</strong> : 여러 클라우드와 온프레미스 환경을 관리 가능.</li>\n</ul>\n<h2>🔨 작업 순서</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/terraform-images/working.png\" alt=\"Terraform working\" width=\"700\">\n<h3>1. 프로젝트 초기화 (terraform init)</h3>\n<ul>\n<li>디렉토리 초기화 및 필요한 플러그인 다운로드.</li>\n<li>terraform 설정 기반, 필요한 모든 종속성을 설정.</li>\n</ul>\n<h3>2. 코드 작성 (tf 파일)</h3>\n<ul>\n<li>코드로 인프라 정의.</li>\n<li>리소스, 변수, 출력 등이 포함.</li>\n</ul>\n<h3>3. 계획 (terraform plan)</h3>\n<ul>\n<li>작성한 코드의 변경 사항 미리 확인.</li>\n<li>실제 인프라가 변경 되는 건 아님 (어떤 변화가 일어날지 미리 확인 가능)</li>\n</ul>\n<h3>4. 적용 (terraform apply)</h3>\n<ul>\n<li>실제로 인프라를 배포, 변경사항 적용.</li>\n<li>plan에서 확인한 내용을 바탕으로 수정 및 삭제 등등..</li>\n</ul>\n<h3>5. 변경 (terraform apply)</h3>\n<ul>\n<li>인프라 변경이 필요 시 코드 수정 반영.</li>\n<li>코드의 차이만 적용.</li>\n</ul>\n<h3>6. 제거 (terraform destroy)</h3>\n<ul>\n<li>인프라 제거 필요시 terraform destroy 명령어 입력</li>\n<li>모든 리소스를 삭제, 클린업 수행.</li>\n</ul>\n<h2>📚 참고자료</h2>\n<p><a href=\"https://developer.hashicorp.com/terraform/docs\">Terraform 공식 문서</a></p>\n<p><a href=\"https://developer.hashicorp.com/terraform/tutorials\">HashiCorp Learn</a></p>","categories":["Terraform","ALL"],"date":"September 02, 2024","description":"Terraform 에 대해.","id":"1407220f-7554-559d-9763-ad93d5fb0939","keywords":["Blog","Terraform","Provisioning"],"slug":"/my-first-article/About-Terraform-1/","title":" 🌟 [Terraform] About Terraform","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Airflow","caption":"Photo by <u><a href=\"https://airflow.apache.org\">Airflow</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACO0lEQVR42oWS3WvTUBTAb+dk4qNPIvgB/gH1A31xyEAnExR88GH4MJ8EBYdPIoh78MUnBd2LWVs2sV1bV9subl0qQjs6ZpdEU21T027DNV0b1vXbmJI1TeNJU3Gg4IHce88593dzvpBaLGggqqrqm1ZTWk9zRVltw7n9l/y+1RUkXDLLVMTgwX2Vy5z9utYltf8Iypw6tD1k1jhG0bSRNI8WmespHhybuVwyyRa2trLZrCAIqRS3mk7zPN9oNIyndbh0+uDm5ZPag+GxcBiRKbT05d73PDhI+pPf5/XD99aDz86Gw6GlSMTpnC6Xy39gZvCMds2ceXz7WDCM6DTAju0qOHYURZblSqUCqyj+aDabakfAUq/X1U6OaPihEx+7c3SRRJROHqa5UlPR4aYy/uJ5uVyq1WpsImEkub6+Zre/9vm8Ql6PDvXho3tj0T3fiiYysf/z6vuaaNxLJpNWi+Xj8jLESdNUIDBPEMTU5CSGvbRMYBzHgQX1fDjfR4weyFaOCDsLkgyY0umHzWYNBgmb1cqy7Nzcu2mHQ5KkGMOsrEQpinK5nHBAJv+V3pkBFLh1M7WgR6u2YC0UChMYFo/H3W4X1CwUCnk8HkiSYRggo9EoWHAcR8g/YPIM9novojfnHrGvjJgl6We1qpcNGgMPiaIIKsCgwv9BVRQly/MI25jf5xtC7v4edz+yn7gbGzd4tW1UV/3neHRbpddQzD9Lz9wgn1yI3D9OjExtBMHY6sS/ezZ3q8ZA/wIAVki+U9pyEAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png","srcSet":"/static/de7402d2a87d0256451392f6590d157b/182cc/airflow-profile.png 91w,\n/static/de7402d2a87d0256451392f6590d157b/2c6ad/airflow-profile.png 181w,\n/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png 362w","sizes":"(min-width: 362px) 362px, 100vw"},"sources":[{"srcSet":"/static/de7402d2a87d0256451392f6590d157b/98c2e/airflow-profile.webp 91w,\n/static/de7402d2a87d0256451392f6590d157b/483ec/airflow-profile.webp 181w,\n/static/de7402d2a87d0256451392f6590d157b/def5b/airflow-profile.webp 362w","type":"image/webp","sizes":"(min-width: 362px) 362px, 100vw"}]},"width":660,"height":399.2817679558011}}}},"body":"<h1>🌬️ Apache Airflow: 공부하기 - 2</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-profile.png\" alt=\"Airflow\"></p>\n<h1>📉 Error를 모니터링 해보자!</h1>\n<h2>🌟 프로젝트 개요</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-ing/monitoring.png\" alt=\"monitoring\" width=\"600\">\n<ul>\n<li>\n<p>오류를 모니터링하고 알림 이메일을 자동으로 보내는 Airflow 데이터 파이프라인을 구축.</p>\n</li>\n<li>\n<p>Kibana나 Grafana등으로 대시 보드 구축은 해봤지만 간단한 Airflow 파이프라인으로 오류를 검출하기 위함.</p>\n</li>\n</ul>\n<h1>✅ 진행 과정</h1>\n<h2>간단한 실습을 위해 가짜 데이터를 생성</h2>\n<h2>데이터를 생성 후 postgres 테이블 생성</h2>\n<h2>테이블이 존재하면 데이터 적재</h2>\n<ul>\n<li>생성한 데이터 적재되는 모습</li>\n</ul>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-slack/data.png\" alt=\"데이터 적재 모습\" width=\"600\">\n<h2>생성된 데이터가 postgres에 적재 완료 시 슬랙 알람 (싪패시 동일하게 알람)</h2>\n<ul>\n<li>간단한 알람</li>\n<li>원하는 알람에 따라 상세하게 조절이 가능</li>\n</ul>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-slack/data.png\" alt=\"간단한 알림\" width=\"600\">\n<h2>Airflow 자동화 모습</h2>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-slack/finish.png\" alt=\"간단한 dag 완료\" width=\"600\">\n<ul>\n<li>코드는 <a href=\"https://github.com/jms0522/Streaming-Data/blob/main/airflow/dags/data_generate_send_postgres_alert_slack.py\">Airflow-basic-send-to-slack</a>에서 확인 가능합니다.</li>\n</ul>\n<h2>📊 사용된 기술 스택</h2>\n<ul>\n<li><strong>Docker</strong>: 애플리케이션을 컨테이너로 패키징하여 독립적으로 실행</li>\n<li><strong>Apache Airflow</strong>: 워크플로우 관리 플랫폼</li>\n<li><strong>PostgreSQL</strong>: 관계형 데이터베이스 관리 시스템</li>\n<li><strong>Postgres Hook</strong>: Airflow Connections</li>\n<li><strong>Slack Hook</strong>: Airflow Connections</li>\n</ul>\n<h1>🧑🏻‍💻 느낀점</h1>\n<ul>\n<li>\n<p>사실 간단한 dag라도 만드는데 좀 까다로운 부분이 있었는데요,</p>\n</li>\n<li>\n<p>버전이라던가 의존성 관리하는 부분이 좀 귀찮고 번거로운 느낌이 있었습니다.</p>\n</li>\n<li>\n<p>더 고도화된 Airflow를 위해 실습을 많이 진행해보겠습니다. 😅</p>\n</li>\n</ul>","categories":["Airflow","ALL"],"date":"September 01, 2024","description":"Airflow 간단한 실습입니다.","id":"97284135-1d5d-5aba-b0c2-904cde6c34e8","keywords":["Workflow","Pipeline","Practice","Blog","Airflow","Monitoring"],"slug":"/my-first-article/About-Airflow-Practice-2/","title":" 🌟 [Airflow] Airflow Practice-2 ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Kubernetes","caption":"Photo by <u><a href=\"https://kubernetes.io/ko/docs/concepts/overview/\">About Kuberbnetes</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABXUlEQVR42mP4jw38+wcm////8esfsggaYMCl8/Hb320b3nl1POvc+O7pu99Y9aNr/vsXRB66/s224Ylz81OgfvvGJ0B07OZ3kOw/vJr/gOVLl7726362/+r3lcc/77n8za/rWeXyN3BZ7Jr/wSwP6n1euuTN5rNfzt3/seXc16JFb0L6niOrwa353//wCc/jp73ceObL1vNf15/6Ejv1RcyUFxAf/cPnZ7Dk9ac/dUofpc9+lTf/Veqsl/plj249/0XAz//ACGLDvivfIia+iJz0ImLSiwPXvoE9jB7g2KLqP7GAATlu33z+8+D1728//119/PPt5z/vv/x58eH3+y9/X3/68/D173dfgCJ/Ie5C0QzxzO3nv6bs/DBv/6eeze8nbns/afuHjae/zNn7sXvze6Bg5yYQ+eHrX7hlDGiR/OkbyB6gDUBFH7/9BboCSAIFP33/9x4s8hvJZgBJxZpJ/ehZAgAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/c435f6866693a67b538f13531975c1c6/8e9ff/kubernetes-cover.png","srcSet":"/static/c435f6866693a67b538f13531975c1c6/4ee5e/kubernetes-cover.png 78w,\n/static/c435f6866693a67b538f13531975c1c6/b1f1f/kubernetes-cover.png 156w,\n/static/c435f6866693a67b538f13531975c1c6/8e9ff/kubernetes-cover.png 311w","sizes":"(min-width: 311px) 311px, 100vw"},"sources":[{"srcSet":"/static/c435f6866693a67b538f13531975c1c6/ddb87/kubernetes-cover.webp 78w,\n/static/c435f6866693a67b538f13531975c1c6/e77d5/kubernetes-cover.webp 156w,\n/static/c435f6866693a67b538f13531975c1c6/da706/kubernetes-cover.webp 311w","type":"image/webp","sizes":"(min-width: 311px) 311px, 100vw"}]},"width":660,"height":398.97106109324756}}}},"body":"<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kubernetes-images/cluster.png\" alt=\"Kubernetes cluster\" width=\"600\">\n<h1>⚒️ Kubernetes 주요 Object</h1>\n<h3>1. <strong>Pod</strong></h3>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kubernetes-images/pod.png\" alt=\"pod\"></p>\n<ul>\n<li>Kubernetes에서 가장 작은 배포 단위로, 하나 이상의 컨테이너를 포함.</li>\n<li>한 개 이상의 컨테이너, 스토리지, 네트워크 속성.</li>\n<li>하나의 컨테이너를 사용하더라도 pod로 관리.</li>\n</ul>\n<h3>2. <strong>Service</strong></h3>\n<ul>\n<li>Pod 집합을 네트워크로 노출.</li>\n<li>Pod 끼리의 연결이나 외부에서 접근할 시.</li>\n</ul>\n<h3>3. <strong>Deployment</strong></h3>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kubernetes-images/deployment.png\" alt=\"Deployment\"></p>\n<ul>\n<li>Pod의 배포 및 관리를 담당, 롤링 업데이트 및 롤백을 지원.</li>\n<li>Pod을 여러 개(한 개 이상) 복제하여 관리하는 오브젝트.</li>\n</ul>\n<h3>4. <strong>ReplicaSet</strong></h3>\n<ul>\n<li>특정 수의 동일한 Pod를 유지하여 애플리케이션 가용성을 보장.</li>\n</ul>\n<h3>5. <strong>StatefulSet</strong></h3>\n<ul>\n<li>상태를 가진 애플리케이션을 위한 리소스, 각 Pod는 고유한 ID를 소유.</li>\n</ul>\n<h3>6. <strong>DaemonSet</strong></h3>\n<ul>\n<li>클러스터 내 모든 노드에서 특정 Pod를 실행하도록 합니다.</li>\n</ul>\n<h3>7. <strong>Job 및 CronJob</strong></h3>\n<ul>\n<li>Job은 일회성 작업을, CronJob은 주기적으로 실행되는 작업을 관리.</li>\n</ul>\n<h3>8. <strong>ConfigMap 및 Secret</strong></h3>\n<ul>\n<li>애플리케이션 설정과 민감한 데이터를 안전하게 관리.</li>\n</ul>\n<h3>9. <strong>Ingress</strong></h3>\n<ul>\n<li>외부 트래픽을 클러스터 내부의 서비스로 라우팅.</li>\n</ul>\n<h3>10. <strong>PersistentVolume 및 PersistentVolumeClaim</strong></h3>\n<ul>\n<li>Pod가 종료되어도 데이터가 유지되는 영구적인 스토리지를 제공.</li>\n</ul>\n<h3>11. <strong>Namespace</strong></h3>\n<ul>\n<li>클러스터 내 리소스를 논리적으로 분리하여 관리.</li>\n</ul>\n<h3>12. <strong>ResourceQuota</strong></h3>\n<ul>\n<li>특정 Namespace 내에서 사용할 수 있는 리소스의 양을 제한.</li>\n</ul>\n<h2>Helm이란?</h2>\n<p>Helm은 Kubernetes에서 애플리케이션을 패키징하고 관리하는 도구입니다. \"차트(Chart)\"라는 형태로 애플리케이션을 정의하고, 쉽게 배포 및 관리할 수 있도록 돕습니다.</p>\n<h2>Manifest 파일이란?</h2>\n<p>Manifest 파일은 Kubernetes 오브젝트를 정의하는 YAML 또는 JSON 형식의 파일입니다. 이를 통해 애플리케이션을 코드로 정의하고, <code class=\"language-text\">kubectl</code> 명령어로 클러스터에 적용할 수 있습니다.</p>\n<h2>Kubernetes의 중요 개념 및 추가 정보</h2>\n<h3>1. <strong>컨트롤 플레인 (Control Plane)</strong></h3>\n<ul>\n<li><strong>역할</strong>: Kubernetes 클러스터의 상태를 전반적으로 관리하는 역할을 합니다.</li>\n<li><strong>구성 요소</strong>: API 서버, etcd, 컨트롤러 매니저, 스케줄러 등이 포함됩니다.</li>\n<li><strong>중요성</strong>: 컨트롤 플레인은 클러스터 내에서 모든 작업을 조정하고, 각 오브젝트의 상태를 모니터링하여 지정된 상태를 유지합니다.</li>\n</ul>\n<h3>2. <strong>노드 컴포넌트 (Node Components)</strong></h3>\n<ul>\n<li><strong>구성 요소</strong>: 각 워커 노드에서 실행되는 kubelet, kube-proxy, 컨테이너 런타임 등이 포함됩니다.</li>\n<li><strong>역할</strong>: 실제 애플리케이션 컨테이너를 실행하고 관리하는 역할을 담당합니다.</li>\n</ul>\n<h3>3. <strong>자동화와 셀프 힐링 (Self-Healing)</strong></h3>\n<ul>\n<li><strong>자동화</strong>: Kubernetes는 배포, 업데이트, 스케일링 작업을 자동으로 처리할 수 있습니다.</li>\n<li><strong>셀프 힐링</strong>: 장애가 발생한 파드를 자동으로 재시작하거나, 필요에 따라 새로운 노드에 파드를 재배치하여 서비스 가용성을 유지합니다.</li>\n</ul>\n<h3>4. <strong>스케일링과 로드 밸런싱</strong></h3>\n<ul>\n<li><strong>자동 스케일링</strong>: Kubernetes는 애플리케이션의 부하에 따라 파드 수를 자동으로 조정할 수 있습니다.</li>\n<li><strong>로드 밸런싱</strong>: 서비스의 트래픽을 여러 파드에 분산시켜 부하를 균등하게 유지합니다.</li>\n</ul>\n<h3>5. <strong>클라우드 네이티브와 이식성</strong></h3>\n<ul>\n<li>Kubernetes는 여러 클라우드 제공자와 온프레미스 환경에서 동일한 방식으로 운영할 수 있는 이식성을 제공합니다. 이를 통해 특정 클라우드 제공자에 종속되지 않고 자유롭게 클러스터를 배포하고 관리할 수 있습니다.</li>\n</ul>\n<h3>6. <strong>보안 관리</strong></h3>\n<ul>\n<li>Kubernetes는 네트워크 폴리시, 역할 기반 접근 제어(RBAC), 시크릿 관리 등을 통해 보안성을 강화합니다. 이러한 기능을 통해 애플리케이션과 데이터를 안전하게 보호할 수 있습니다.</li>\n</ul>\n<h2>결론</h2>\n<p>Kubernetes는 컨테이너화된 애플리케이션을 관리하기 위한 필수 도구로, 자동화, 확장성, 이식성 등 다양한 장점을 제공합니다. Helm과 Manifest 파일을 활용해 애플리케이션의 배포와 관리를 더욱 효율적으로 수행할 수 있으며, Kubernetes의 주요 오브젝트와 개념을 이해하는 것이 클러스터 운영의 핵심입니다.</p>","categories":["Kubernetes","ALL"],"date":"August 31, 2024","description":"Kubernetes에 대해.","id":"b520347c-0f8b-5c44-8188-ed47321cb38a","keywords":["Container","Blog","Kubernetes"],"slug":"/my-first-article/About-Kubernetes-2/","title":" 🌟 [Kubernetes] About Kubernetes-2","readingTime":{"text":"6 min read"}},{"banner":{"alt":"Kubernetes","caption":"Photo by <u><a href=\"https://kubernetes.io/ko/docs/concepts/overview/\">About Kuberbnetes</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABXUlEQVR42mP4jw38+wcm////8esfsggaYMCl8/Hb320b3nl1POvc+O7pu99Y9aNr/vsXRB66/s224Ylz81OgfvvGJ0B07OZ3kOw/vJr/gOVLl7726362/+r3lcc/77n8za/rWeXyN3BZ7Jr/wSwP6n1euuTN5rNfzt3/seXc16JFb0L6niOrwa353//wCc/jp73ceObL1vNf15/6Ejv1RcyUFxAf/cPnZ7Dk9ac/dUofpc9+lTf/Veqsl/plj249/0XAz//ACGLDvivfIia+iJz0ImLSiwPXvoE9jB7g2KLqP7GAATlu33z+8+D1728//119/PPt5z/vv/x58eH3+y9/X3/68/D173dfgCJ/Ie5C0QzxzO3nv6bs/DBv/6eeze8nbns/afuHjae/zNn7sXvze6Bg5yYQ+eHrX7hlDGiR/OkbyB6gDUBFH7/9BboCSAIFP33/9x4s8hvJZgBJxZpJ/ehZAgAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/c435f6866693a67b538f13531975c1c6/8e9ff/kubernetes-cover.png","srcSet":"/static/c435f6866693a67b538f13531975c1c6/4ee5e/kubernetes-cover.png 78w,\n/static/c435f6866693a67b538f13531975c1c6/b1f1f/kubernetes-cover.png 156w,\n/static/c435f6866693a67b538f13531975c1c6/8e9ff/kubernetes-cover.png 311w","sizes":"(min-width: 311px) 311px, 100vw"},"sources":[{"srcSet":"/static/c435f6866693a67b538f13531975c1c6/ddb87/kubernetes-cover.webp 78w,\n/static/c435f6866693a67b538f13531975c1c6/e77d5/kubernetes-cover.webp 156w,\n/static/c435f6866693a67b538f13531975c1c6/da706/kubernetes-cover.webp 311w","type":"image/webp","sizes":"(min-width: 311px) 311px, 100vw"}]},"width":660,"height":398.97106109324756}}}},"body":"<h1>💬 Kubernetes 소개</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kubernetes-images/kubernetes-cover.png\" alt=\"Kubernetes\"></p>\n<h2>👑 쿠버네티스란 무엇인가?</h2>\n<p>\"쿠버네티스는 컨테이너화된 워크로드와 서비스를 관리하기 위한 이식할 수 있고, 확장 가능한 오픈소스 플랫폼으로, 선언적 구성과 자동화를 모두 지원한다.</p>\n<p>쿠버네티스는 크고 빠르게 성장하는 생태계를 가지고 있다. 쿠버네티스 서비스, 지원 그리고 도구들은 광범위하게 제공된다.\"</p>\n<p>라고 공식 페이지에 적혀있다.</p>\n<p>컨테이너라는 기술이 도입되면서 컨테이너를 대규모로 운영하기 위해서 효과적인 관리 및 조정 방법이 필요했고</p>\n<p>그로 인해 쿠버네티스가 등장하게 되었다는 말이다.</p>\n<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kubernetes-images/kubernetes-working.png\" alt=\"Kubernetes working\" width=\"400\">\n<p>쿠버네티스는 컨테이너 오케스트레이션 도구로, 많은 컨테이너에 있는 어플리케이션을 클러스터에서 효율적으로 관리할 수 있게 해준다.</p>\n<p>쿠버네티스는 처음의 구글의 'Borg'라는 이름의 컨테이너 관리 기술에서 나왔으며 현재는 CNCF(Cloud Native Computing Foundation)에서 관리하고 있다.</p>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kubernetes-images/borg.png\" alt=\"Borg\"></p>\n<h2>📊 사용 이유</h2>\n<h3>자동화된 운영</h3>\n<ul>\n<li>컨테이너화된 어플리케이션의 배포, 스케일링, 운영을 자동화.</li>\n</ul>\n<h3>우수한 확장성</h3>\n<ul>\n<li>수요에 따라 리소스를 동적 할당 가능.</li>\n</ul>\n<h3>이식성</h3>\n<ul>\n<li>다양한 환경에서 동일한 방식으로 배포 가능.</li>\n</ul>\n<h3>복원력</h3>\n<ul>\n<li>장애 발생시 자동으로 복구</li>\n</ul>\n<h2>🌟 쿠버네티스 배포 종류</h2>\n<h3>✓관리형 쿠버네티스</h3>\n<ul>\n<li>클라우드 서비스 제공자가 클러스터를 설치, 관리 유지보수.</li>\n<li>배포와 운영에 집중 가능.</li>\n<li>자동화된 업데이트, 우수한 확정성 및 안정성.</li>\n</ul>\n<p>•\tGoogle Kubernetes Engine (GKE)</p>\n<p>•\tAmazon Elastic Kubernetes Service (EKS)</p>\n<p>•\tAzure Kubernetes Service (AKS)</p>\n<h3>✓설치형 쿠버네티스</h3>\n<ul>\n<li>사용자가 직접 설치하고 관리하는 방식.</li>\n<li>자율적인 제어 가능.</li>\n<li>유연 및 클라우드 독립성.</li>\n</ul>\n<p>•\tRancher</p>\n<p>•\tOpenShift</p>\n<h3>✓구성형 쿠버네티스</h3>\n<ul>\n<li>환경이나 목젝에 맞게 맞춰 배포.</li>\n<li>최적화 설정을 통한 효율적 구성.</li>\n<li>요구사항 충족.</li>\n<li>부가 기능 운영을 통한 효율성 증대.</li>\n</ul>\n<p>•\tKubeadm</p>","categories":["Kubernetes","ALL"],"date":"August 30, 2024","description":"Kubernetes에 대해.","id":"e75723f1-50a3-5315-a663-135046f05e6a","keywords":["Container","Blog","Kubernetes"],"slug":"/my-first-article/About-Kubernetes-1/","title":" 🌟 [Kubernetes] About Kubernetes","readingTime":{"text":"4 min read"}},{"banner":{"alt":"Airflow","caption":"Photo by <u><a href=\"https://airflow.apache.org\">Airflow</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACO0lEQVR42oWS3WvTUBTAb+dk4qNPIvgB/gH1A31xyEAnExR88GH4MJ8EBYdPIoh78MUnBd2LWVs2sV1bV9subl0qQjs6ZpdEU21T027DNV0b1vXbmJI1TeNJU3Gg4IHce88593dzvpBaLGggqqrqm1ZTWk9zRVltw7n9l/y+1RUkXDLLVMTgwX2Vy5z9utYltf8Iypw6tD1k1jhG0bSRNI8WmespHhybuVwyyRa2trLZrCAIqRS3mk7zPN9oNIyndbh0+uDm5ZPag+GxcBiRKbT05d73PDhI+pPf5/XD99aDz86Gw6GlSMTpnC6Xy39gZvCMds2ceXz7WDCM6DTAju0qOHYURZblSqUCqyj+aDabakfAUq/X1U6OaPihEx+7c3SRRJROHqa5UlPR4aYy/uJ5uVyq1WpsImEkub6+Zre/9vm8Ql6PDvXho3tj0T3fiiYysf/z6vuaaNxLJpNWi+Xj8jLESdNUIDBPEMTU5CSGvbRMYBzHgQX1fDjfR4weyFaOCDsLkgyY0umHzWYNBgmb1cqy7Nzcu2mHQ5KkGMOsrEQpinK5nHBAJv+V3pkBFLh1M7WgR6u2YC0UChMYFo/H3W4X1CwUCnk8HkiSYRggo9EoWHAcR8g/YPIM9novojfnHrGvjJgl6We1qpcNGgMPiaIIKsCgwv9BVRQly/MI25jf5xtC7v4edz+yn7gbGzd4tW1UV/3neHRbpddQzD9Lz9wgn1yI3D9OjExtBMHY6sS/ezZ3q8ZA/wIAVki+U9pyEAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png","srcSet":"/static/de7402d2a87d0256451392f6590d157b/182cc/airflow-profile.png 91w,\n/static/de7402d2a87d0256451392f6590d157b/2c6ad/airflow-profile.png 181w,\n/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png 362w","sizes":"(min-width: 362px) 362px, 100vw"},"sources":[{"srcSet":"/static/de7402d2a87d0256451392f6590d157b/98c2e/airflow-profile.webp 91w,\n/static/de7402d2a87d0256451392f6590d157b/483ec/airflow-profile.webp 181w,\n/static/de7402d2a87d0256451392f6590d157b/def5b/airflow-profile.webp 362w","type":"image/webp","sizes":"(min-width: 362px) 362px, 100vw"}]},"width":660,"height":399.2817679558011}}}},"body":"<h1>🌬️ Apache Airflow: 공부하기</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-profile.png\" alt=\"Airflow\"></p>\n<h1>🔫 Airflow를 활용한 Faker 데이터 생성 * 활용하기</h1>\n<h2>🌟 프로젝트 개요</h2>\n<p>Airflow를 활용해 다양한 데이터 워크플로우를 구성하고, 데이터를 처리하는 방식을 공부하고 실습합니다.</p>\n<h3>📊 사용된 기술 스택</h3>\n<ul>\n<li><strong>Docker</strong>: 애플리케이션을 컨테이너로 패키징하여 독립적으로 실행</li>\n<li><strong>Apache Airflow</strong>: 워크플로우 관리 플랫폼</li>\n<li><strong>Faker</strong>: 다양한 가짜 데이터를 생성하는 라이브러리</li>\n<li><strong>Kafka</strong>: 분산 스트리밍 플랫폼</li>\n<li><strong>PostgreSQL</strong>: 관계형 데이터베이스 관리 시스템</li>\n</ul>\n<h2>⚒️ 환경 설정</h2>\n<h2>💬 진행 상황</h2>\n<h3>1. Faker 라이브러리를 통해 가짜 데이터 생성</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from faker import Faker\nimport shortuuid\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndef create_fake_user() -> dict:\n    fake = Faker()\n    fake_profile = fake.profile()\n    \n    key_list = [\"name\", \"job\", \"residence\", \"blood_group\", \"sex\", \"birthdate\"]\n    fake_dict = {}\n\n    # 선택된 키의 값을 fake_dict에 추가\n    for key in key_list:\n        fake_dict[key] = fake_profile[key]\n        \n    fake_dict[\"phone_number\"] = fake.phone_number()\n    fake_dict[\"email\"] = fake.email()\n    fake_dict[\"uuid\"] = shortuuid.uuid()\n    # YYYYMMDD 변환\n    fake_dict['birthdate'] = fake_dict['birthdate'].strftime(\"%Y%m%d\")\n    fake_dict['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    return fake_dict\n\ndef generate_fake_data(num_records: int):\n    fake_users = []\n    for _ in range(num_records):\n        user = create_fake_user()\n        fake_users.append(user)\n    return fake_users\n\nif __name__ == \"__main__\":\n    data = generate_fake_data(30)\n    for user in data:\n        print(user)</code></pre></div>\n<h3>2. 모든 airflow 데이터 postgres에 저장.</h3>\n<ul>\n<li>Airflow 데이터 목록과 제가 만든 <strong>fake_data</strong> 목록 결과입니다.</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-ing/postgres_data_list.png\" alt=\"Airflow\"></p>\n<ul>\n<li><strong>fake_data</strong>의 데이터를 확인한 결과입니다.</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-ing/postgres_data_show.png\" alt=\"Airflow\"></p>\n<h2>3. 데이터 Kafka에 전달</h2>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> Postgres에 있는 데이터를 읽어서 Kafka로 보내기.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Airflow DAG를 이용해 스케줄러를 통해 관리하기.\n<ul>\n<li>Kafka에 데이터 전송하는 과정에서 계속 문제가 발생함.</li>\n</ul>\n</li>\n</ul>\n<h1>📌 진행하고 싶은 작업</h1>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Airflow를 통해 스트리밍 데이터 배치 처리하기.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> 데이터 정합성 보장</li>\n</ul>","categories":["Airflow","ALL"],"date":"August 27, 2024","description":"Airflow 간단한 실습입니다.","id":"7eab20e5-7f02-5592-a710-08626c3dcd36","keywords":["Workflow","Pipeline","Data","Blog","Airflow","Scheduler"],"slug":"/my-first-article/About-Airflow-Practice-1/","title":" 🌟 [Airflow] Airflow Practice-1 ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org\">Kafka</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR42pWS20vCYBTA/asC6UIEQVFa/QNdnFozTUjsQummZdZLSI9dtLQsNnvpRpFlD2l/wCJXUbTtqYTc5iaImp2pmGAPdeD7OJxzft93bqpSqSTLcvz2VhRF0AuFQrFYLP1NVHCWl7wGHeJyYhXTV1kqSiNQ81bhUb1hJ7CtH9HJknQVjdLJZI38apD6VxSYeXvr7OhgGTZCkmbUZBod41gW7DzPQwn1DCgf7+/pdDqfz1d/PiTJVnXzbjCkGxrGHE5I4fnpCYqfnZ4ReF6S5Iwovr68QOTVZdRus3ncC49JOpvNqu4pStvTyzLMgEa77Q8cHx0l4nGIy2QypjHU6/GcnZxO2+0TZjNqNGIOh33StrS46JybjxCEAvf1ajiO69dodwIKfJdIAAyJtbe2WS2Wm1gMALBAU/2bW3uh0ALualGrKYpS0o4QpLqpKRQMjgwOuTAcgiBtgN047ltdPQiHZ6amJq1WI6InCeIgvL+5vrHm88FdbhjD9HR1cywHPqvZMo6ilYbB5KGw9OcndIimaUEQsmWRJCmXy6VSKQU2IEhgy4/AqGT54vw8+fBQG1K12w1rU/Eq8IrXC6N2Y/ivS1L/Si3gZ87wYew6Jgr/Xs9vFqVLFScdcPIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png","srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/966b5/kafka.png 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/37b8e/kafka.png 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/be504/kafka.png 318w","sizes":"(min-width: 318px) 318px, 100vw"},"sources":[{"srcSet":"/static/63fdbbcf1c93c7c30e2a1c96954db211/41d58/kafka.webp 80w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/d2420/kafka.webp 159w,\n/static/63fdbbcf1c93c7c30e2a1c96954db211/7b932/kafka.webp 318w","type":"image/webp","sizes":"(min-width: 318px) 318px, 100vw"}]},"width":660,"height":400.566037735849}}}},"body":"<h1>🚀 Apache Kafka: 실시간 데이터 스트리밍 플랫폼</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/kafka/kafka.png\" alt=\"Kafka\"></p>\n<h2>💬 들어가며</h2>\n<p>Apache Kafka는 요즘 데이터 세계에서 빠질 수 없는 도구 중 하나입니다. 처음에는 LinkedIn에서 시작된 프로젝트였지만, 지금은 오픈 소스가 되어 많은 기업에서 사용하고 있습니다. 대규모 데이터를 실시간으로 처리해야 하는 환경에서 특히 강점을 보이며, 다양한 서비스의 뒷단에서 데이터를 실시간으로 주고받는 데에 중요한 역할을 합니다.</p>\n<h2>💫 Kafka의 주요 개념</h2>\n<h3>1. <strong>Producer (생산자)</strong></h3>\n<ul>\n<li>Kafka에서 프로듀서는 데이터를 만드는 주체입니다. 예를 들어, 사용자가 웹사이트에서 클릭을 하면 그 정보가 프로듀서를 통해 Kafka에 전달됩니다. 이때 이 데이터는 특정 \"토픽\"에 게시됩니다.</li>\n</ul>\n<h3>2. <strong>Consumer (소비자)</strong></h3>\n<ul>\n<li>컨슈머는 반대로 Kafka에서 데이터를 가져오는 역할을 합니다. 예를 들어, 실시간으로 클릭 데이터를 분석하는 시스템이 있다면, 이 시스템이 컨슈머가 됩니다. 컨슈머는 토픽에서 데이터를 가져와 필요한 처리를 합니다.</li>\n</ul>\n<h3>3. <strong>Topic (토픽)</strong></h3>\n<ul>\n<li>토픽은 데이터를 저장하는 논리적인 통로입니다. 프로듀서가 데이터를 토픽에 넣으면, 컨슈머는 이 토픽에서 데이터를 읽어갑니다. 토픽은 여러 개의 파티션으로 나눠져 있어 병렬 처리가 가능합니다.</li>\n</ul>\n<h3>4. <strong>Partition (파티션)</strong></h3>\n<ul>\n<li>하나의 토픽은 여러 파티션으로 나뉠 수 있습니다. 파티션은 데이터의 순서를 보장해 주며, 여러 서버에 분산되어 저장되기 때문에 확장성이 뛰어납니다.</li>\n</ul>\n<h3>5. <strong>Broker (브로커)</strong></h3>\n<ul>\n<li>브로커는 Kafka 클러스터의 서버를 말합니다. 각 브로커는 특정 파티션을 관리하고, 데이터를 저장하며, 클라이언트 요청에 따라 데이터를 주고받습니다.</li>\n</ul>\n<h3>6. <strong>Zookeeper</strong></h3>\n<ul>\n<li>Kafka는 Zookeeper를 이용해 클러스터 상태를 관리합니다. 브로커가 어떻게 분포되어 있는지, 클러스터 상태가 어떤지 등의 메타데이터를 Zookeeper가 담당합니다.</li>\n</ul>\n<h2>👑 Kafka의 장점</h2>\n<h3>1. <strong>고성능 처리</strong></h3>\n<p>Kafka는 초당 수백만 건의 메시지를 처리할 수 있는 강력한 성능을 자랑합니다. 대규모 데이터 환경에서도 성능 저하 없이 데이터를 처리할 수 있습니다.</p>\n<h3>2. <strong>확장성</strong></h3>\n<p>Kafka는 필요에 따라 클러스터의 크기를 쉽게 확장할 수 있습니다. 데이터 양이 늘어나도 브로커를 추가하는 것만으로 처리 성능을 향상시킬 수 있습니다.</p>\n<h3>3. <strong>안정성</strong></h3>\n<p>Kafka는 데이터 복제를 통해 안정성을 보장합니다. 만약 한 브로커에 문제가 생기더라도, 복제된 데이터 덕분에 데이터 손실 없이 계속해서 서비스를 운영할 수 있습니다.</p>\n<h3>4. <strong>유연한 데이터 모델</strong></h3>\n<p>Kafka는 데이터를 스트림 형태로 저장합니다. 덕분에 실시간 데이터뿐만 아니라 과거 데이터를 조회하거나 분석하는 것도 가능합니다.</p>\n<h2>✅ Kafka를 활용한 다양한 사례</h2>\n<h3>1. <strong>실시간 로그 수집 및 분석</strong></h3>\n<p>웹사이트나 애플리케이션에서 발생하는 모든 로그 데이터를 Kafka에 모아 실시간으로 분석할 수 있습니다. 이를 통해 시스템의 상태를 지속적으로 모니터링하고, 문제를 사전에 발견할 수 있습니다.</p>\n<h3>2. <strong>이벤트 소싱(Event Sourcing)</strong></h3>\n<p>Kafka를 이용해 시스템의 상태 변화를 이벤트로 기록하는 패턴입니다. 이를 통해 시스템의 상태를 되돌리거나, 특정 시점의 상태를 복원할 수 있습니다.</p>\n<h3>3. <strong>메시지 큐</strong></h3>\n<p>Kafka는 기존의 메시지 큐 시스템보다 뛰어난 성능과 안정성을 제공합니다. 특히 높은 처리량을 요구하는 환경에서 유리합니다.</p>\n<h3>4. <strong>스트리밍 데이터 파이프라인</strong></h3>\n<p>여러 데이터 소스에서 데이터를 수집해, 실시간으로 분석하거나 다른 시스템에 전달하는 데 Kafka가 핵심 역할을 합니다.</p>\n<h2>🧑🏻‍💻 마무리하며</h2>\n<p>Apache Kafka는 실시간 데이터 스트리밍 환경에서 없어서는 안 될 도구라고 생각합니다.</p>\n<p>뛰어난 성능과 확장성, 안정성을 바탕으로 다양한 산업 분야에서 활발하게 사용되고 있습니다.</p>\n<p>Kafka를 활용하면 대규모 데이터를 실시간으로 처리하고, 분석하는 시스템을 구축할 수 있습니다.</p>\n<p>Kafka를 사용은 해봤지만 좀 더 디테일한 작업을 수행하려고 합니다.</p>\n<p>다음 블로그는 kafka 실습 블로그로 돌아오겠습니다.</p>","categories":["Kafka","ALL"],"date":"August 26, 2024","description":"Kafka 간단한 소개글입니다.","id":"18ee02a4-fb67-54e9-97ae-da77188fcc5e","keywords":["Pipeline","Big-Data","Blog","Kafka"],"slug":"/my-first-article/About-Kafka-1/","title":" 🚀 [Spark] About Kafka ","readingTime":{"text":"7 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org\">Spark</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABKklEQVR42o2S3UvDMBTF94crDp+UOR8m6iaCU5ko3Z6GdtBG6frdPYgvE6Z0XSuKlcZ+0I1ikxrtEMfasvNwSeD+cpKTW0pWEcYJRj91UaWszgURDKN4mcyA58C/LalB72x6f/flvM1vUeDseV4YhtCBvu8H09mMOY802r3aiZ4ffmGUAacm9rvdPDlVFRWAW4EXJEnhLo4vW62Absa2leucwpb1Ut7Y3N6q1A8be7X9amW3WjtYWy9/vFoJinHem1MYQkgMGYZ1HMcYG6PR08SYmKb56bq5gaWkruttqjMcPvJ9XlU1WVYUWQEsEAVpoA3IQX+dmfCYojqiKNM3PZYB3e41x/XJQhLlRv2IBEF6EEJFaWeK5B9FUdE/E3+UI7zKkOQPKF7mvwHvMaPrfenbCQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png","srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/41200/spark-st-cover.png 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/f979a/spark-st-cover.png 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/bc51f/spark-st-cover.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/6334b12b709ea25b787ddf070a70ab16/322ad/spark-st-cover.webp 165w,\n/static/6334b12b709ea25b787ddf070a70ab16/de3b3/spark-st-cover.webp 330w,\n/static/6334b12b709ea25b787ddf070a70ab16/2b2b5/spark-st-cover.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>💬 Apache Spark Streaming</h1>\n<div style=\"text-align: center;\">\n    <img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/streaming1.png\" alt=\"Structured Streaming\" style=\"max-width:100%; height:auto;\">\n</div>\n<h1>🌟 Apache Spark 구조적 스트리밍(Structured Streaming)</h1>\n<p>구조적 스트리밍(Structured Streaming)은 Apache Spark에서 제공하는 실시간 데이터를 처리할 수 있는 도구입니다.</p>\n<p>배치및 스트리밍 처리,두 가지 작업을 모두 처리할 수 있는 기능을 제공합니다.</p>\n<h2>✅ 주요 개념</h2>\n<h3>1. <strong>연속 처리</strong></h3>\n<p>구조적 스트리밍은 스트리밍 데이터를 <strong>DataFrame</strong> 또는 <strong>Dataset</strong> API로 처리합니다.</p>\n<p>이를 통해 기존의 정적 데이터를 처리하던 방식과 동일한 코드로 실시간 데이터를 처리할 수 있습니다.</p>\n<h3>2. <strong>내결함성(Fault Tolerance)</strong></h3>\n<p>구조적 스트리밍은 Spark의 내장된 내결함성 메커니즘을 사용하여 데이터를 처리합니다.</p>\n<p>체크포인팅(checkpointing) 등의 기능을 통해 장애 발생 시 데이터 손실을 최소화하고 정확한 일관성을 유지할 수 있습니다.</p>\n<h3>3. <strong>확장성(Scalability)</strong></h3>\n<p>구조적 스트리밍은 Spark의 분산 처리 능력을 기반으로 하기 때문에, 매우 큰 규모의 스트리밍 데이터를 처리할 수 있습니다.</p>\n<p>필요에 따라 노드를 추가하거나 등의 방법으로 높은 확장성을 가지고 있습니다.</p>\n<h3>4. <strong>일관된 배치 및 스트리밍 처리</strong></h3>\n<p>구조적 스트리밍은 배치 처리와 스트리밍 처리 간의 API 차이를 최소화하여, 동일한 코드로 두 가지 작업을 모두 처리할 수 있습니다.</p>\n<h2>⚒️ 구성 요소</h2>\n<h3>1. <strong>Input Sources</strong></h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">![Input Sources](https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-streaming3.png)</code></pre></div>\n<p>구조적 스트리밍은 다양한 입력 소스를 지원합니다. Kafka, 파일 시스템(HDFS, S3 등), 소켓 등에서 실시간 데이터를 수집할 수 있습니다.</p>\n<h3>2. <strong>Processing Engine</strong></h3>\n<p>스트리밍 데이터를 마치 배치 데이터처럼 다룰 수 있어, SQL 쿼리를 사용하여 데이터를 쉽게 변환하고 분석할 수 있습니다.</p>\n<h3>3. <strong>Output Sinks</strong></h3>\n<p>구조적 스트리밍은 다양한 출력 싱크를 지원합니다. 파일 시스템, Kafka, 콘솔, 메모리 테이블 등으로 결과 데이터를 저장하거나 전달할 수 있습니다.</p>\n<h2>💫 구조적 스트리밍의 작동 방식</h2>\n<p>구조적 스트리밍은 <strong>마이크로 배치(Micro-Batch)</strong> 방식과 <strong>연속 처리(Continuous Processing)</strong> 방식을 통해 작동.</p>\n<ul>\n<li><strong>마이크로 배치</strong>: 실시간 데이터를 짧은 간격으로 작은 배치로 묶어 처리합니다. 이는 기존의 배치 처리 방식과 비슷하지만, 매우 짧은 간격으로 실행됩니다.</li>\n<li><strong>연속 처리</strong>: 연속적으로 데이터를 처리하며, 지연 시간을 줄이고 실시간성을 높여줍니다.</li>\n</ul>\n<h2>⚙️ 구조적 스트리밍의 주요 기능</h2>\n<h3>1. <strong>트리거(Trigger)</strong></h3>\n<p>트리거는 데이터 처리의 주기를 정의합니다. 기본값은 <code class=\"language-text\">micro-batch</code>로, 일정한 간격으로 데이터를 처리합니다.</p>\n<p>트리거 설정을 통해 주기를 선택할 수 있습니다.</p>\n<h3>2. <strong>상태 저장 처리(Stateful Processing)</strong></h3>\n<p>구조적 스트리밍은 윈도우 연산, 조인, 집계 등의 상태 저장 연산을 지원합니다.</p>\n<h3>3. <strong>Watermarking</strong></h3>\n<p>이벤트 타임 기반의 데이터에서 늦게 도착하는 데이터를 처리하기 위해 Watermarking을 사용할 수 있습니다.</p>\n<p>구조적 스트리밍은 데이터의 지연 허용 및 처리 가능 시간대를 설정할 수 있습니다.</p>\n<h2>🚀 구조적 스트리밍의 장점</h2>\n<ul>\n<li><strong>사용의 용이성</strong>: 기존의 배치 처리 코드와 동일한 API를 사용하여 스트리밍 데이터를 처리할 수 있어 학습 곡선이 낮습니다.</li>\n<li><strong>높은 신뢰성</strong>: 내장된 내결함성 메커니즘을 통해 데이터 손실을 최소화할 수 있습니다.</li>\n<li><strong>확장성</strong>: Spark의 분산 처리 능력을 통해 대규모 데이터 스트림을 효율적으로 처리할 수 있습니다.</li>\n<li><strong>유연성</strong>: 다양한 입력 소스 및 출력 싱크를 지원하여, 다양한 환경에서 사용할 수 있습니다.</li>\n</ul>\n<h2>🧑🏻‍💻 결론</h2>\n<p>스트리밍 처리는 데이터의 순서처리 , 중복제거 등 복잡하고 오류가 발생하기 쉬운 작업이 많다고 생각합니다.</p>\n<p>spark의 구조적 스트리밍으로 처리하면 마치 배치 처리 코드와 거의 비슷한 방식으로 스트리밍 처리가 가능하기에 어려움이 상당히 해소 될 거라고 생각합니다.</p>\n<p>마이크로 배치로 처리를 한다면 스트림을 작은 배치 단위로 넣어서 처리하기에 익숙한 패턴으로 스트림 데이터를 처리 할 수 있을거라 생각합니다.</p>\n<p>실시간 데이터 처리에 진입 장벽을 낮추는데 도움이 되는 도구라고 생각합니다.</p>\n<p>감사합니다.</p>","categories":["Spark","Streaming","ALL"],"date":"August 25, 2024","description":"Spark의 구조적 스트리밍에 대해.","id":"f6ffd767-63f4-5b5c-b2fb-ac2874e84141","keywords":["Pipeline","Structured Streaming","Blog","Spark"],"slug":"/my-first-article/About-Spark-Structured-Streaming/","title":" 🌟 [Spark] Structured Streaming -1 ","readingTime":{"text":"7 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org\">Spark</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACQklEQVR42mP4Twz4+xdIfD2x+Wmm4d+Pb0Ai//4BCQbCOoHqfv8C0u+XND5OUHk7u/TXk9ufd857v6yF4S/Y1H9gAGfAAdySv18/Pit1+HZ218uW0NfdcY+i5V61RzGg2gHV/+fPHxDj718g/9fDq2+mZL9oCXu/oPbft89AL3w5tPp1T+Lf718YHj9+DFR38MChM2fOHjly9Nix4ydPnLx29drx4yeOHT5y+tL170fW3HFj+jAh+c/Xj2DT/r9sCPi0bRbIz4H+wa9evb59+/aDBw8/fPhw8cLFG9dvvHn95smTJ3fv3rt16xbISR8eH+kqmN7b/evf/xfXz39e1/u82vP/n98Mz58///79+4b1G/JzCxYsWAh084/vP6ZOmZadmbN2zVqgxn1790+fNa+ps6+1pb29d/K27oqPVS4PIiR/PrjC8OPHj5kzZqUkpx08eCg4MHTtmnWtLW1VldX79x3w8fLbumVbVGTMimXLmxuazE0t58yeAzTuz6d3v57c+vf3D0N+XmF6WubVq9eAorNmzU5NSc9Iz3z79i2Q297aUVNVW1xY+uvXr+KiUhcnt+nTZoBj/S8kgBkqyio93b0m9E/cvWt3aEj4kcNHYqLigM7eu3dfVmZOZUV1cWHJz58//X0DgMEZEhy2bt16oLbfv36CNAODNyIsqrysEmj/kcNHgUJnz5zNSM9qaW5dtXI1EG3csOnP7z8tzW1fv34FKu5o73z54iUQgDRv2by1ob4R7hhowsCe0v6hyQIAb+47xF+wYqUAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png","srcSet":"/static/be399145ad033fb2b5626e243b7bde07/4ee5e/spark-cover.png 78w,\n/static/be399145ad033fb2b5626e243b7bde07/b1f1f/spark-cover.png 156w,\n/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png 312w","sizes":"(min-width: 312px) 312px, 100vw"},"sources":[{"srcSet":"/static/be399145ad033fb2b5626e243b7bde07/ddb87/spark-cover.webp 78w,\n/static/be399145ad033fb2b5626e243b7bde07/e77d5/spark-cover.webp 156w,\n/static/be399145ad033fb2b5626e243b7bde07/b6f70/spark-cover.webp 312w","type":"image/webp","sizes":"(min-width: 312px) 312px, 100vw"}]},"width":660,"height":399.8076923076923}}}},"body":"<h1>🚀 Apache Spark</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-cover.png\" alt=\"Spark\"></p>\n<h1>🌟 Spark Streaming vs Structured Streaming: 둘의 차이점은?</h1>\n<p>실시간 스트리밍 데이터 처리를 위해 두 가지 API를 제공합니다:</p>\n<p><strong>Spark Streaming</strong>과 <strong>Structured Streaming</strong>.</p>\n<p>둘은 비슷해 보이지만 차이점이 존재하여 블로그를 작성합니다.</p>\n<h2>💬 기본 개념</h2>\n<h3>Spark Streaming</h3>\n<ul>\n<li><strong>개념:</strong> Spark Streaming은 <strong>마이크로 배치 (Micro-batching)</strong> 개념에 기반하여 실시간 데이터를 처리합니다. 스트리밍 데이터를 작은 배치로 나누어 정기적으로 처리합니다.</li>\n<li><strong>처리 방식:</strong> 일정 간격으로 데이터를 수집한 후, 이를 RDD 형태로 변환하여 처리합니다.</li>\n<li><strong>출시:</strong> Spark Streaming은 Apache Spark의 초기 버전에서 도입된 스트리밍 API입니다.</li>\n</ul>\n<h3>Structured Streaming</h3>\n<ul>\n<li><strong>개념:</strong> Structured Streaming은 <strong>엔드-투-엔드 (End-to-End)</strong> 스트리밍 프레임워크로, 데이터 처리를 \"Continuous\"하게 수행합니다. DataFrame과 Dataset API를 기반으로 하며, 처리 논리가 더 명확하고 쉽게 작성될 수 있습니다.</li>\n<li><strong>처리 방식:</strong> 실시간 데이터를 DataFrame 또는 Dataset 형태로 처리하며, 배치와 스트리밍 작업을 동일한 방식으로 실행할 수 있습니다.</li>\n<li><strong>출시:</strong> Structured Streaming은 Spark 2.0에서 도입.</li>\n</ul>\n<h2>❗️ 주요 차이점</h2>\n<table>\n<thead>\n<tr>\n<th><strong>특징</strong></th>\n<th><strong>Spark Streaming</strong></th>\n<th><strong>Structured Streaming</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>처리 모델</strong></td>\n<td>마이크로 배칭</td>\n<td>Continuous 처리</td>\n</tr>\n<tr>\n<td><strong>API</strong></td>\n<td>RDD 기반</td>\n<td>DataFrame/Dataset 기반</td>\n</tr>\n<tr>\n<td><strong>고장 복구</strong></td>\n<td>체크포인트 기반</td>\n<td>체크포인트 및 트랜잭션 로그 기반</td>\n</tr>\n<tr>\n<td><strong>성능</strong></td>\n<td>지연 시간이 상대적으로 길다</td>\n<td>낮은 지연 시간 및 고성능</td>\n</tr>\n<tr>\n<td><strong>내부 엔진</strong></td>\n<td>DStream 엔진</td>\n<td>Catalyst 옵티마이저 사용</td>\n</tr>\n<tr>\n<td><strong>정확한 처리 (Exactly-Once)</strong></td>\n<td>상태 저장 스트리밍에서만 지원</td>\n<td>기본적으로 Exactly-Once 보장</td>\n</tr>\n<tr>\n<td><strong>윈도우 지원</strong></td>\n<td>지원</td>\n<td>기본 제공</td>\n</tr>\n<tr>\n<td><strong>통합성</strong></td>\n<td>배치와 스트리밍 코드가 다름</td>\n<td>배치와 스트리밍 코드 통합</td>\n</tr>\n</tbody>\n</table>\n<h2>📚 코드 예시 비교</h2>\n<h3>Spark Streaming 코드 예시</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from pyspark import SparkConf\nfrom pyspark.streaming import StreamingContext\n\n# SparkConf 설정\nconf = SparkConf().setAppName(\"Spark Streaming Example\")\n\n# StreamingContext 생성\nssc = StreamingContext(conf, 1)\n\n# 소켓에서 스트리밍 데이터 수신\nlines = ssc.socketTextStream(\"localhost\", 9999)\n\n# 단어 분리\nwords = lines.flatMap(lambda line: line.split(\" \"))\n\n# 단어 빈도 계산\nwordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n\n# 콘솔에 출력\nwordCounts.pprint()\n\n# 스트리밍 시작\nssc.start()\nssc.awaitTermination()</code></pre></div>","categories":["Spark","ALL"],"date":"August 25, 2024","description":"Streaming , Structured의 차이 ","id":"a3842320-86ae-520d-90a4-6f60cf45bf3f","keywords":["Pipeline","Big-Data","Blog","Spark"],"slug":"/my-first-article/About-Spark-Streaming_vs_Structured/","title":" 🌟 [Spark] Streaming VS Structured ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org\">Spark</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACQklEQVR42mP4Twz4+xdIfD2x+Wmm4d+Pb0Ai//4BCQbCOoHqfv8C0u+XND5OUHk7u/TXk9ufd857v6yF4S/Y1H9gAGfAAdySv18/Pit1+HZ218uW0NfdcY+i5V61RzGg2gHV/+fPHxDj718g/9fDq2+mZL9oCXu/oPbft89AL3w5tPp1T+Lf718YHj9+DFR38MChM2fOHjly9Nix4ydPnLx29drx4yeOHT5y+tL170fW3HFj+jAh+c/Xj2DT/r9sCPi0bRbIz4H+wa9evb59+/aDBw8/fPhw8cLFG9dvvHn95smTJ3fv3rt16xbISR8eH+kqmN7b/evf/xfXz39e1/u82vP/n98Mz58///79+4b1G/JzCxYsWAh084/vP6ZOmZadmbN2zVqgxn1790+fNa+ps6+1pb29d/K27oqPVS4PIiR/PrjC8OPHj5kzZqUkpx08eCg4MHTtmnWtLW1VldX79x3w8fLbumVbVGTMimXLmxuazE0t58yeAzTuz6d3v57c+vf3D0N+XmF6WubVq9eAorNmzU5NSc9Iz3z79i2Q297aUVNVW1xY+uvXr+KiUhcnt+nTZoBj/S8kgBkqyio93b0m9E/cvWt3aEj4kcNHYqLigM7eu3dfVmZOZUV1cWHJz58//X0DgMEZEhy2bt16oLbfv36CNAODNyIsqrysEmj/kcNHgUJnz5zNSM9qaW5dtXI1EG3csOnP7z8tzW1fv34FKu5o73z54iUQgDRv2by1ob4R7hhowsCe0v6hyQIAb+47xF+wYqUAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png","srcSet":"/static/be399145ad033fb2b5626e243b7bde07/4ee5e/spark-cover.png 78w,\n/static/be399145ad033fb2b5626e243b7bde07/b1f1f/spark-cover.png 156w,\n/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png 312w","sizes":"(min-width: 312px) 312px, 100vw"},"sources":[{"srcSet":"/static/be399145ad033fb2b5626e243b7bde07/ddb87/spark-cover.webp 78w,\n/static/be399145ad033fb2b5626e243b7bde07/e77d5/spark-cover.webp 156w,\n/static/be399145ad033fb2b5626e243b7bde07/b6f70/spark-cover.webp 312w","type":"image/webp","sizes":"(min-width: 312px) 312px, 100vw"}]},"width":660,"height":399.8076923076923}}}},"body":"<h1>💬 Apache Spark</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-cover.png\" alt=\"Spark\"></p>\n<h2>💫 RDD란 무엇인가?</h2>\n<p><a href=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/rdd-2.png\">RDD</a></p>\n<p><strong>RDD</strong>(Resilient Distributed Dataset)는 Apache Spark의 핵심 데이터 구조로,</p>\n<p>읽기 전용 권한이 있는 분산 메모리 컬렉션 이며, 가장 중요한 것은 내결함성이 있습니다.</p>\n<p>RDD는 대규모 데이터를 다루는 데 있어 강력한 성능과 내결함성(Fault-Tolerance)을 제공하며, 여러 클러스터 노드에 걸쳐 데이터를 분산 처리할 수 있게 해줍니다.</p>\n<p><a href=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/rdd-1.png\">RDD</a></p>\n<h3>✓ RDD의 주요 특징</h3>\n<ul>\n<li><strong>불변성(Immutability)</strong>: RDD는 한 번 생성되면 변경할 수 없습니다. 모든 변환(Transformation)은 기존 RDD를 바탕으로 새로운 RDD를 생성합니다.</li>\n<li><strong>내결함성(Fault-Tolerance)</strong>: RDD는 데이터를 복구할 수 있는 lineage 정보를 저장하여, 작업 도중 일부 데이터가 손실되더라도 복구할 수 있습니다.</li>\n<li><strong>병렬 처리(Parallel Processing)</strong>: RDD는 클러스터의 여러 노드에서 데이터를 병렬로 처리하여 대규모 데이터를 효율적으로 처리할 수 있습니다.</li>\n<li><strong>Lazy Evaluation</strong>: RDD의 변환은 즉시 실행되지 않으며, 액션(Action)이 호출될 때까지 지연되어 실행 계획을 최적화합니다.</li>\n</ul>\n<h2>🚀 RDD의 작동 방식</h2>\n<h3>RDD의 변환(Transformations)</h3>\n<p>변환은 RDD를 입력으로 받아서 새로운 RDD를 생성하는 함수입니다. 변환은 Lazy Evaluation으로 실행되며, 그 결과는 액션이 호출될 때 계산됩니다.</p>\n<ul>\n<li><strong>map(func)</strong>: 각 요소에 대해 함수 <code class=\"language-text\">func</code>를 적용하여 새로운 RDD를 생성합니다.</li>\n<li><strong>filter(func)</strong>: 조건에 맞는 요소들만을 포함하는 새로운 RDD를 생성합니다.</li>\n<li><strong>flatMap(func)</strong>: 각 요소를 여러 개의 요소로 매핑하여 펼쳐진(flattened) RDD를 생성합니다.</li>\n</ul>\n<h3>✓ RDD의 액션(Actions)</h3>\n<p>액션은 RDD에서 결과를 추출하거나 외부 저장소에 저장하는 연산입니다. 액션을 호출하면 변환된 RDD가 실제로 평가됩니다.</p>\n<ul>\n<li><strong>collect()</strong>: 모든 요소를 드라이버 프로그램으로 반환합니다.</li>\n<li><strong>count()</strong>: RDD의 요소 수를 셉니다.</li>\n<li><strong>saveAsTextFile(path)</strong>: RDD를 텍스트 파일로 저장합니다.</li>\n</ul>\n<h2>⚒️ RDD와 다른 데이터 구조 비교</h2>\n<h3>✓ RDD vs DataFrame</h3>\n<p><strong>DataFrame</strong>은 구조화된 데이터의 분산 컬렉션으로, 스키마를 가지며 SQL 쿼리와 같은 방식으로 데이터를 다룰 수 있습니다. DataFrame은 RDD의 상위 추상화로 볼 수 있습니다.</p>\n<ul>\n<li><strong>API</strong>: DataFrame은 SQL 쿼리와 유사한 고수준의 API를 제공하여, 데이터를 더 쉽게 조작할 수 있습니다. 반면 RDD는 저수준 API로, 더 세밀한 제어가 가능합니다.</li>\n<li><strong>성능</strong>: DataFrame은 Catalyst 옵티마이저를 사용하여 쿼리 최적화가 가능하므로, RDD보다 일반적으로 더 높은 성능을 제공합니다.</li>\n<li><strong>타입 안정성</strong>: RDD는 타입 안정성을 제공하지만, DataFrame은 런타임에 스키마가 적용되므로 컴파일 시 타입 검사가 불가능할 수 있습니다.</li>\n</ul>\n<h3>✓ RDD vs Dataset</h3>\n<p><strong>Dataset</strong>은 DataFrame과 유사하지만, 타입 안정성과 객체 지향 프로그래밍을 지원합니다. Dataset은 RDD와 DataFrame의 장점을 결합한 데이터 구조입니다.</p>\n<ul>\n<li><strong>타입 안정성</strong>: Dataset은 강력한 타입 안정성을 제공하여, 컴파일 타임에 데이터 타입을 검증할 수 있습니다.</li>\n<li><strong>성능</strong>: Dataset은 DataFrame처럼 Catalyst 옵티마이저를 사용하여 높은 성능을 제공하지만, RDD보다 성능 면에서 더 유리할 수 있습니다.</li>\n<li><strong>유연성</strong>: Dataset은 RDD처럼 복잡한 로직을 쉽게 구현할 수 있으며, 동시에 DataFrame처럼 고수준 API를 사용할 수 있습니다.</li>\n</ul>\n<h2>🌟 RDD의 장단점</h2>\n<h3>✓ 장점</h3>\n<ul>\n<li><strong>유연성</strong>: 다양한 데이터를 처리할 수 있는 유연한 API를 제공합니다.</li>\n<li><strong>내결함성</strong>: 데이터 손실이 발생해도 lineage를 통해 복구가 가능합니다.</li>\n<li><strong>병렬 처리</strong>: 대규모 데이터를 분산 처리하여 성능을 극대화할 수 있습니다.</li>\n</ul>\n<h3>✓ 단점</h3>\n<ul>\n<li><strong>성능</strong>: DataFrame이나 Dataset에 비해 성능이 낮을 수 있습니다. (복잡한 쿼리일수록 더욱 성능 낮아짐)</li>\n<li><strong>복잡성</strong>: 저수준 API를 사용해야 하므로, 복잡한 작업을 구현할 때 더 많은 코드가 필요할 수 있습니다.</li>\n</ul>\n<h2>🧑🏻‍💻 결론</h2>\n<p>Apache Spark의 RDD는 빅데이터 처리에 있어 강력하고 꼭 필요한 도구라고 생각됩니다.</p>\n<p>MapReduce와 비교했을 때, 인메모리 데이터 공유는 RDD를 네트워크 및 디스크 공유보다 10~100배 더 빠르다는 말을 들어본 적 있다.</p>\n<p>하지만 이것은 <strong>절대적인 기준이 아니라 상대적인 성능 차이</strong>라는 걸 생각해야한다.</p>\n<p><strong>메모리 내 연산</strong>을 하기에 반복적인 작업(머신러닝) 을 더욱 우수한 성능으로 진행할 수 있고</p>\n<p><strong>dag의 최적화, Lazy Evaluation</strong>(작업을 지연하여 실제로 액션이 호출될 때 까지 실행을 하지 않는다 -> 전체 작업 계획 최적화)</p>\n<p>또, 우수한 커뮤니티 생태계 등의 이유로 말하는 것이라고 생각한다.</p>\n<p>충분한 메모리 자원이 없을 땐 오히려 디스크 I/O를 사용하는 Map Reduce가 우수할 수 있기 때문이다.</p>\n<p>상황과 환경에 맞게 데이터 처리에 적합한 도구를 선택해야 하고</p>\n<p>그러기 위해 어떤 환경에서 어떤 도구가 효율적이고 우수한 지 공부할 필요가 확실하게 있다.</p>\n<p><a href=\"https://edureka.co/blog/rdd-using-spark/#pokemon-use-case\">참고로 본 사이트</a>에서 포켓몬 사용 사례를 한 번 보고 해보는 것도 이해하는데 도움이 될 거 같다.</p>","categories":["Spark","ALL"],"date":"August 23, 2024","description":"Spark RDD","id":"362fa633-32b5-52fd-94ad-efaf53af5c51","keywords":["Pipeline","RDD","Blog","Spark"],"slug":"/my-first-article/About-Spark-RDD/","title":" 🌟 [Spark] Spark RDD","readingTime":{"text":"8 min read"}},{"banner":{"alt":"Spark","caption":"Photo by <u><a href=\"https://spark.apache.org\">Spark</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACQklEQVR42mP4Twz4+xdIfD2x+Wmm4d+Pb0Ai//4BCQbCOoHqfv8C0u+XND5OUHk7u/TXk9ufd857v6yF4S/Y1H9gAGfAAdySv18/Pit1+HZ218uW0NfdcY+i5V61RzGg2gHV/+fPHxDj718g/9fDq2+mZL9oCXu/oPbft89AL3w5tPp1T+Lf718YHj9+DFR38MChM2fOHjly9Nix4ydPnLx29drx4yeOHT5y+tL170fW3HFj+jAh+c/Xj2DT/r9sCPi0bRbIz4H+wa9evb59+/aDBw8/fPhw8cLFG9dvvHn95smTJ3fv3rt16xbISR8eH+kqmN7b/evf/xfXz39e1/u82vP/n98Mz58///79+4b1G/JzCxYsWAh084/vP6ZOmZadmbN2zVqgxn1790+fNa+ps6+1pb29d/K27oqPVS4PIiR/PrjC8OPHj5kzZqUkpx08eCg4MHTtmnWtLW1VldX79x3w8fLbumVbVGTMimXLmxuazE0t58yeAzTuz6d3v57c+vf3D0N+XmF6WubVq9eAorNmzU5NSc9Iz3z79i2Q297aUVNVW1xY+uvXr+KiUhcnt+nTZoBj/S8kgBkqyio93b0m9E/cvWt3aEj4kcNHYqLigM7eu3dfVmZOZUV1cWHJz58//X0DgMEZEhy2bt16oLbfv36CNAODNyIsqrysEmj/kcNHgUJnz5zNSM9qaW5dtXI1EG3csOnP7z8tzW1fv34FKu5o73z54iUQgDRv2by1ob4R7hhowsCe0v6hyQIAb+47xF+wYqUAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png","srcSet":"/static/be399145ad033fb2b5626e243b7bde07/4ee5e/spark-cover.png 78w,\n/static/be399145ad033fb2b5626e243b7bde07/b1f1f/spark-cover.png 156w,\n/static/be399145ad033fb2b5626e243b7bde07/5d48c/spark-cover.png 312w","sizes":"(min-width: 312px) 312px, 100vw"},"sources":[{"srcSet":"/static/be399145ad033fb2b5626e243b7bde07/ddb87/spark-cover.webp 78w,\n/static/be399145ad033fb2b5626e243b7bde07/e77d5/spark-cover.webp 156w,\n/static/be399145ad033fb2b5626e243b7bde07/b6f70/spark-cover.webp 312w","type":"image/webp","sizes":"(min-width: 312px) 312px, 100vw"}]},"width":660,"height":399.8076923076923}}}},"body":"<h1>💬 Apache Spark 소개</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/spark-cover.png\" alt=\"Spark\"></p>\n<h2>✅ 개요</h2>\n<p><strong>Apache Spark</strong>는 대규모 데이터 처리에 최적화된 오픈 소스 분산 처리 시스템입니다.</p>\n<p>주로 빅데이터 분석에 사용되며, 빠른 속도와 유연성을 제공합니다.</p>\n<p>Spark는 메모리 내에서 데이터를 처리하기 때문에, 기존의 하둡(MapReduce)보다 훨씬 빠르게 데이터 분석 작업을 수행할 수 있습니다.</p>\n<h2>⚙️ 주요 특징</h2>\n<ul>\n<li><strong>속도</strong>: 메모리 내에서 연산을 수행하여 디스크 기반의 처리보다 최대 100배 빠른 성능을 자랑합니다.</li>\n<li><strong>사용 편의성</strong>: 다양한 프로그래밍 언어(Scala, Python, Java, R)를 지원하며, 간단한 API로 복잡한 작업을 쉽게 구현할 수 있습니다.</li>\n<li><strong>유연성</strong>: 배치 처리, 스트리밍 처리, SQL, 머신 러닝, 그래프 처리 등 다양한 데이터 처리 작업을 하나의 통합된 프레임워크에서 수행할 수 있습니다.</li>\n<li><strong>확장성</strong>: 수천 대의 노드를 사용하는 클러스터에서 실행할 수 있어, 페타바이트 규모의 데이터도 효율적으로 처리할 수 있습니다.</li>\n</ul>\n<h2>⚒️ Spark의 주요 구성 요소</h2>\n<ul>\n<li><strong>Spark Core</strong>: Spark의 기본 엔진으로, 작업 스케줄링, 메모리 관리, 장애 복구 등의 기능을 제공합니다.</li>\n<li><strong>Spark SQL</strong>: SQL과 비슷한 방식으로 구조화된 데이터를 처리할 수 있는 모듈입니다. 데이터 프레임(DataFrame)과 데이터셋(Dataset) API를 지원합니다.</li>\n<li><strong>Spark Streaming</strong>: 실시간 스트리밍 데이터를 처리할 수 있는 모듈로, 배치 작업을 마이크로 배치로 전환하여 스트리밍 데이터를 처리합니다.</li>\n<li><strong>MLlib</strong>: 머신 러닝 라이브러리로, 다양한 알고리즘과 도구들을 제공하여 빅데이터에 대한 머신 러닝 작업을 지원합니다.</li>\n<li><strong>GraphX</strong>: 그래프와 병렬 연산을 위한 라이브러리로, 그래프 데이터에 대한 분석 작업을 수행할 수 있습니다.</li>\n</ul>\n<h2>📊 Spark의 활용 사례</h2>\n<ul>\n<li><strong>데이터 분석</strong>: 대규모 로그 데이터 분석, 추천 시스템 구현, 사용자 행동 분석 등 다양한 빅데이터 분석 작업에 활용됩니다.</li>\n<li><strong>실시간 처리</strong>: 실시간 트랜잭션 분석, 실시간 데이터 스트리밍 처리, 실시간 데이터 시각화 등에 사용됩니다.</li>\n<li><strong>머신 러닝</strong>: 대규모 데이터셋에 대한 머신 러닝 모델 학습, 데이터 전처리 및 피처 엔지니어링, 예측 모델링 등에 활용됩니다.</li>\n</ul>\n<h2>🧑🏻‍💻 결론</h2>\n<p>Apache Spark는 빠르고 유연하며 확장 가능한 데이터 처리 시스템으로, 빅데이터 시대의 필수 도구로 자리잡고 있습니다.</p>\n<p>데이터를 처리하고 분석하는 도구로 많이 사용하기 때문에 많은 실습과 공부를 해야겠다는 생각이 듭니다.</p>\n<p>Spark-Streaming을 적용해서 프로젝트를 한 번 진행해야겠습니다. 😊</p>\n<p>감사합니다.</p>","categories":["Spark","ALL"],"date":"August 22, 2024","description":"Spark 간단한 소개글입니다.","id":"9556aa93-182b-59dc-96f5-a8218da549d6","keywords":["Pipeline","Big-Data","Blog","Spark"],"slug":"/my-first-article/About-Spark-Introduce/","title":" 🌟 [Spark] About Spark ","readingTime":{"text":"5 min read"}},{"banner":{"alt":"Airflow","caption":"Photo by <u><a href=\"https://airflow.apache.org\">Airflow</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACO0lEQVR42oWS3WvTUBTAb+dk4qNPIvgB/gH1A31xyEAnExR88GH4MJ8EBYdPIoh78MUnBd2LWVs2sV1bV9subl0qQjs6ZpdEU21T027DNV0b1vXbmJI1TeNJU3Gg4IHce88593dzvpBaLGggqqrqm1ZTWk9zRVltw7n9l/y+1RUkXDLLVMTgwX2Vy5z9utYltf8Iypw6tD1k1jhG0bSRNI8WmespHhybuVwyyRa2trLZrCAIqRS3mk7zPN9oNIyndbh0+uDm5ZPag+GxcBiRKbT05d73PDhI+pPf5/XD99aDz86Gw6GlSMTpnC6Xy39gZvCMds2ceXz7WDCM6DTAju0qOHYURZblSqUCqyj+aDabakfAUq/X1U6OaPihEx+7c3SRRJROHqa5UlPR4aYy/uJ5uVyq1WpsImEkub6+Zre/9vm8Ql6PDvXho3tj0T3fiiYysf/z6vuaaNxLJpNWi+Xj8jLESdNUIDBPEMTU5CSGvbRMYBzHgQX1fDjfR4weyFaOCDsLkgyY0umHzWYNBgmb1cqy7Nzcu2mHQ5KkGMOsrEQpinK5nHBAJv+V3pkBFLh1M7WgR6u2YC0UChMYFo/H3W4X1CwUCnk8HkiSYRggo9EoWHAcR8g/YPIM9novojfnHrGvjJgl6We1qpcNGgMPiaIIKsCgwv9BVRQly/MI25jf5xtC7v4edz+yn7gbGzd4tW1UV/3neHRbpddQzD9Lz9wgn1yI3D9OjExtBMHY6sS/ezZ3q8ZA/wIAVki+U9pyEAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png","srcSet":"/static/de7402d2a87d0256451392f6590d157b/182cc/airflow-profile.png 91w,\n/static/de7402d2a87d0256451392f6590d157b/2c6ad/airflow-profile.png 181w,\n/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png 362w","sizes":"(min-width: 362px) 362px, 100vw"},"sources":[{"srcSet":"/static/de7402d2a87d0256451392f6590d157b/98c2e/airflow-profile.webp 91w,\n/static/de7402d2a87d0256451392f6590d157b/483ec/airflow-profile.webp 181w,\n/static/de7402d2a87d0256451392f6590d157b/def5b/airflow-profile.webp 362w","type":"image/webp","sizes":"(min-width: 362px) 362px, 100vw"}]},"width":660,"height":399.2817679558011}}}},"body":"<h1>🌬️ Apache Airflow: 공부하기</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-profile.png\" alt=\"Airflow\"></p>\n<h2>✅ DAG 파일 배치</h2>\n<p>airflow의 dags 폴더에 배치하면 자동으로 인식됩니다.</p>\n<h2>✅ 의존성 설정</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">start 태스크가 end 태스크 전에 실행되도록 설정하는 방법입니다.\n\nstart >> end</code></pre></div>\n<h2>✅  효율적인 DAG 관리 팁</h2>\n<p><strong>DAG의 동적 생성 피하기</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">동적 DAG 생성 자제: 동적으로 DAG를 생성하는 것은 코드 복잡성을 높이고, 성능 문제를 일으킬 수 있습니다. \n\n대신, 파라미터화된 태스크를 사용하여 유사한 작업을 수행하는 여러 태스크를 생성하는 것이 좋습니다.</code></pre></div>\n<p><strong>에러 핸들링</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">테스크 실패 시 알람 : 이메일 혹은 슬랙 알람을 설정해서 확인이 가능합니다. 저는 개인적으로 슬랙을 선호합니다. 간단하게 확인하기 좋아요.</code></pre></div>\n<p><strong>성능 최적화</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">병렬 실행 활용 : 병렬로 실행되도록 실행하여 전체 실행 시간을 단축할 수 있습니다.</code></pre></div>\n<p><strong>리소스 제한</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">resource 파라미터 조정으로 CPU나 메모리 사용량을 조절하여 특정 테스크가 너무 많은 리소스를 사용하지 않도록 방지할 수 있습니다.</code></pre></div>\n<p><strong>DAG 실행 모니터링</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">내장 로그 외에도 Prometheus, Granfana등 외부 모니터링 도구를 사용하여 메트릭 수집 후 시각화가 가능합니다.\n\ngrafana, prometheus는 경험이 있어 이것도 진행할 생각입니다.</code></pre></div>\n<p><strong>에러 처리 및 예외 처리</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">발생할 수 있는 예외를 처리하여 중단되지 않도록 할 수 있습니다.</code></pre></div>\n<h2>💬 느끼는 점</h2>\n<p>지금까지 airflow를 집중적으로 다뤄본 적 없어서 생각보다 여러가지로 복잡하다.\n하지만 데이터 엔지니어에게 필수적이라고 할 만큼 중요한 기술 스택이기에, 집중해서 공부하고\n다양한 프로젝트에서 사용하여 숙련도를 확실하게 높여야겠다.</p>","categories":["Airflow","ALL"],"date":"August 21, 2024","description":"Airflow 간단한 소개글입니다. - 기초 내용","id":"6a4258b4-d4ac-56e0-a4d9-0decf3dec7ee","keywords":["Workflow","Pipeline","Data","Blog","Airflow","Scheduler"],"slug":"/my-first-article/About-Airflow-2/","title":" 🌟 [Airflow] Airflow Tip ","readingTime":{"text":"3 min read"}},{"banner":{"alt":"Airflow","caption":"Photo by <u><a href=\"https://airflow.apache.org\">Airflow</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACO0lEQVR42oWS3WvTUBTAb+dk4qNPIvgB/gH1A31xyEAnExR88GH4MJ8EBYdPIoh78MUnBd2LWVs2sV1bV9subl0qQjs6ZpdEU21T027DNV0b1vXbmJI1TeNJU3Gg4IHce88593dzvpBaLGggqqrqm1ZTWk9zRVltw7n9l/y+1RUkXDLLVMTgwX2Vy5z9utYltf8Iypw6tD1k1jhG0bSRNI8WmespHhybuVwyyRa2trLZrCAIqRS3mk7zPN9oNIyndbh0+uDm5ZPag+GxcBiRKbT05d73PDhI+pPf5/XD99aDz86Gw6GlSMTpnC6Xy39gZvCMds2ceXz7WDCM6DTAju0qOHYURZblSqUCqyj+aDabakfAUq/X1U6OaPihEx+7c3SRRJROHqa5UlPR4aYy/uJ5uVyq1WpsImEkub6+Zre/9vm8Ql6PDvXho3tj0T3fiiYysf/z6vuaaNxLJpNWi+Xj8jLESdNUIDBPEMTU5CSGvbRMYBzHgQX1fDjfR4weyFaOCDsLkgyY0umHzWYNBgmb1cqy7Nzcu2mHQ5KkGMOsrEQpinK5nHBAJv+V3pkBFLh1M7WgR6u2YC0UChMYFo/H3W4X1CwUCnk8HkiSYRggo9EoWHAcR8g/YPIM9novojfnHrGvjJgl6We1qpcNGgMPiaIIKsCgwv9BVRQly/MI25jf5xtC7v4edz+yn7gbGzd4tW1UV/3neHRbpddQzD9Lz9wgn1yI3D9OjExtBMHY6sS/ezZ3q8ZA/wIAVki+U9pyEAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png","srcSet":"/static/de7402d2a87d0256451392f6590d157b/182cc/airflow-profile.png 91w,\n/static/de7402d2a87d0256451392f6590d157b/2c6ad/airflow-profile.png 181w,\n/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png 362w","sizes":"(min-width: 362px) 362px, 100vw"},"sources":[{"srcSet":"/static/de7402d2a87d0256451392f6590d157b/98c2e/airflow-profile.webp 91w,\n/static/de7402d2a87d0256451392f6590d157b/483ec/airflow-profile.webp 181w,\n/static/de7402d2a87d0256451392f6590d157b/def5b/airflow-profile.webp 362w","type":"image/webp","sizes":"(min-width: 362px) 362px, 100vw"}]},"width":660,"height":399.2817679558011}}}},"body":"<h1>🌬️ Apache Airflow: 데이터 파이프라인을 코드로 관리하는 방법</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/airflow-profile.png\" alt=\"Airflow\"></p>\n<h2>💬 소개</h2>\n<p>현대 데이터 엔지니어링과 머신러닝 워크플로우에서 데이터 파이프라인은 핵심적인 역할을 합니다. Apache Airflow는 이러한 파이프라인을 쉽고 효율적으로 관리할 수 있도록 도와주는 강력한 오픈 소스 플랫폼입니다. Airflow는 워크플로우를 스케줄링하고 모니터링하며, 데이터 파이프라인을 코드로 정의할 수 있게 해줍니다.</p>\n<h2>🌟 Airflow란 무엇인가요?</h2>\n<p>Apache Airflow는 <strong>워크플로우 자동화 및 스케줄링 도구</strong>로, 복잡한 데이터 파이프라인을 관리하고 모니터링하는 데 사용됩니다. 데이터 수집, 처리, 배포 과정을 포함하는 일련의 작업을 **DAG(Directed Acyclic Graph)**로 정의하며, 이 DAG를 통해 다양한 태스크를 순차적으로 실행할 수 있습니다.</p>\n<p>Airflow의 핵심 철학은 모든 워크플로우를 <strong>코드로 정의</strong>하는 것입니다. 이를 통해 복잡한 데이터 파이프라인을 더욱 쉽게 관리하고, 재사용 가능하며, 버전 관리를 할 수 있습니다.</p>\n<h2>📌 주요 기능</h2>\n<h3>1. 워크플로우 스케줄링</h3>\n<p>Airflow는 특정 시간에 작업이 자동으로 실행되도록 스케줄링할 수 있습니다.</p>\n<p>하루에 한 번, 매 시간마다, 혹은 복잡한 크론 표현식을 사용하여 워크플로우를 유연하게 관리할 수 있습니다.</p>\n<h3>2. 강력한 DAG 기반 구조</h3>\n<p>모든 워크플로우는 DAG로 정의됩니다. DAG는 작업 간의 종속성을 명확하게 나타내며, 각 작업의 순서를 시각적으로 이해할 수 있게 도와줍니다.</p>\n<h3>3. 유연한 연산자(Operators)</h3>\n<p>Airflow는 다양한 연산자들을 제공합니다. <strong>PythonOperator</strong>, <strong>BashOperator</strong>, <strong>MySqlOperator</strong> 등 여러 연산자를 사용하여 Python 코드 실행, Bash 명령어 실행, 데이터베이스 작업 등을 쉽게 수행할 수 있습니다.</p>\n<h3>4. 확장성</h3>\n<p>Airflow는 <strong>분산 아키텍처</strong>를 지원하여, 대규모 데이터 파이프라인을 효율적으로 처리할 수 있습니다.</p>\n<p><strong>CeleryExecutor</strong>, <strong>KubernetesExecutor</strong> 등을 사용해 작업을 클러스터로 분산시킬 수 있습니다.</p>\n<h3>5. 모니터링과 알림</h3>\n<p>Airflow는 실행된 태스크의 상태를 실시간으로 모니터링할 수 있는 웹 인터페이스를 제공합니다.</p>\n<p>또한, 작업 실패 시 이메일 알림이나 Slack 메시지를 통해 즉시 통보받을 수 있습니다.</p>\n<h2>❓ 왜 Airflow를 선택해야 하나요?</h2>\n<ol>\n<li><strong>코드로 워크플로우 정의</strong>: Airflow는 워크플로우를 코드로 정의하기 때문에, 파이프라인의 복잡성을 쉽게 관리할 수 있습니다.</li>\n<li><strong>커뮤니티와 플러그인</strong>: Airflow는 활발한 오픈 소스 커뮤니티를 가지고 있으며, 다양한 플러그인과 확장 기능을 통해 사용자 요구에 맞게 커스터마이징할 수 있습니다.</li>\n<li><strong>확장성과 유연성</strong>: 다양한 스케줄링 옵션과 분산 실행 환경을 지원하여, 작은 프로젝트부터 대규모 데이터 파이프라인까지 확장 가능합니다.</li>\n<li><strong>시각적 인터페이스</strong>: Airflow의 웹 UI는 작업 흐름을 시각적으로 모니터링하고 관리하는 데 유용합니다. 이는 실시간으로 워크플로우를 파악하고, 문제를 빠르게 해결할 수 있도록 도와줍니다.</li>\n</ol>\n<h2>🔫 결론</h2>\n<p>Apache Airflow는 데이터 엔지니어링의 필수 도구로 요구되고 많이 사용하고 있습니다.</p>\n<p>기업의 요구사항에서도 필수적인 조건이라고 생각합니다.</p>\n<p>워크플로우의 복잡성을 단순화하고, 자동화된 스케줄링과 모니터링을 통해 데이터 파이프라인을 더 효율적으로 관리할 수 있습니다.</p>\n<p>데이터 중심의 비즈니스 환경에서 Airflow는 워크플로우를 더욱 강력하고 유연하게 만들어 줍니다.</p>\n<p>장점이 많은 도구로 앞으로도 많이 공부하고 사용할 예정입니다. 🌟</p>\n<p>부족한 글을 봐주셔서 감사합니다.</p>\n<p>자세한 내용은 깃허브에서 확인하실 수 있습니다. <a href=\"https://github.com/jms0522/Streaming-Data/issues/6\">GitHub Issue - Airflow</a></p>","categories":["Airflow","ALL"],"date":"August 20, 2024","description":"Airflow 간단한 소개글입니다. - 기초 내용","id":"dbe2fc00-10e4-5b29-8d44-ce7fc1e53e12","keywords":["Workflow","Pipeline","Data","Blog","Airflow","Scheduler"],"slug":"/my-first-article/About-Airflow-Intro/","title":" 💬 [Airflow] About Airflow","readingTime":{"text":"6 min read"}},{"banner":{"alt":"ELK","caption":"Photo by <u><a href=\"https://sematext.com/guides/elk-stack/\">ELK-stack-guide</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACo0lEQVR42oVQ+0tTYRg+/0aUBSGFkbOZBK7STYfNGRprXqIL0kUzkVqB/eA0MZmrGSX90JDoBoaQGMhCSsuocJlLS9tkp93cOWdna56ddGfn9p1z1jf9A/p4v/d7eN6X93ueF9FfuHv4bH9Vi0PbPFB58U5Zs6282VZx3l56uk910qoyWYvre9TmniKTtaSht9jcs7+uC0aRqXunrgPJN3Zur7i2o8KyTXsVgjz99fzqzsITXWXnbGduOruGxh6Nvn/pco+9nX897Rmfmh9xzQ6/mrE/ftN++yky/dU3PuUZcbkhOzo5NzGz+GUBjcYpEUjZ/x2ESqUVRYFIApIogqyS5XkxRlLBMIkGiFAkvr7BwowGCYKkYFssnlrxY+HVuCTJiN8XwYg1EYBYnIYdeIwSBIARyeUV4sevSChMwqYovra4FIknaPhLMEQuo5gPxRiGQyZDSV4QeXhFUZJzUgUIgZIV/mQlVhAlAUiKLMliRpIVjhPkbFaO00qGFyQJsX36zPA8EaPd7rnvHg+GRQlyjUpxfu+HGe9shgVRLEkmyY8+N8eJ/t84nO6aeLdKJnKeSQJPp5mNNMuyHMdDBUKaYWl6HQ2setEATf+lKDqKEyGcJIhYik4zaWYhgAZxTBQAIssKkKAv+CowwXlwf4qsQA6AHA0L8uZGt0qQFDkBiCIsIJIMIMtk+AzLszy3Egx/W/LiicSW/wwrZDI5PWGCmPvpRSNRTuA39wKgWKSpZbDV4tTWdFeZ+g2Ntt1HO/I0bXvK2xsuOS5bnJW1t/R1fcamgb1aS57myi5Nq+FUb9sNZ7W5X1tjRVTH7hfoBg4YBtXGoZK64SONL2CUmp+oDEP7dPaiKofa+OBgrVNT/xzymvpn6pqHBTp7od5x6Pi9fzI3GxQ8N7ChAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/d0df3a6d22dc0c84dcef322f849e7b0f/bc51f/ELK-Stack-architecture.png","srcSet":"/static/d0df3a6d22dc0c84dcef322f849e7b0f/41200/ELK-Stack-architecture.png 165w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/f979a/ELK-Stack-architecture.png 330w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/bc51f/ELK-Stack-architecture.png 660w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/f57dd/ELK-Stack-architecture.png 1320w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/d0df3a6d22dc0c84dcef322f849e7b0f/322ad/ELK-Stack-architecture.webp 165w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/de3b3/ELK-Stack-architecture.webp 330w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/2b2b5/ELK-Stack-architecture.webp 660w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/e36a7/ELK-Stack-architecture.webp 1320w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>🚀 ELK V1</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/multipipeline.png\" alt=\"V1\"></p>\n<p><strong>ELK Stack</strong>은 Elasticsearch, Logstash, 그리고 Kibana의 약자로, 로그 및 이벤트 데이터를 수집, 저장, 검색, 분석, 시각화하는 데 사용되는 오픈 소스 소프트웨어 스택입니다. ELK Stack은 DevOps 및 데이터 분석에서 널리 사용되며, 시스템 성능을 모니터링하고, 애플리케이션 로그를 분석하며, 보안 위협을 감지하는 데 매우 유용합니다.</p>\n<h2>💬 포스트 개요</h2>\n<ul>\n<li><strong>제목</strong>: Kafka와 ELK Stack을 이용한 실시간 로그 처리 및 시각화</li>\n<li><strong>소개</strong>: 이번 포스트에서는 Kafka와 ELK Stack(Filebeat, Logstash, Elasticsearch, Kibana)을 활용해 실시간 로그 데이터를 수집, 처리, 그리고 시각화하는 방법을 다룹니다. 이 과정을 통해 실시간 데이터 파이프라인의 구성과 각 구성 요소의 역할을 이해할 수 있습니다.</li>\n</ul>\n<h2>🌟 프로젝트 배경</h2>\n<p>서비스 운영 중 발생하는 로그 데이터를 실시간으로 수집하고, 이를 통해 빠른 문제 해결과 데이터 기반 의사 결정을 지원하기 위해 이 프로젝트를 시작했습니다.</p>\n<h2>⚙️ 아키텍처 설명</h2>\n<p>이 프로젝트의 아키텍처는 <strong>Filebeat, Kafka, Logstash, Elasticsearch, Kibana</strong>로 구성됩니다. Filebeat는 로그 데이터를 수집하고, Kafka는 이를 중개하며, Logstash는 데이터를 처리하여 Elasticsearch에 저장하고, Kibana는 시각화를 담당합니다.</p>\n<h2>❗️ 개선 사항</h2>\n<p>향후에는 Logstash에서 더 복잡한 필터링 작업을 추가하고, Kafka 클러스터를 확장하여 가용성을 높이는 것을 고려하고 있습니다.</p>\n<h2>🧑🏻‍💻 마치며</h2>\n<p>Kafka와 ELK Stack을 사용하여 로그 데이터를 실시간으로 수집하고 시각화하는 환경을 구축할 수 있었습니다. 익숙한 작업이라 구성하는 자체는 어렵지 않았습니다. 처음 V1을 구성하였고, 더 디테일한 작업들과 hadoop-spark 파이프라인을 구성하여 비교해 볼 예정입니다. 모든 이슈와 코드 사항은 <a href=\"https://github.com/jms0522/Streaming-Data\">Github</a>에서 확인 가능합니다.</p>","categories":["Project","Pipeline","ALL"],"date":"August 19, 2024","description":"Streaming-data Project version.1","id":"866f99e7-92d5-54a6-8429-b13ec3a4c095","keywords":["ELK","Pipeline","Data","Blog","Streaming","V1"],"slug":"/my-first-article/About-Streaming-Data-Project-V1/","title":"🚀 [Streaming data] ELK-V1","readingTime":{"text":"3 min read"}},{"banner":{"alt":"ELK","caption":"Photo by <u><a href=\"https://kafka.apache.org/documentation\">kafka-docs</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABbUlEQVR42qVSPYvCQBTMP/If+BNS2gqapLEy2mkhCGosVEiaEEEFNRA7g1qIAUGwEguL2EmKcJBOz29NvNEc4u0Vh9zAhrcvb96bnV3qdrt5nvf8vgUK63Q6zWaz4/GI+Hq9ei/wi15jklytVsPhsCRJPvm9yaqqMgxTr9d7vZ4gCIfDAVp2u93nA1DkOI5t274o13X94Ju8Wq2CwWC3241EIhzHDYdDaFksFvP5fLlc4m+xWCyXy/1+n5BGnc/nRCIxHo9DoRCUNxoNzMT86XRqmmar1ZpMJjzPK4oiy7JlWYPBQNO0zWZDkkVRhHhkaZpGslAoNJvNeDyeTqdrtVogEAAfNdlsFh1/yNZ1PRqNsixrGEalUslkMvl8PpfLpVIptBiNRrFYrFQqQVQymex0Ovv9/k5ut9swDL2fhsGP7Xa7Xq/RF0VQd7lc4CL8+3gAWzh3J+M8xFUR9+wD1UTmj0fyu8VrhvrP8/wCYP1it3m1MlAAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/77e7816f1744d462ccda1d49c464b4d3/e9748/newkafka.png","srcSet":"/static/77e7816f1744d462ccda1d49c464b4d3/0667a/newkafka.png 56w,\n/static/77e7816f1744d462ccda1d49c464b4d3/5efbf/newkafka.png 113w,\n/static/77e7816f1744d462ccda1d49c464b4d3/e9748/newkafka.png 225w","sizes":"(min-width: 225px) 225px, 100vw"},"sources":[{"srcSet":"/static/77e7816f1744d462ccda1d49c464b4d3/ab25b/newkafka.webp 56w,\n/static/77e7816f1744d462ccda1d49c464b4d3/28652/newkafka.webp 113w,\n/static/77e7816f1744d462ccda1d49c464b4d3/b921d/newkafka.webp 225w","type":"image/webp","sizes":"(min-width: 225px) 225px, 100vw"}]},"width":660,"height":398.93333333333334}}}},"body":"<h1>📊 Topic</h1>\n<p><img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/newkafka.png\" alt=\"Visualization\"></p>\n<p>최근에 재미로 진행한 프로젝트에서 **Instagram Scraper API (Rapid API)**를 사용해 Instagram 데이터를 수집한 후, Kafka를 통해 전달하고, Logstash를 통해 Elasticsearch로 데이터를 전송했습니다.</p>\n<p>이 과정에서 인덱스를 instagram-profiles로 생성한 후 Kibana에서 시각화를 시도했는데, 예상치 못한 문제들이 발생했습니다.</p>\n<p>이번 포스트에서는 이 문제를 해결하기 위해 고민한 내용을 공유하고자 합니다. 😀</p>\n<h2>🌟 프로젝트 개요</h2>\n<p>이 프로젝트의 목표는 Instagram Scraper API를 통해 수집한 데이터를 실시간으로 처리하고, Elasticsearch에 저장하여 Kibana에서 시각화하는 것이었습니다.</p>\n<p>전체적인 데이터 흐름은 다음과 같습니다:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1.\tInstagram Scraper API: 데이터를 수집하고, 이를 JSON 형식으로 Kafka로 전송.\n2.\tKafka: 데이터를 중간에서 처리하여 Logstash로 전달.\n3.\tLogstash: Kafka에서 받은 데이터를 Elasticsearch로 전달하며, 이 과정에서 데이터 변환을 수행.\n4.\tElasticsearch: 데이터를 저장하고 인덱싱하며, Kibana에서 검색 및 시각화에 사용할 수 있도록 준비.\n5.\tKibana: Elasticsearch에서 데이터를 가져와 시각화.</code></pre></div>\n<h2>❗️ 문제 발생</h2>\n<p>데이터 파이프라인을 설정하고, Kibana에서 데이터를 시각화하려는 과정에서 문제가 발생했습니다.</p>\n<p>데이터는 instagram-profiles 인덱스로 정상적으로 전달된 것처럼 보였지만, Kibana에서 시각화하는 데 어려움이 있었습니다.</p>\n<p>구체적으로, 여러 필드가 empty fields로 나타나거나, 데이터가 제대로 변환되지 않은 상태로 저장되고 있었습니다.</p>\n<h2>❓ 문제 원인 추정</h2>\n<ol>\n<li><strong>인덱스 매핑</strong>: Elasticsearch에서 <code class=\"language-text\">instagram-profiles</code> 인덱스의 매핑이 문제일 수 있습니다. 필드 타입이 적절하게 설정되지 않아, 데이터가 올바르게 저장되지 않고 빈 값으로 처리되고 있을 가능성이 있습니다.\n<ul>\n<li>예를 들어, <code class=\"language-text\">id</code> 필드는 <code class=\"language-text\">keyword</code>로 매핑되어야 하는데, 문자열로 매핑되어 데이터 검색에 문제가 발생할 수 있습니다.</li>\n</ul>\n</li>\n<li><strong>Logstash의 데이터 변환</strong>: Logstash 설정에서 JSON 데이터를 제대로 파싱하지 못하거나, 각 필드를 적절한 데이터 타입으로 변환하지 못하는 문제가 있을 수 있습니다. 특히, Logstash가 데이터를 파싱할 때 필드 이름이나 데이터 타입이 잘못 지정되면 Elasticsearch로 전달된 데이터가 예상한 대로 저장되지 않을 수 있습니다.\n<ul>\n<li>JSON 파싱 후 필드가 올바르게 변환되지 않아, Kibana에서 시각화가 제대로 이루어지지 않을 수 있습니다.</li>\n</ul>\n</li>\n</ol>\n<h2>📚 해결을 위한 시도</h2>\n<ol>\n<li>인덱스 매핑 확인 및 수정</li>\n</ol>\n<ul>\n<li>Elasticsearch에서 인덱스 매핑을 다시 한 번 확인하고, 각 필드가 적절한 데이터 타입으로 설정되어 있는지 검토했습니다.</li>\n</ul>\n<ol start=\"2\">\n<li>Logstash 필터 설정 검토</li>\n</ol>\n<ul>\n<li>Logstash의 필터 설정을 다시 검토하여, JSON 파싱과 필드 변환이 올바르게 이루어지고 있는지 확인했습니다.</li>\n<li>특히, mutate 필터를 사용해 각 필드를 적절한 데이터 타입으로 변환하고, 필요 없는 필드를 제거하는 작업을 강화했습니다.</li>\n</ul>\n<ol start=\"3\">\n<li>Kibana 인덱스 패턴 재색인</li>\n</ol>\n<ul>\n<li>위의 변경 사항을 적용한 후, Kibana에서 인덱스 패턴을 다시 색인하여 새로운 필드 설정이 반영되었는지 확인했습니다.</li>\n</ul>\n<h2>✅ 결론</h2>\n<p>이 프로젝트에서 직면한 문제는 주로 인덱스 매핑과 Logstash의 데이터 변환 설정에서 비롯된 것으로 보입니다.</p>\n<p>Elasticsearch와 Kibana에서 데이터를 올바르게 시각화하려면, 각 구성 요소가 데이터를 정확히 처리하고 전달하는 것이 중요합니다.</p>\n<p>앞으로도 이러한 문제를 해결하기 위해 지속적으로 모니터링하고, 필요한 경우 설정을 수정해 나갈 계획입니다.</p>\n<p>이와 같은 과정을 통해 문제를 해결하는 경험은 실무에서 매우 중요하다는 것을 깨닫게 되었습니다.</p>\n<p>여러분도 비슷한 상황을 겪고 있다면, 인덱스 매핑과 데이터 파이프라인의 각 단계를 꼼꼼히 점검해 보시길 권장합니다.</p>\n<p>자세한 내용은 깃허브에서 확인하실 수 있습니다. <a href=\"https://github.com/jms0522/Streaming-Data/issues/7\">GitHub Issue #7</a></p>\n<p>감사합니다 🙌🏻</p>","categories":["Kafka","ALL"],"date":"August 19, 2024","description":"Api로 받은 데이터를 Kafka - Kibana 시각화","id":"a7814413-2fff-5a52-9957-b036a7f81a47","keywords":["API","Pipeline","Data","Blog","Kafka","Visualization"],"slug":"/my-first-article/About-Kibana-Visualization/","title":"🚀 [Kafka-ELK] Topic Visualization","readingTime":{"text":"7 min read"}},{"banner":{"alt":"ELK","caption":"Photo by <u><a href=\"https://unsplash.com/photos/Nc5Q_CEcY44\">ELK</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABqklEQVR42nVSzy8DQRRu3By5uThJBBf8Bf0PRLi5urVxFJESJ3HgoGeNaDRSoSiNRhTdhk3Iduk2IdXV7Q8/upuqIrVrdmbM7narlfUyM3lv3vvm++bN2LBpCKpkYqQSV49JiMwcqo1ms5lZLQFruL+GzH2kQgswMVDwIW4ICfPEl7Oej4Tz+/sTQA0FaP7TtVOa8stMpibEACOoHfZwz20uDm8HHVdJt5YVpvGZDZcPjaq8n0p0T8iuADaYTXk2oJJL4lXven/vYM/CkZPTdg825iZHWpb33Cs5AWDsX/MO2O2eJNNIqzPrQemrOhYJjlKhmetzcvjJKd3W2tHuGO+L7r4AxRfY6lya7TryB3K81pq67LqK1Pvbdp6/rZSNxB13s89eHpeeFQgzb68Hj5nQU5Z5lRpUN3TbkCBJEp9Oi6IoyzKhIJcUi0VJFLHVQ/x2W9U7ly8UOI6LxWKCIFzQNMOyqVTqOBIpVyr1GguwwVytVuPxOEVRLMuGw2Gapnmej0ajRFG9xgJc41dVRVGMFUIIACD6CUbVH+Vf2U1fymQwHIQsPx7+ARXJj6JfkGjDAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/6f079f582ee9c2883f1783a2f8f3dcbe/ab14c/elk-stack.png","srcSet":"/static/6f079f582ee9c2883f1783a2f8f3dcbe/c7491/elk-stack.png 74w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/e8188/elk-stack.png 149w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/ab14c/elk-stack.png 297w","sizes":"(min-width: 297px) 297px, 100vw"},"sources":[{"srcSet":"/static/6f079f582ee9c2883f1783a2f8f3dcbe/21fa2/elk-stack.webp 74w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/c1258/elk-stack.webp 149w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/280a7/elk-stack.webp 297w","type":"image/webp","sizes":"(min-width: 297px) 297px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>⚒️ About ELK</h1>\n<p>!<img src=\"https://raw.githubusercontent.com/jms0522/jms0522.github.io/main/content/images/elk-stack.png\" alt=\"ELK-stack\"></p>\n<h2>🚀 ELK</h2>\n<p>ELK Stack은 Elasticsearch, Logstash, 그리고 Kibana의 약자로, 로그 및 이벤트 데이터를 수집, 저장, 검색, 분석, 시각화하는 데 사용되는 오픈 소스 소프트웨어 스택입니다.\nELK Stack은 DevOps 및 데이터 분석에서 널리 사용되며, 시스템 성능을 모니터링하고, 애플리케이션 로그를 분석하며, 보안 위협을 감지하는 데 매우 유용합니다.</p>\n<h2>💬 들어가며</h2>\n<p><strong>Medical-AI 프로젝트에서 처음 실시간 데이터 파이프라인을 구축할 때 알게 되어 사용하게 되었고 다양한 장점이 있어 자주 사용하는 스택 중 하나입니다.</strong>\n지금 진행하고 있는 프로젝트에서도 사용하고 있기에, 오늘은 간단한 소개와 함께 공부해보는 시간을 갖기 위해 글을 작성합니다.\n제 개인적인 생각도 포함하고 있어 모든 게 정답은 아니니 참고하시는 용도로 봐주시면 감사하겠습니다. 🫡</p>\n<h3>Elasticsearch</h3>\n<ul>\n<li>\n<p><strong>개요</strong>: Elasticsearch는 분산 RESTful 검색 및 분석 엔진으로, 로그 데이터를 빠르게 저장하고 검색하는 데 최적화되어 있습니다.\nJSON 형식의 문서를 기반으로 데이터를 인덱싱하며, 다양한 유형의 데이터를 처리할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>특징</strong>:</p>\n<ul>\n<li><strong>분산형</strong>: 데이터가 여러 노드에 걸쳐 분산되어 저장되고 검색됩니다.</li>\n<li><strong>스케일링</strong>: 대량의 데이터를 실시간으로 처리하며, 클러스터를 확장하여 성능을 향상시킬 수 있습니다.</li>\n<li><strong>고급 검색 기능</strong>: 정규 표현식, 와일드카드, 유사 검색 등 다양한 검색 기능을 지원합니다.</li>\n</ul>\n</li>\n</ul>\n<h3>Logstash</h3>\n<ul>\n<li>\n<p><strong>개요</strong>: Logstash는 서버 측 데이터 처리 파이프라인으로, 다양한 소스에서 데이터를 수집하고, 필터링 및 변환하여 Elasticsearch로 전송합니다.</p>\n</li>\n<li>\n<p><strong>특징</strong>:</p>\n<ul>\n<li><strong>다양한 입력 플러그인</strong>: 파일, 데이터베이스, 메시징 시스템 등 다양한 데이터 소스에서 로그 데이터를 수집할 수 있습니다.</li>\n<li><strong>필터링</strong>: 데이터 변환, 정규화, 정제 작업을 수행할 수 있는 다양한 필터 플러그인을 제공합니다.</li>\n<li><strong>출력</strong>: 데이터를 Elasticsearch 외에도 다양한 출력 대상(파일, 메시지 큐 등)으로 보낼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<h3>Kibana</h3>\n<ul>\n<li>\n<p><strong>개요</strong>: Kibana는 Elasticsearch의 데이터 시각화 도구로, 대시보드와 차트를 사용하여 데이터를 시각적으로 분석할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>특징</strong>:</p>\n<ul>\n<li><strong>대시보드</strong>: 로그 및 메트릭 데이터를 시각화하여 대시보드에 표시합니다.</li>\n<li><strong>검색 및 탐색</strong>: Kibana의 UI를 통해 Elasticsearch에서 데이터를 검색하고 분석할 수 있습니다.</li>\n<li><strong>경고 기능</strong>: 특정 조건이 충족되면 경고를 발생시키는 기능을 제공합니다.</li>\n</ul>\n</li>\n</ul>\n<h3>Beats</h3>\n<ul>\n<li><strong>개요</strong>: 경량 데이터 수집 에이전트입니다.\nBeats는 시스템의 로그, 메트릭스, 네트워크 데이터를 수집하여 Logstash나 Elasticsearch로 전송하는 역할을 합니다.\nFilebeat, Metricbeat 등 다양한 종류가 있습니다.</li>\n</ul>\n<h2>✅ 활용사례</h2>\n<ul>\n<li>\n<p><strong>로그 관리</strong>: 애플리케이션, 서버, 네트워크 장비의 로그를 수집하고 분석하여 문제를 신속하게 진단할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>보안 모니터링</strong>: 보안 로그를 분석하여 침입 탐지, 비정상적인 활동 감지, 보안 정책 준수를 확인할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>애플리케이션 성능 모니터링(APM)</strong>: 애플리케이션의 성능 데이터를 실시간으로 수집하여 성능 병목 지점을 식별하고 최적화할 수 있습니다.</p>\n<p>💬 저는 프로젝트 시, 로그 관리에 목적으로 많이 사용을 하거나, 수집된 데이터를 시각화하는 대시보드로도 많이 사용하고 있습니다.\nLogstash에서 데이터를 변환, 정체 작업을 수행할 수 있어 간단한 전처리가 가능한 점도 사용하기에 편리하다고 생각합니다.\n다양한 플러그인과 오픈 소스의 풍부한 생태계도 높은 확장성과 유연성을 제공합니다.</p>\n</li>\n</ul>\n<h2>❗️고려사항</h2>\n<ul>\n<li><strong>복잡성</strong>: 초기 설정 및 구성은 복잡할 수 있으며, 대규모 클러스터를 운영하려면 적절한 모니터링과 유지 관리가 필요합니다.</li>\n<li><strong>자원 소모</strong>: 특히 Elasticsearch는 데이터 양과 쿼리 복잡도에 따라 높은 자원을 요구할 수 있습니다.</li>\n</ul>\n<h2>🧑🏻‍💻 마치며</h2>\n<p>오늘은 간단하게 ELK를 소개해봤는데요,\n제 <a href=\"https://github.com/jms0522/Streaming-Data\">GitHub</a>에서 자세한 코드와 이슈를 확인하실 수 있습니다.\n감사합니다.</p>","categories":["Pipeline","ALL"],"date":"August 18, 2024","description":"다양한 상황에서 사용하는 ELK-stack에 대한 설명과 경험","id":"98217d97-19e9-505a-8fb7-8ac9619edabc","keywords":["ELK","Pipeline","Data","Blog","Streaming"],"slug":"/my-first-article/About-ELK/","title":"⚒️ [ELK-stack] About ELK-Stack ","readingTime":{"text":"7 min read"}}]}},"staticQueryHashes":["3262260831","948380417"],"slicesMap":{}}