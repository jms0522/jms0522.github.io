{"componentChunkName":"component---node-modules-gatsby-theme-portfolio-minimal-src-templates-article-listing-index-tsx","path":"/blog/","result":{"pageContext":{"articles":[{"banner":{"alt":"Airflow","caption":"Photo by <u><a href=\"https://airflow.apache.org\">Airflow</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACO0lEQVR42oWS3WvTUBTAb+dk4qNPIvgB/gH1A31xyEAnExR88GH4MJ8EBYdPIoh78MUnBd2LWVs2sV1bV9subl0qQjs6ZpdEU21T027DNV0b1vXbmJI1TeNJU3Gg4IHce88593dzvpBaLGggqqrqm1ZTWk9zRVltw7n9l/y+1RUkXDLLVMTgwX2Vy5z9utYltf8Iypw6tD1k1jhG0bSRNI8WmespHhybuVwyyRa2trLZrCAIqRS3mk7zPN9oNIyndbh0+uDm5ZPag+GxcBiRKbT05d73PDhI+pPf5/XD99aDz86Gw6GlSMTpnC6Xy39gZvCMds2ceXz7WDCM6DTAju0qOHYURZblSqUCqyj+aDabakfAUq/X1U6OaPihEx+7c3SRRJROHqa5UlPR4aYy/uJ5uVyq1WpsImEkub6+Zre/9vm8Ql6PDvXho3tj0T3fiiYysf/z6vuaaNxLJpNWi+Xj8jLESdNUIDBPEMTU5CSGvbRMYBzHgQX1fDjfR4weyFaOCDsLkgyY0umHzWYNBgmb1cqy7Nzcu2mHQ5KkGMOsrEQpinK5nHBAJv+V3pkBFLh1M7WgR6u2YC0UChMYFo/H3W4X1CwUCnk8HkiSYRggo9EoWHAcR8g/YPIM9novojfnHrGvjJgl6We1qpcNGgMPiaIIKsCgwv9BVRQly/MI25jf5xtC7v4edz+yn7gbGzd4tW1UV/3neHRbpddQzD9Lz9wgn1yI3D9OjExtBMHY6sS/ezZ3q8ZA/wIAVki+U9pyEAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png","srcSet":"/static/de7402d2a87d0256451392f6590d157b/182cc/airflow-profile.png 91w,\n/static/de7402d2a87d0256451392f6590d157b/2c6ad/airflow-profile.png 181w,\n/static/de7402d2a87d0256451392f6590d157b/b7b7c/airflow-profile.png 362w","sizes":"(min-width: 362px) 362px, 100vw"},"sources":[{"srcSet":"/static/de7402d2a87d0256451392f6590d157b/98c2e/airflow-profile.webp 91w,\n/static/de7402d2a87d0256451392f6590d157b/483ec/airflow-profile.webp 181w,\n/static/de7402d2a87d0256451392f6590d157b/def5b/airflow-profile.webp 362w","type":"image/webp","sizes":"(min-width: 362px) 362px, 100vw"}]},"width":660,"height":399.2817679558011}}}},"body":"<h1>🌬️ Apache Airflow: 데이터 파이프라인을 코드로 관리하는 방법</h1>\n<p><img src=\"/Users/jangminsoo/Desktop/github-blog/portfolio-minimal/content/images/airflow-workflow.png\" alt=\"Airflow\"></p>\n<h2>💬 소개</h2>\n<p>현대 데이터 엔지니어링과 머신러닝 워크플로우에서 데이터 파이프라인은 핵심적인 역할을 합니다. Apache Airflow는 이러한 파이프라인을 쉽고 효율적으로 관리할 수 있도록 도와주는 강력한 오픈 소스 플랫폼입니다. Airflow는 워크플로우를 스케줄링하고 모니터링하며, 데이터 파이프라인을 코드로 정의할 수 있게 해줍니다.</p>\n<h2>🌟 Airflow란 무엇인가요?</h2>\n<p>Apache Airflow는 <strong>워크플로우 자동화 및 스케줄링 도구</strong>로, 복잡한 데이터 파이프라인을 관리하고 모니터링하는 데 사용됩니다. 데이터 수집, 처리, 배포 과정을 포함하는 일련의 작업을 **DAG(Directed Acyclic Graph)**로 정의하며, 이 DAG를 통해 다양한 태스크를 순차적으로 실행할 수 있습니다.</p>\n<p>Airflow의 핵심 철학은 모든 워크플로우를 <strong>코드로 정의</strong>하는 것입니다. 이를 통해 복잡한 데이터 파이프라인을 더욱 쉽게 관리하고, 재사용 가능하며, 버전 관리를 할 수 있습니다.</p>\n<h2>📌 주요 기능</h2>\n<h3>1. 워크플로우 스케줄링</h3>\n<p>Airflow는 특정 시간에 작업이 자동으로 실행되도록 스케줄링할 수 있습니다.</p>\n<p>하루에 한 번, 매 시간마다, 혹은 복잡한 크론 표현식을 사용하여 워크플로우를 유연하게 관리할 수 있습니다.</p>\n<h3>2. 강력한 DAG 기반 구조</h3>\n<p>모든 워크플로우는 DAG로 정의됩니다. DAG는 작업 간의 종속성을 명확하게 나타내며, 각 작업의 순서를 시각적으로 이해할 수 있게 도와줍니다.</p>\n<h3>3. 유연한 연산자(Operators)</h3>\n<p>Airflow는 다양한 연산자들을 제공합니다. <strong>PythonOperator</strong>, <strong>BashOperator</strong>, <strong>MySqlOperator</strong> 등 여러 연산자를 사용하여 Python 코드 실행, Bash 명령어 실행, 데이터베이스 작업 등을 쉽게 수행할 수 있습니다.</p>\n<h3>4. 확장성</h3>\n<p>Airflow는 <strong>분산 아키텍처</strong>를 지원하여, 대규모 데이터 파이프라인을 효율적으로 처리할 수 있습니다.</p>\n<p><strong>CeleryExecutor</strong>, <strong>KubernetesExecutor</strong> 등을 사용해 작업을 클러스터로 분산시킬 수 있습니다.</p>\n<h3>5. 모니터링과 알림</h3>\n<p>Airflow는 실행된 태스크의 상태를 실시간으로 모니터링할 수 있는 웹 인터페이스를 제공합니다.</p>\n<p>또한, 작업 실패 시 이메일 알림이나 Slack 메시지를 통해 즉시 통보받을 수 있습니다.</p>\n<h2>❓ 왜 Airflow를 선택해야 하나요?</h2>\n<ol>\n<li><strong>코드로 워크플로우 정의</strong>: Airflow는 워크플로우를 코드로 정의하기 때문에, 파이프라인의 복잡성을 쉽게 관리할 수 있습니다.</li>\n<li><strong>커뮤니티와 플러그인</strong>: Airflow는 활발한 오픈 소스 커뮤니티를 가지고 있으며, 다양한 플러그인과 확장 기능을 통해 사용자 요구에 맞게 커스터마이징할 수 있습니다.</li>\n<li><strong>확장성과 유연성</strong>: 다양한 스케줄링 옵션과 분산 실행 환경을 지원하여, 작은 프로젝트부터 대규모 데이터 파이프라인까지 확장 가능합니다.</li>\n<li><strong>시각적 인터페이스</strong>: Airflow의 웹 UI는 작업 흐름을 시각적으로 모니터링하고 관리하는 데 유용합니다. 이는 실시간으로 워크플로우를 파악하고, 문제를 빠르게 해결할 수 있도록 도와줍니다.</li>\n</ol>\n<h2>🔫 결론</h2>\n<p>Apache Airflow는 데이터 엔지니어링의 필수 도구로 요구되고 많이 사용하고 있습니다.</p>\n<p>기업의 요구사항에서도 필수적인 조건이라고 생각합니다.</p>\n<p>워크플로우의 복잡성을 단순화하고, 자동화된 스케줄링과 모니터링을 통해 데이터 파이프라인을 더 효율적으로 관리할 수 있습니다.</p>\n<p>데이터 중심의 비즈니스 환경에서 Airflow는 워크플로우를 더욱 강력하고 유연하게 만들어 줍니다.</p>\n<p>장점이 많은 도구로 앞으로도 많이 공부하고 사용할 예정입니다. 🌟</p>\n<p>부족한 글을 봐주셔서 감사합니다.</p>\n<p>자세한 내용은 깃허브에서 확인하실 수 있습니다. <a href=\"https://github.com/jms0522/Streaming-Data/issues/6\">GitHub Issue - Airflow</a></p>","categories":["Airflow","ALL"],"date":"August 20, 2024","description":"Airflow 간단한 소개글입니다. - 기초 내용","id":"9c46b6a3-7da1-5fb5-abef-6e46281093b1","keywords":["Workflow","Pipeline","Data","Blog","Airflow","Scheduler"],"slug":"/my-first-article/airflow-intro/","title":" 💬 [Airflow] About Airflow","readingTime":{"text":"6 min read"}},{"banner":{"alt":"ELK","caption":"Photo by <u><a href=\"https://sematext.com/guides/elk-stack/\">ELK-stack-guide</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACo0lEQVR42oVQ+0tTYRg+/0aUBSGFkbOZBK7STYfNGRprXqIL0kUzkVqB/eA0MZmrGSX90JDoBoaQGMhCSsuocJlLS9tkp93cOWdna56ddGfn9p1z1jf9A/p4v/d7eN6X93ueF9FfuHv4bH9Vi0PbPFB58U5Zs6282VZx3l56uk910qoyWYvre9TmniKTtaSht9jcs7+uC0aRqXunrgPJN3Zur7i2o8KyTXsVgjz99fzqzsITXWXnbGduOruGxh6Nvn/pco+9nX897Rmfmh9xzQ6/mrE/ftN++yky/dU3PuUZcbkhOzo5NzGz+GUBjcYpEUjZ/x2ESqUVRYFIApIogqyS5XkxRlLBMIkGiFAkvr7BwowGCYKkYFssnlrxY+HVuCTJiN8XwYg1EYBYnIYdeIwSBIARyeUV4sevSChMwqYovra4FIknaPhLMEQuo5gPxRiGQyZDSV4QeXhFUZJzUgUIgZIV/mQlVhAlAUiKLMliRpIVjhPkbFaO00qGFyQJsX36zPA8EaPd7rnvHg+GRQlyjUpxfu+HGe9shgVRLEkmyY8+N8eJ/t84nO6aeLdKJnKeSQJPp5mNNMuyHMdDBUKaYWl6HQ2setEATf+lKDqKEyGcJIhYik4zaWYhgAZxTBQAIssKkKAv+CowwXlwf4qsQA6AHA0L8uZGt0qQFDkBiCIsIJIMIMtk+AzLszy3Egx/W/LiicSW/wwrZDI5PWGCmPvpRSNRTuA39wKgWKSpZbDV4tTWdFeZ+g2Ntt1HO/I0bXvK2xsuOS5bnJW1t/R1fcamgb1aS57myi5Nq+FUb9sNZ7W5X1tjRVTH7hfoBg4YBtXGoZK64SONL2CUmp+oDEP7dPaiKofa+OBgrVNT/xzymvpn6pqHBTp7od5x6Pi9fzI3GxQ8N7ChAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/d0df3a6d22dc0c84dcef322f849e7b0f/bc51f/ELK-Stack-architecture.png","srcSet":"/static/d0df3a6d22dc0c84dcef322f849e7b0f/41200/ELK-Stack-architecture.png 165w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/f979a/ELK-Stack-architecture.png 330w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/bc51f/ELK-Stack-architecture.png 660w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/f57dd/ELK-Stack-architecture.png 1320w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/d0df3a6d22dc0c84dcef322f849e7b0f/322ad/ELK-Stack-architecture.webp 165w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/de3b3/ELK-Stack-architecture.webp 330w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/2b2b5/ELK-Stack-architecture.webp 660w,\n/static/d0df3a6d22dc0c84dcef322f849e7b0f/e36a7/ELK-Stack-architecture.webp 1320w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>🚀 ELK V1</h1>\n<p><img src=\"/Users/jangminsoo/Desktop/github-blog/portfolio-minimal/content/images/multipipeline.png\" alt=\"ELK V1\"></p>\n<p><strong>ELK Stack</strong>은 Elasticsearch, Logstash, 그리고 Kibana의 약자로, 로그 및 이벤트 데이터를 수집, 저장, 검색, 분석, 시각화하는 데 사용되는 오픈 소스 소프트웨어 스택입니다. ELK Stack은 DevOps 및 데이터 분석에서 널리 사용되며, 시스템 성능을 모니터링하고, 애플리케이션 로그를 분석하며, 보안 위협을 감지하는 데 매우 유용합니다.</p>\n<h2>💬 포스트 개요</h2>\n<ul>\n<li><strong>제목</strong>: Kafka와 ELK Stack을 이용한 실시간 로그 처리 및 시각화</li>\n<li><strong>소개</strong>: 이번 포스트에서는 Kafka와 ELK Stack(Filebeat, Logstash, Elasticsearch, Kibana)을 활용해 실시간 로그 데이터를 수집, 처리, 그리고 시각화하는 방법을 다룹니다. 이 과정을 통해 실시간 데이터 파이프라인의 구성과 각 구성 요소의 역할을 이해할 수 있습니다.</li>\n</ul>\n<h2>🌟 프로젝트 배경</h2>\n<p>서비스 운영 중 발생하는 로그 데이터를 실시간으로 수집하고, 이를 통해 빠른 문제 해결과 데이터 기반 의사 결정을 지원하기 위해 이 프로젝트를 시작했습니다.</p>\n<h2>⚙️ 아키텍처 설명</h2>\n<p>이 프로젝트의 아키텍처는 <strong>Filebeat, Kafka, Logstash, Elasticsearch, Kibana</strong>로 구성됩니다. Filebeat는 로그 데이터를 수집하고, Kafka는 이를 중개하며, Logstash는 데이터를 처리하여 Elasticsearch에 저장하고, Kibana는 시각화를 담당합니다.</p>\n<h2>❗️ 개선 사항</h2>\n<p>향후에는 Logstash에서 더 복잡한 필터링 작업을 추가하고, Kafka 클러스터를 확장하여 가용성을 높이는 것을 고려하고 있습니다.</p>\n<h2>🧑🏻‍💻 마치며</h2>\n<p>Kafka와 ELK Stack을 사용하여 로그 데이터를 실시간으로 수집하고 시각화하는 환경을 구축할 수 있었습니다. 익숙한 작업이라 구성하는 자체는 어렵지 않았습니다. 처음 V1을 구성하였고, 더 디테일한 작업들과 hadoop-spark 파이프라인을 구성하여 비교해 볼 예정입니다. 모든 이슈와 코드 사항은 <a href=\"https://github.com/jms0522/Streaming-Data\">Github</a>에서 확인 가능합니다.</p>","categories":["Project","Pipeline","ALL"],"date":"August 19, 2024","description":"Streaming-data Project version.1","id":"2a48be21-9db7-573c-904f-58e4ad6ede44","keywords":["ELK","Pipeline","Data","Blog","Streaming","V1"],"slug":"/my-first-article/streaming-data-project-V1/","title":"🚀 [Streaming data] ELK-V1","readingTime":{"text":"3 min read"}},{"banner":{"alt":"ELK","caption":"Photo by <u><a href=\"https://kafka.apache.org/documentation\">kafka-docs</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABbUlEQVR42qVSPYvCQBTMP/If+BNS2gqapLEy2mkhCGosVEiaEEEFNRA7g1qIAUGwEguL2EmKcJBOz29NvNEc4u0Vh9zAhrcvb96bnV3qdrt5nvf8vgUK63Q6zWaz4/GI+Hq9ei/wi15jklytVsPhsCRJPvm9yaqqMgxTr9d7vZ4gCIfDAVp2u93nA1DkOI5t274o13X94Ju8Wq2CwWC3241EIhzHDYdDaFksFvP5fLlc4m+xWCyXy/1+n5BGnc/nRCIxHo9DoRCUNxoNzMT86XRqmmar1ZpMJjzPK4oiy7JlWYPBQNO0zWZDkkVRhHhkaZpGslAoNJvNeDyeTqdrtVogEAAfNdlsFh1/yNZ1PRqNsixrGEalUslkMvl8PpfLpVIptBiNRrFYrFQqQVQymex0Ovv9/k5ut9swDL2fhsGP7Xa7Xq/RF0VQd7lc4CL8+3gAWzh3J+M8xFUR9+wD1UTmj0fyu8VrhvrP8/wCYP1it3m1MlAAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/77e7816f1744d462ccda1d49c464b4d3/e9748/newkafka.png","srcSet":"/static/77e7816f1744d462ccda1d49c464b4d3/0667a/newkafka.png 56w,\n/static/77e7816f1744d462ccda1d49c464b4d3/5efbf/newkafka.png 113w,\n/static/77e7816f1744d462ccda1d49c464b4d3/e9748/newkafka.png 225w","sizes":"(min-width: 225px) 225px, 100vw"},"sources":[{"srcSet":"/static/77e7816f1744d462ccda1d49c464b4d3/ab25b/newkafka.webp 56w,\n/static/77e7816f1744d462ccda1d49c464b4d3/28652/newkafka.webp 113w,\n/static/77e7816f1744d462ccda1d49c464b4d3/b921d/newkafka.webp 225w","type":"image/webp","sizes":"(min-width: 225px) 225px, 100vw"}]},"width":660,"height":398.93333333333334}}}},"body":"<h1>📊 Topic Visualization</h1>\n<p><img src=\"/Users/jangminsoo/Desktop/github-blog/portfolio-minimal/content/images/newkafka.png\" alt=\"Visualization\"></p>\n<p>최근에 재미로 진행한 프로젝트에서 **Instagram Scraper API (Rapid API)**를 사용해 Instagram 데이터를 수집한 후, Kafka를 통해 전달하고, Logstash를 통해 Elasticsearch로 데이터를 전송했습니다.</p>\n<p>이 과정에서 인덱스를 instagram-profiles로 생성한 후 Kibana에서 시각화를 시도했는데, 예상치 못한 문제들이 발생했습니다.</p>\n<p>이번 포스트에서는 이 문제를 해결하기 위해 고민한 내용을 공유하고자 합니다. 😀</p>\n<h2>🌟 프로젝트 개요</h2>\n<p>이 프로젝트의 목표는 Instagram Scraper API를 통해 수집한 데이터를 실시간으로 처리하고, Elasticsearch에 저장하여 Kibana에서 시각화하는 것이었습니다.</p>\n<p>전체적인 데이터 흐름은 다음과 같습니다:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1.\tInstagram Scraper API: 데이터를 수집하고, 이를 JSON 형식으로 Kafka로 전송.\n2.\tKafka: 데이터를 중간에서 처리하여 Logstash로 전달.\n3.\tLogstash: Kafka에서 받은 데이터를 Elasticsearch로 전달하며, 이 과정에서 데이터 변환을 수행.\n4.\tElasticsearch: 데이터를 저장하고 인덱싱하며, Kibana에서 검색 및 시각화에 사용할 수 있도록 준비.\n5.\tKibana: Elasticsearch에서 데이터를 가져와 시각화.</code></pre></div>\n<h2>❗️ 문제 발생</h2>\n<p>데이터 파이프라인을 설정하고, Kibana에서 데이터를 시각화하려는 과정에서 문제가 발생했습니다.</p>\n<p>데이터는 instagram-profiles 인덱스로 정상적으로 전달된 것처럼 보였지만, Kibana에서 시각화하는 데 어려움이 있었습니다.</p>\n<p>구체적으로, 여러 필드가 empty fields로 나타나거나, 데이터가 제대로 변환되지 않은 상태로 저장되고 있었습니다.</p>\n<h2>❓ 문제 원인 추정</h2>\n<ol>\n<li><strong>인덱스 매핑</strong>: Elasticsearch에서 <code class=\"language-text\">instagram-profiles</code> 인덱스의 매핑이 문제일 수 있습니다. 필드 타입이 적절하게 설정되지 않아, 데이터가 올바르게 저장되지 않고 빈 값으로 처리되고 있을 가능성이 있습니다.\n<ul>\n<li>예를 들어, <code class=\"language-text\">id</code> 필드는 <code class=\"language-text\">keyword</code>로 매핑되어야 하는데, 문자열로 매핑되어 데이터 검색에 문제가 발생할 수 있습니다.</li>\n</ul>\n</li>\n<li><strong>Logstash의 데이터 변환</strong>: Logstash 설정에서 JSON 데이터를 제대로 파싱하지 못하거나, 각 필드를 적절한 데이터 타입으로 변환하지 못하는 문제가 있을 수 있습니다. 특히, Logstash가 데이터를 파싱할 때 필드 이름이나 데이터 타입이 잘못 지정되면 Elasticsearch로 전달된 데이터가 예상한 대로 저장되지 않을 수 있습니다.\n<ul>\n<li>JSON 파싱 후 필드가 올바르게 변환되지 않아, Kibana에서 시각화가 제대로 이루어지지 않을 수 있습니다.</li>\n</ul>\n</li>\n</ol>\n<h2>📚 해결을 위한 시도</h2>\n<ol>\n<li>인덱스 매핑 확인 및 수정</li>\n</ol>\n<ul>\n<li>Elasticsearch에서 인덱스 매핑을 다시 한 번 확인하고, 각 필드가 적절한 데이터 타입으로 설정되어 있는지 검토했습니다.</li>\n</ul>\n<ol start=\"2\">\n<li>Logstash 필터 설정 검토</li>\n</ol>\n<ul>\n<li>Logstash의 필터 설정을 다시 검토하여, JSON 파싱과 필드 변환이 올바르게 이루어지고 있는지 확인했습니다.</li>\n<li>특히, mutate 필터를 사용해 각 필드를 적절한 데이터 타입으로 변환하고, 필요 없는 필드를 제거하는 작업을 강화했습니다.</li>\n</ul>\n<ol start=\"3\">\n<li>Kibana 인덱스 패턴 재색인</li>\n</ol>\n<ul>\n<li>위의 변경 사항을 적용한 후, Kibana에서 인덱스 패턴을 다시 색인하여 새로운 필드 설정이 반영되었는지 확인했습니다.</li>\n</ul>\n<h2>✅ 결론</h2>\n<p>이 프로젝트에서 직면한 문제는 주로 인덱스 매핑과 Logstash의 데이터 변환 설정에서 비롯된 것으로 보입니다.</p>\n<p>Elasticsearch와 Kibana에서 데이터를 올바르게 시각화하려면, 각 구성 요소가 데이터를 정확히 처리하고 전달하는 것이 중요합니다.</p>\n<p>앞으로도 이러한 문제를 해결하기 위해 지속적으로 모니터링하고, 필요한 경우 설정을 수정해 나갈 계획입니다.</p>\n<p>이와 같은 과정을 통해 문제를 해결하는 경험은 실무에서 매우 중요하다는 것을 깨닫게 되었습니다.</p>\n<p>여러분도 비슷한 상황을 겪고 있다면, 인덱스 매핑과 데이터 파이프라인의 각 단계를 꼼꼼히 점검해 보시길 권장합니다.</p>\n<p>자세한 내용은 깃허브에서 확인하실 수 있습니다. <a href=\"https://github.com/jms0522/Streaming-Data/issues/7\">GitHub Issue #7</a></p>\n<p>감사합니다 🙌🏻</p>","categories":["Kafka","ALL"],"date":"August 19, 2024","description":"Api로 받은 데이터를 Kafka - Kibana 시각화","id":"acb090c6-32b4-55b3-8768-e8c928ccad14","keywords":["API","Pipeline","Data","Blog","Kafka","Visualization"],"slug":"/my-first-article/instagram-profiles-kafka/","title":"🚀 [Kafka-ELK] Topic Visualization","readingTime":{"text":"7 min read"}},{"banner":{"alt":"ELK","caption":"Photo by <u><a href=\"https://unsplash.com/photos/Nc5Q_CEcY44\">ELK</a></u>","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABqklEQVR42nVSzy8DQRRu3By5uThJBBf8Bf0PRLi5urVxFJESJ3HgoGeNaDRSoSiNRhTdhk3Iduk2IdXV7Q8/upuqIrVrdmbM7narlfUyM3lv3vvm++bN2LBpCKpkYqQSV49JiMwcqo1ms5lZLQFruL+GzH2kQgswMVDwIW4ICfPEl7Oej4Tz+/sTQA0FaP7TtVOa8stMpibEACOoHfZwz20uDm8HHVdJt5YVpvGZDZcPjaq8n0p0T8iuADaYTXk2oJJL4lXven/vYM/CkZPTdg825iZHWpb33Cs5AWDsX/MO2O2eJNNIqzPrQemrOhYJjlKhmetzcvjJKd3W2tHuGO+L7r4AxRfY6lya7TryB3K81pq67LqK1Pvbdp6/rZSNxB13s89eHpeeFQgzb68Hj5nQU5Z5lRpUN3TbkCBJEp9Oi6IoyzKhIJcUi0VJFLHVQ/x2W9U7ly8UOI6LxWKCIFzQNMOyqVTqOBIpVyr1GguwwVytVuPxOEVRLMuGw2Gapnmej0ajRFG9xgJc41dVRVGMFUIIACD6CUbVH+Vf2U1fymQwHIQsPx7+ARXJj6JfkGjDAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/6f079f582ee9c2883f1783a2f8f3dcbe/ab14c/elk-stack.png","srcSet":"/static/6f079f582ee9c2883f1783a2f8f3dcbe/c7491/elk-stack.png 74w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/e8188/elk-stack.png 149w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/ab14c/elk-stack.png 297w","sizes":"(min-width: 297px) 297px, 100vw"},"sources":[{"srcSet":"/static/6f079f582ee9c2883f1783a2f8f3dcbe/21fa2/elk-stack.webp 74w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/c1258/elk-stack.webp 149w,\n/static/6f079f582ee9c2883f1783a2f8f3dcbe/280a7/elk-stack.webp 297w","type":"image/webp","sizes":"(min-width: 297px) 297px, 100vw"}]},"width":660,"height":400}}}},"body":"<h1>⚒️ About ELK</h1>\n<p><img src=\"/Users/jangminsoo/Desktop/github-blog/portfolio-minimal/content/images/elk-stack.png\" alt=\"ELK Stack\"></p>\n<h2>🚀 ELK</h2>\n<p>ELK Stack은 Elasticsearch, Logstash, 그리고 Kibana의 약자로, 로그 및 이벤트 데이터를 수집, 저장, 검색, 분석, 시각화하는 데 사용되는 오픈 소스 소프트웨어 스택입니다.\nELK Stack은 DevOps 및 데이터 분석에서 널리 사용되며, 시스템 성능을 모니터링하고, 애플리케이션 로그를 분석하며, 보안 위협을 감지하는 데 매우 유용합니다.</p>\n<h2>💬 들어가며</h2>\n<p><strong>Medical-AI 프로젝트에서 처음 실시간 데이터 파이프라인을 구축할 때 알게 되어 사용하게 되었고 다양한 장점이 있어 자주 사용하는 스택 중 하나입니다.</strong>\n지금 진행하고 있는 프로젝트에서도 사용하고 있기에, 오늘은 간단한 소개와 함께 공부해보는 시간을 갖기 위해 글을 작성합니다.\n제 개인적인 생각도 포함하고 있어 모든 게 정답은 아니니 참고하시는 용도로 봐주시면 감사하겠습니다. 🫡</p>\n<h3>Elasticsearch</h3>\n<ul>\n<li>\n<p><strong>개요</strong>: Elasticsearch는 분산 RESTful 검색 및 분석 엔진으로, 로그 데이터를 빠르게 저장하고 검색하는 데 최적화되어 있습니다.\nJSON 형식의 문서를 기반으로 데이터를 인덱싱하며, 다양한 유형의 데이터를 처리할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>특징</strong>:</p>\n<ul>\n<li><strong>분산형</strong>: 데이터가 여러 노드에 걸쳐 분산되어 저장되고 검색됩니다.</li>\n<li><strong>스케일링</strong>: 대량의 데이터를 실시간으로 처리하며, 클러스터를 확장하여 성능을 향상시킬 수 있습니다.</li>\n<li><strong>고급 검색 기능</strong>: 정규 표현식, 와일드카드, 유사 검색 등 다양한 검색 기능을 지원합니다.</li>\n</ul>\n</li>\n</ul>\n<h3>Logstash</h3>\n<ul>\n<li>\n<p><strong>개요</strong>: Logstash는 서버 측 데이터 처리 파이프라인으로, 다양한 소스에서 데이터를 수집하고, 필터링 및 변환하여 Elasticsearch로 전송합니다.</p>\n</li>\n<li>\n<p><strong>특징</strong>:</p>\n<ul>\n<li><strong>다양한 입력 플러그인</strong>: 파일, 데이터베이스, 메시징 시스템 등 다양한 데이터 소스에서 로그 데이터를 수집할 수 있습니다.</li>\n<li><strong>필터링</strong>: 데이터 변환, 정규화, 정제 작업을 수행할 수 있는 다양한 필터 플러그인을 제공합니다.</li>\n<li><strong>출력</strong>: 데이터를 Elasticsearch 외에도 다양한 출력 대상(파일, 메시지 큐 등)으로 보낼 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n<h3>Kibana</h3>\n<ul>\n<li>\n<p><strong>개요</strong>: Kibana는 Elasticsearch의 데이터 시각화 도구로, 대시보드와 차트를 사용하여 데이터를 시각적으로 분석할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>특징</strong>:</p>\n<ul>\n<li><strong>대시보드</strong>: 로그 및 메트릭 데이터를 시각화하여 대시보드에 표시합니다.</li>\n<li><strong>검색 및 탐색</strong>: Kibana의 UI를 통해 Elasticsearch에서 데이터를 검색하고 분석할 수 있습니다.</li>\n<li><strong>경고 기능</strong>: 특정 조건이 충족되면 경고를 발생시키는 기능을 제공합니다.</li>\n</ul>\n</li>\n</ul>\n<h3>Beats</h3>\n<ul>\n<li><strong>개요</strong>: 경량 데이터 수집 에이전트입니다.\nBeats는 시스템의 로그, 메트릭스, 네트워크 데이터를 수집하여 Logstash나 Elasticsearch로 전송하는 역할을 합니다.\nFilebeat, Metricbeat 등 다양한 종류가 있습니다.</li>\n</ul>\n<h2>✅ 활용사례</h2>\n<ul>\n<li>\n<p><strong>로그 관리</strong>: 애플리케이션, 서버, 네트워크 장비의 로그를 수집하고 분석하여 문제를 신속하게 진단할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>보안 모니터링</strong>: 보안 로그를 분석하여 침입 탐지, 비정상적인 활동 감지, 보안 정책 준수를 확인할 수 있습니다.</p>\n</li>\n<li>\n<p><strong>애플리케이션 성능 모니터링(APM)</strong>: 애플리케이션의 성능 데이터를 실시간으로 수집하여 성능 병목 지점을 식별하고 최적화할 수 있습니다.</p>\n<p>💬 저는 프로젝트 시, 로그 관리에 목적으로 많이 사용을 하거나, 수집된 데이터를 시각화하는 대시보드로도 많이 사용하고 있습니다.\nLogstash에서 데이터를 변환, 정체 작업을 수행할 수 있어 간단한 전처리가 가능한 점도 사용하기에 편리하다고 생각합니다.\n다양한 플러그인과 오픈 소스의 풍부한 생태계도 높은 확장성과 유연성을 제공합니다.</p>\n</li>\n</ul>\n<h2>❗️고려사항</h2>\n<ul>\n<li><strong>복잡성</strong>: 초기 설정 및 구성은 복잡할 수 있으며, 대규모 클러스터를 운영하려면 적절한 모니터링과 유지 관리가 필요합니다.</li>\n<li><strong>자원 소모</strong>: 특히 Elasticsearch는 데이터 양과 쿼리 복잡도에 따라 높은 자원을 요구할 수 있습니다.</li>\n</ul>\n<h2>🧑🏻‍💻 마치며</h2>\n<p>오늘은 간단하게 ELK를 소개해봤는데요,\n제 <a href=\"https://github.com/jms0522/Streaming-Data\">GitHub</a>에서 자세한 코드와 이슈를 확인하실 수 있습니다.\n감사합니다.</p>","categories":["Pipeline","ALL"],"date":"August 18, 2024","description":"다양한 상황에서 사용하는 ELK-stack에 대한 설명과 경험","id":"04bf8d8b-8adf-599a-ba1b-2eeb49c3b1a9","keywords":["ELK","Pipeline","Data","Blog","Streaming"],"slug":"/my-first-article/","title":"⚒️ [ELK-stack] About ELK-Stack ","readingTime":{"text":"7 min read"}}]}},"staticQueryHashes":["3262260831","948380417"],"slicesMap":{}}